{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e17cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports and installs ###\n",
    "# Installs\n",
    "# !pip install yfinance\n",
    "# !pip install pandas_datareader.date as web\n",
    "\n",
    "# Libraries\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "### Set start and end date\n",
    "START_DATE = \"2014-01-31\"\n",
    "END_DATE = \"2021-12-31\"\n",
    "\n",
    "# Set start date for stock universe\n",
    "STOCK_UNIVERSE_START_DATE = \"2013-12-31\"\n",
    "\n",
    "# Set epoch start\n",
    "EPOCH_START = pd.Timestamp(\"1970-01-01\") # Do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20816d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load model predictions ###\n",
    "# Baseline models\n",
    "ols3 = pd.read_parquet(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/OLS/ols_output.parquet\")\n",
    "enet = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/ElasticNet/en_prediction_df.csv\")\n",
    "pcr = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/PCR/pcr_prediction_df.csv\")\n",
    "pls = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/PLS/pls_prediction_df.csv\")\n",
    "rf = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/RandomForest/rf_prediction_df.csv\")\n",
    "gb = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/GradBoost/gb_prediction_df.csv\")\n",
    "nn1 = pd.read_parquet(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/NN1/nn1_output.parquet\")\n",
    "nn2 = pd.read_parquet(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/NN2/nn2_output.parquet\")\n",
    "nn3 = pd.read_parquet(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/NN3/nn3_output.parquet\")\n",
    "nn4 = pd.read_parquet(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/NN4/nn4_output.parquet\")\n",
    "\n",
    "# Outsider models\n",
    "ols3_out = pd.read_parquet(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/OLS_outsider/ols_outsider_output.parquet\")\n",
    "enet_out = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/ElasticNet_outsider/en_outsider_output.csv\")\n",
    "pcr_out = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/PCR_outsider/pcr_outsider_output.csv\")\n",
    "pls_out = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/PLS_outsider/pls_outsider_output.csv\")\n",
    "rf_out = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/RandomForest_outsider/rf_outsider_output.csv\")\n",
    "gb_out = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/GradBoost_outsider/gb_outsider_output.csv\")\n",
    "nn1_out = pd.read_parquet(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/NN1_outsider/nn1_outsider_output.parquet\")\n",
    "nn2_out = pd.read_parquet(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/NN2_outsider/nn2_outsider_output.parquet\")\n",
    "nn3_out = pd.read_parquet(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/NN3_outsider/nn3_outsider_output.parquet\")\n",
    "nn4_out = pd.read_parquet(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/NN4_outsider/nn4_outsider_output.parquet\")\n",
    "\n",
    "# Insider models\n",
    "ols3_is = pd.read_parquet(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/OLS_insider/ols_insider_output.parquet\")\n",
    "en_is = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/ElasticNet_insider/en_insider_output.csv\")\n",
    "pcr_is = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/PCR_insider/pcr_insider_output.csv\")\n",
    "pls_is = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/PLS_insider/pls_insider_output.csv\")\n",
    "rf_is = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/RandomForest_insider/rf_insider_output.csv\")\n",
    "gb_is = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/GradBoost_insider/gb_insider_output.csv\")\n",
    "nn1_is = pd.read_parquet(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/NN1_insider/nn1_insider_output.parquet\")\n",
    "nn2_is = pd.read_parquet(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/NN2_insider/nn2_insider_output.parquet\")\n",
    "nn3_is = pd.read_parquet(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/NN3_insider/nn3_insider_output.parquet\")\n",
    "nn4_is = pd.read_parquet(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/4.0 Empirical Results/NN4_insider/nn4_insider_output.parquet\")\n",
    "\n",
    "\n",
    "### Dictionaries for models ###\n",
    "\n",
    "# Model dict for main analysis\n",
    "model_dict = {\n",
    "    \"OLS3\": ols3,\n",
    "    \"OLS3 Outsider\": ols3_out,\n",
    "    \"OLS3 Insider\": ols3_is,\n",
    "    \"PLS\": pls,\n",
    "    \"PLS Outsider\": pls_out,\n",
    "    \"PLS Insider\": pls_is,\n",
    "    \"NN3\": nn3,\n",
    "    \"NN3 Outsider\": nn3_out,\n",
    "    \"NN3 Insider\": nn3_is,\n",
    "    \"NN4\": nn4,\n",
    "    \"NN4 Outsider\": nn4_out,\n",
    "    \"NN4 Insider\": nn4_is,\n",
    "}\n",
    "\n",
    "# # Model dict for apendix\n",
    "# model_dict = {\n",
    "#     \"OLS3\": ols3,\n",
    "#     \"OLS3 Outsider\": ols3_out,\n",
    "#     \"OLS3 Insider\": ols3_is,\n",
    "#     \"Elastic Net\": enet,\n",
    "#     \"Elastic Net Outsider\": enet_out,\n",
    "#     \"Elastic Net Insider\": en_is,\n",
    "#     \"PCR\": pcr,\n",
    "#     \"PCR Outsider\": pcr_out,\n",
    "#     \"PCR Insider\": pcr_is,\n",
    "#     \"PLS\": pls,\n",
    "#     \"PLS Outsider\": pls_out,\n",
    "#     \"PLS Insider\": pls_is,\n",
    "#     \"Random Forest\": rf,\n",
    "#     \"Random Forest Outsider\": rf_out,\n",
    "#     \"Random Forest Insider\": rf_is,\n",
    "#     \"Grad Boost\": gb,\n",
    "#     \"Grad Boost Outsider\": gb_out,\n",
    "#     \"Grad Boost Insider\": gb_is,\n",
    "#     \"NN1\": nn1,\n",
    "#     \"NN1 Outsider\": nn1_out,\n",
    "#     \"NN1 Insider\": nn1_is,\n",
    "#     \"NN2\": nn2,\n",
    "#     \"NN2 Outsider\": nn2_out,\n",
    "#     \"NN2 Insider\": nn2_is,\n",
    "#     \"NN3\": nn3,\n",
    "#     \"NN3 Outsider\": nn3_out,\n",
    "#     \"NN3 Insider\": nn3_is,\n",
    "#     \"NN4\": nn4,\n",
    "#     \"NN4 Outsider\": nn4_out,\n",
    "#     \"NN4 Insider\": nn4_is,\n",
    "# }\n",
    "\n",
    "# All to datetime\n",
    "for model_name in model_dict.keys():\n",
    "    model_dict[model_name][\"month\"] = pd.to_datetime(model_dict[model_name][\"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download FF5 + Momentum - Download is simplest from FF site directly ###\n",
    "\n",
    "# Load data\n",
    "mom = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/Fama-French/F-F_Momentum_Factor.csv\") # May be downloaded from Ken French's data library\n",
    "ff5 = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/Fama-French/F-F_Research_Data_5_Factors_2x3.csv\")\n",
    "\n",
    "# Change all columns to lowercase\n",
    "mom.columns = mom.columns.str.lower()\n",
    "ff5.columns = ff5.columns.str.lower()\n",
    "\n",
    "# Drop mkt-rf as sp500 is used as market factor\n",
    "ff5 = ff5.drop(columns=['mkt-rf'])\n",
    "\n",
    "# Convert 'month' to datetime\n",
    "mom['month'] = pd.to_datetime(mom['month'].astype(str), format='%Y%m') + pd.offsets.MonthEnd(0)\n",
    "ff5['month'] = pd.to_datetime(ff5['month'].astype(str), format='%Y%m') + pd.offsets.MonthEnd(0)\n",
    "\n",
    "# Merge together on 'month'\n",
    "ff_df = pd.merge(ff5, mom, on='month', how='inner')\n",
    "\n",
    "### Only use data within start and end date ###\n",
    "ff_df = ff_df[(ff_df['month'] >= pd.to_datetime(START_DATE)) & (ff_df['month'] <= pd.to_datetime(END_DATE))]\n",
    "\n",
    "### Preperation ###\n",
    "# Ensure no missing values and no values equal to -99.99 or -999\n",
    "assert (ff_df.isin([-99.99, -999])).any().any() == False, \"Invalid values\"\n",
    "assert ff_df.isnull().sum().sum() == 0, \"Missing values present\"\n",
    "\n",
    "# Convert percentages to decimals\n",
    "for col in ff_df.columns:\n",
    "    if col != 'month':\n",
    "        ff_df[col] = ff_df[col] / 100\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ece524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Download S&P 500 data from Yahoo Finance ###\n",
    "# sp500 = yf.download(\n",
    "#     \"^GSPC\",\n",
    "#     start=\"2013-11-30\",\n",
    "#     end=\"2021-12-31\",\n",
    "#     interval=\"1mo\",\n",
    "#     auto_adjust=True,\n",
    "#     progress=False,\n",
    "# )\n",
    "\n",
    "# # Save as csv for future use\n",
    "# sp500.to_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/SP500/sp500_monthly.csv\")\n",
    "\n",
    "### Load S&P 500 data ###\n",
    "sp500 = pd.read_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/SP500/sp500_monthly.csv\")\n",
    "\n",
    "# Remove two first rows\n",
    "sp500 = sp500[2:]\n",
    "\n",
    "# Rename Price to month - wierd formatting from yfinance\n",
    "sp500 = sp500.rename(columns={\"Price\": \"month\"})\n",
    "\n",
    "# Convert month to datetime and month end\n",
    "sp500['month'] = pd.to_datetime(sp500['month']) + pd.offsets.MonthEnd(0)\n",
    "\n",
    "# Convert Close to numeric\n",
    "sp500['Close'] = pd.to_numeric(sp500['Close'], errors='coerce')\n",
    "\n",
    "# Compute pct_change to get monthly returns\n",
    "sp500['sp500_ret'] = sp500['Close'].pct_change()\n",
    "\n",
    "# Drop all rows but month and sp500_ret\n",
    "sp500 = sp500[['month', 'sp500_ret']]\n",
    "\n",
    "# Drop na values\n",
    "sp500 = sp500.dropna()\n",
    "\n",
    "# Check\n",
    "assert sp500.isnull().sum().sum() == 0, \"Missing values present in sp500\"\n",
    "\n",
    "# Compute excess returns\n",
    "sp500 = pd.merge(sp500, ff_df[['month', 'rf']], on='month', how='left')\n",
    "sp500['sp500_rf'] = sp500['sp500_ret'] - sp500['rf']\n",
    "sp500 = sp500[['month', 'sp500_rf']]\n",
    "\n",
    "# Output Sharpe Ratio of sp500\n",
    "sp500_sr = (sp500['sp500_rf'].mean() / sp500['sp500_rf'].std()) * np.sqrt(12)\n",
    "print(f\"S&P 500 Annualized Sharpe Ratio: {sp500_sr:.4f}\")\n",
    "\n",
    "# # Done\n",
    "# sp500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge factor data with sp500 ###\n",
    "ff_df = pd.merge(ff_df, sp500, on='month', how='left')\n",
    "\n",
    "# Ensure no missing values in ff_df\n",
    "assert ff_df.isnull().sum().sum() == 0, \"Missing values present in ff_df\"\n",
    "\n",
    "# Drop mkt-rf and rf\n",
    "ff_df = ff_df.drop(columns=['rf'])\n",
    "\n",
    "# Rearrange columns to sp500_rf first\n",
    "cols = ff_df.columns.tolist()\n",
    "cols.insert(1, cols.pop(cols.index('sp500_rf')))\n",
    "ff_df = ff_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db1d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Amihud (2002) illiquidity — correct daily monthly pipeline ###\n",
    "\n",
    "### Load CRSP MONTHLY data ###\n",
    "path = \"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/SQLite/Tables\"\n",
    "\n",
    "if os.path.exists(path):\n",
    "    crsp_monthly = pd.read_parquet(os.path.join(path, \"crsp_monthly.parquet\"))\n",
    "else:\n",
    "    raise FileNotFoundError(\"No path found\")\n",
    "\n",
    "# Rename and align to month end\n",
    "crsp_monthly = crsp_monthly.rename(columns={\"date\": \"month\"})\n",
    "crsp_monthly[\"month\"] = EPOCH_START + pd.to_timedelta(crsp_monthly[\"month\"], unit=\"D\")\n",
    "crsp_monthly[\"month\"] = crsp_monthly[\"month\"] + pd.offsets.MonthEnd(0)\n",
    "\n",
    "# Restrict sample period\n",
    "crsp_monthly = crsp_monthly[\n",
    "    (crsp_monthly[\"month\"] >= pd.to_datetime(STOCK_UNIVERSE_START_DATE)) &\n",
    "    (crsp_monthly[\"month\"] <= pd.to_datetime(END_DATE))\n",
    "]\n",
    "\n",
    "# Ensure correct permno format\n",
    "crsp_monthly[\"permno\"] = (\n",
    "    crsp_monthly[\"permno\"]\n",
    "    .astype(str)\n",
    "    .str.lstrip(\"0\")\n",
    ")\n",
    "crsp_monthly[\"permno\"] = pd.to_numeric(\n",
    "    crsp_monthly[\"permno\"], errors=\"coerce\"\n",
    ").astype(\"Int32\")\n",
    "\n",
    "### Assign exchange ###\n",
    "def assign_exchange(primaryexch):\n",
    "    if primaryexch == \"N\":\n",
    "        return \"NYSE\"\n",
    "    elif primaryexch == \"A\":\n",
    "        return \"AMEX\"\n",
    "    elif primaryexch == \"Q\":\n",
    "        return \"NASDAQ\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "crsp_monthly[\"exchange\"] = crsp_monthly[\"primaryexch\"].apply(assign_exchange)\n",
    "\n",
    "\n",
    "\n",
    "### NYSE sample for breakpoints ###\n",
    "crsp_nyse = crsp_monthly[crsp_monthly[\"exchange\"] == \"NYSE\"].copy()\n",
    "\n",
    "\n",
    "\n",
    "### Load CRSP DAILY data ###\n",
    "crsp_daily = pd.read_feather(\n",
    "    \"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/Daily Data/crsp_daily.feather\"\n",
    ")\n",
    "\n",
    "# Ensure date is datetime\n",
    "crsp_daily[\"date\"] = pd.to_datetime(crsp_daily[\"date\"], errors =\"coerce\")\n",
    "\n",
    "# Restrict sample period\n",
    "crsp_daily = crsp_daily[\n",
    "    (crsp_daily[\"date\"] >= pd.to_datetime(STOCK_UNIVERSE_START_DATE)) &\n",
    "    (crsp_daily[\"date\"] <= pd.to_datetime(END_DATE))\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "### Amihud construction ###\n",
    "\n",
    "# Dollar volume\n",
    "crsp_daily[\"dollar_volume\"] = crsp_daily[\"prc_adj\"].abs() * crsp_daily[\"vol_adj\"]\n",
    "\n",
    "crsp_daily = crsp_daily[\n",
    "    (crsp_daily[\"dollar_volume\"] > 0) &\n",
    "    (crsp_daily[\"ret_excess\"].notna())\n",
    "]\n",
    "\n",
    "# Daily Amihud\n",
    "crsp_daily[\"amihud\"] = (\n",
    "    crsp_daily[\"ret_excess\"].abs() / crsp_daily[\"dollar_volume\"]\n",
    ")\n",
    "\n",
    "# Change to month to aggegate\n",
    "crsp_daily[\"month\"] = crsp_daily[\"date\"] + pd.offsets.MonthEnd(0)\n",
    "\n",
    "\n",
    "\n",
    "### Monthly Amihud (mean of daily ratios) ###\n",
    "crsp_monthly_amihud = (\n",
    "    crsp_daily\n",
    "    .groupby([\"permno\", \"month\"], as_index=False)\n",
    "    .agg({\"amihud\": \"mean\"})\n",
    ")\n",
    "\n",
    "### NYSE-based breakpoints for Amihud and Size ###\n",
    "\n",
    "# Merge NYSE stocks with monthly Amihud\n",
    "nyse_break_df = crsp_nyse.merge(\n",
    "    crsp_monthly_amihud,\n",
    "    on=[\"permno\", \"month\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Function to compute breakpoints\n",
    "def nyse_breakpoints(df):\n",
    "    out = []\n",
    "    for month, group in df.groupby(\"month\"):\n",
    "        out.append({\n",
    "            \"month\": month,\n",
    "            \"amihud_p33\": group[\"amihud\"].quantile(0.33),\n",
    "            \"amihud_p67\": group[\"amihud\"].quantile(0.67),\n",
    "            \"size_p33\": group[\"mktcap\"].quantile(0.33),\n",
    "            \"size_p67\": group[\"mktcap\"].quantile(0.67),\n",
    "        })\n",
    "    return (\n",
    "        pd.DataFrame(out)\n",
    "        .sort_values(\"month\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "# Call function\n",
    "breakpoints = nyse_breakpoints(nyse_break_df)\n",
    "\n",
    "\n",
    "# Lag break points (t-1 applies at t)\n",
    "breakpoints_lag = breakpoints.copy()\n",
    "breakpoints_lag[\"month\"] = breakpoints_lag[\"month\"] + pd.offsets.MonthEnd(1)\n",
    "\n",
    "\n",
    "\n",
    "### Full stock universe ###\n",
    "stock_universe = pd.read_csv(\n",
    "    \"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/Stock Universe/stock_universe.csv\"\n",
    ")\n",
    "\n",
    "# Ensure month is datetime\n",
    "stock_universe[\"month\"] = pd.to_datetime(stock_universe[\"month\"], errors =\"coerce\")\n",
    "\n",
    "# Restrict sample period\n",
    "stock_universe = stock_universe[\n",
    "    (stock_universe[\"month\"] >= pd.to_datetime(STOCK_UNIVERSE_START_DATE)) &\n",
    "    (stock_universe[\"month\"] <= pd.to_datetime(END_DATE))\n",
    "]\n",
    "\n",
    "# Merge Amihud with stock universe on permno and month\n",
    "stock_universe = stock_universe.merge(\n",
    "    crsp_monthly_amihud,\n",
    "    on=[\"permno\", \"month\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Lag Amihud\n",
    "stock_universe = stock_universe.sort_values([\"permno\", \"month\"])\n",
    "stock_universe[\"amihud_lag\"] = (\n",
    "    stock_universe\n",
    "    .groupby(\"permno\")[\"amihud\"]\n",
    "    .shift(1)\n",
    ")\n",
    "\n",
    "# Drop amihud column\n",
    "stock_universe = stock_universe.drop(columns=[\"amihud\"])\n",
    "\n",
    "### Restrict the stock universe ###\n",
    "stock_universe_restricted = stock_universe.merge(\n",
    "    breakpoints_lag,\n",
    "    on=\"month\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Drop rows that do not satisfy amihud <= amihud_p33 and mktcap >= size_p67\n",
    "stock_universe_restricted = stock_universe_restricted[\n",
    "    (stock_universe_restricted[\"amihud_lag\"] <= stock_universe_restricted[\"amihud_p33\"]) &\n",
    "    (stock_universe_restricted[\"mktcap_lag\"] > stock_universe_restricted[\"size_p67\"])\n",
    "    # (stock_universe_restricted[\"mktcap_lag\"] > stock_universe_restricted[\"size_p33\"])\n",
    "    \n",
    "]\n",
    "\n",
    "# Keep only necessary columns\n",
    "stock_universe_restricted = stock_universe_restricted[\n",
    "    [\"month\", \"permno\", \"amihud_lag\", \"mktcap_lag\"]\n",
    "].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "### Final dataset ###\n",
    "stock_universe_restricted = (\n",
    "    stock_universe_restricted\n",
    "    .dropna(subset=[\"amihud_lag\", \"mktcap_lag\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "### Descriptive Statistics ###\n",
    "amihud_describe = stock_universe_restricted.copy()\n",
    "stocks_describe = stock_universe_restricted.copy()\n",
    "\n",
    "# Scale for readability - 1 million dollar trading volume\n",
    "amihud_describe[\"amihud_lag\"] = amihud_describe[\"amihud_lag\"] * 1e6\n",
    "display(amihud_describe[[\"amihud_lag\", \"mktcap_lag\"]].describe())\n",
    "\n",
    "\n",
    "print(f\"Unique permnos: {stocks_describe['permno'].nunique()}\")\n",
    "display(stocks_describe.groupby(\"month\")[\"permno\"].nunique().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### If restricted == True ###\n",
    "\n",
    "# Keep only identifiers from restricted universe\n",
    "stock_universe_restricted_ids = (\n",
    "    stock_universe_restricted\n",
    "    [[\"month\", \"permno\"]]\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "restricted = True  # Change to False to use full stock universe\n",
    "\n",
    "if restricted:\n",
    "    for model in model_dict:\n",
    "        model_dict[model] = (\n",
    "            model_dict[model]\n",
    "            .merge(\n",
    "                stock_universe_restricted_ids,\n",
    "                on=[\"month\", \"permno\"],\n",
    "                how=\"inner\"\n",
    "            )\n",
    "        )\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60def9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Table 7 ###\n",
    "\n",
    "### Assign deciles ###\n",
    "\n",
    "def assign_deciles(df, pred_ret_excess):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Detect months with a genuine cross-sectional signal\n",
    "    df[\"has_signal\"] = df.groupby(\"month\")[pred_ret_excess].transform(\n",
    "        lambda x: x.nunique() > 1\n",
    "    )\n",
    "\n",
    "    # Default: in no-signal months all stocks get decile = 1\n",
    "    df[\"decile\"] = 1\n",
    "    mask = df[\"has_signal\"]\n",
    "\n",
    "    # Construct deciles in signal months\n",
    "    df.loc[mask, \"decile\"] = (\n",
    "        df.loc[mask]\n",
    "        .groupby(\"month\")[pred_ret_excess]\n",
    "        .transform(\n",
    "            lambda x: pd.qcut(\n",
    "                x.rank(method=\"first\"),\n",
    "                4,\n",
    "                labels=False,\n",
    "                duplicates=\"drop\"\n",
    "            ) + 1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df.drop(columns=[\"has_signal\"])\n",
    "\n",
    "\n",
    "### Compute monthly decile portfolio returns ###\n",
    "\n",
    "def compute_monthly_returns(df, ret_excess, weight=\"equal\"):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify no-signal months\n",
    "    no_signal_months = df.groupby(\"month\")[\"decile\"].nunique() == 1\n",
    "\n",
    "    # Equal-weighted portfolios\n",
    "    if weight == \"equal\":\n",
    "        portfolios = (\n",
    "            df.groupby([\"month\", \"decile\"])[ret_excess]\n",
    "              .mean()\n",
    "              .unstack(\"decile\")\n",
    "        )\n",
    "\n",
    "    # Value-weighted portfolios\n",
    "    elif weight == \"value\":\n",
    "        df[\"weighted_return\"] = df[ret_excess] * df[\"mktcap_lag\"]\n",
    "        portfolios = (\n",
    "            df.groupby([\"month\", \"decile\"])\n",
    "              .apply(\n",
    "                  lambda x: x[\"weighted_return\"].sum()\n",
    "                  / x[\"mktcap_lag\"].sum()\n",
    "              )\n",
    "              .unstack(\"decile\")\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"weight must be 'equal' or 'value'\")\n",
    "\n",
    "    # Force H = L in no-signal months\n",
    "    for month in portfolios.index:\n",
    "        if no_signal_months.loc[month]:\n",
    "            portfolios.loc[month, :] = portfolios.loc[month, 1]\n",
    "\n",
    "    portfolios.columns = [f\"decile_{c}\" for c in portfolios.columns]\n",
    "    return portfolios\n",
    "\n",
    "\n",
    "# Build Table 7 summary\n",
    "\n",
    "def summarize(df,\n",
    "              monthly_portfolios,\n",
    "              pred_ret_excess,\n",
    "              weight,                      \n",
    "              portfolio_type=\"HL\"):\n",
    "\n",
    "    # Predicted decile returns\n",
    "    pred_monthly = compute_monthly_returns(\n",
    "        df=df,\n",
    "        ret_excess=pred_ret_excess,\n",
    "        weight=weight\n",
    "    )\n",
    "\n",
    "    pred = (\n",
    "        pred_monthly\n",
    "          .mean()\n",
    "          .reindex([f\"decile_{i}\" for i in range(1, 5)])\n",
    "          .values\n",
    "    )\n",
    "\n",
    "    # Observed portfolio statistics\n",
    "    avg = monthly_portfolios.mean()\n",
    "    sd  = monthly_portfolios.std()\n",
    "    sr  = avg / sd * np.sqrt(12)\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"Pred\": pred * 100,\n",
    "            \"Avg\":  avg.values * 100,\n",
    "            \"SD\":   sd.values * 100,\n",
    "            \"SR\":   sr.values, # Don't put * sqrt(12) here\n",
    "        },\n",
    "        index=[str(i) for i in range(1, 5)]\n",
    "    )\n",
    "\n",
    "    # Selected portfolio row\n",
    "    if portfolio_type == \"HL\":\n",
    "        series = monthly_portfolios[\"decile_4\"] - monthly_portfolios[\"decile_1\"]\n",
    "        pred_val = out.loc[\"4\", \"Pred\"] - out.loc[\"1\", \"Pred\"]\n",
    "        label = \"H-L\"\n",
    "\n",
    "    elif portfolio_type == \"H\":\n",
    "        series = monthly_portfolios[\"decile_4\"]\n",
    "        pred_val = out.loc[\"4\", \"Pred\"]\n",
    "        label = \"H\"\n",
    "\n",
    "    elif portfolio_type == \"L\":\n",
    "        series = monthly_portfolios[\"decile_1\"]\n",
    "        pred_val = out.loc[\"1\", \"Pred\"]\n",
    "        label = \"L\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"portfolio_type must be 'HL', 'H', or 'L'\")\n",
    "\n",
    "    avg_val = series.mean()\n",
    "    sd_val  = series.std()\n",
    "    sr_val  = avg_val / sd_val\n",
    "\n",
    "    out.loc[label] = [pred_val, avg_val * 100, sd_val * 100, sr_val * np.sqrt(12)]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# Full Table 7 pipeline\n",
    "    \n",
    "def portfolio_summary(df,\n",
    "                      ret_excess=\"ret_excess\",\n",
    "                      pred_ret_excess=\"pred_ret_excess\",\n",
    "                      weight=\"value\",\n",
    "                      portfolio_type=\"HL\"):\n",
    "\n",
    "    df_dec = assign_deciles(df, pred_ret_excess)\n",
    "\n",
    "    monthly_portfolios = compute_monthly_returns(\n",
    "        df=df_dec,\n",
    "        ret_excess=ret_excess,\n",
    "        weight=weight\n",
    "    )\n",
    "\n",
    "    summary_table = summarize(\n",
    "        df=df_dec,\n",
    "        monthly_portfolios=monthly_portfolios,\n",
    "        pred_ret_excess=pred_ret_excess,\n",
    "        weight=weight,\n",
    "        portfolio_type=portfolio_type\n",
    "    )\n",
    "\n",
    "    return summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31224caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Table 8 ###\n",
    "\n",
    "# Portfolio extraction helper\n",
    "\n",
    "def extract_portfolio_series(monthly, portfolio_type):\n",
    "    if portfolio_type == \"HL\":\n",
    "        return monthly[\"decile_4\"] - monthly[\"decile_1\"]\n",
    "    elif portfolio_type == \"H\":\n",
    "        return monthly[\"decile_4\"]\n",
    "    elif portfolio_type == \"L\":\n",
    "        return monthly[\"decile_1\"]\n",
    "    else:\n",
    "        raise ValueError(\"portfolio_type must be 'HL', 'H', or 'L'\")\n",
    "\n",
    "\n",
    "# GKX Turnover (H, L, HL) — exact formula with failsafe\n",
    "\n",
    "def compute_gkx_turnover(df_dec,\n",
    "                         weight_type,\n",
    "                         portfolio_type=\"HL\"):\n",
    "    \"\"\"\n",
    "    GKX turnover as in Gu, Kelly, and Xiu (2020):\n",
    "\n",
    "    Turnover_t = sum_i | w_{i,t+1}\n",
    "                  - w_{i,t}(1+r_{i,t+1})\n",
    "                    / (1 + sum_j w_{j,t} r_{j,t+1}) |\n",
    "\n",
    "    Implemented for:\n",
    "        HL : long–short (decile 10 long, decile 1 short)\n",
    "        H  : long-only high portfolio\n",
    "        L  : short-only low portfolio\n",
    "\n",
    "    Failsafe:\n",
    "        If denominator == 0 or non-finite, turnover_t = np.nan\n",
    "    \"\"\"\n",
    "\n",
    "    df = df_dec.copy()\n",
    "    df[\"month\"] = pd.to_datetime(df[\"month\"])\n",
    "\n",
    "    # Select universe\n",
    "    if portfolio_type == \"HL\":\n",
    "        df_u = df[df[\"decile\"].isin([1, 4])].copy()\n",
    "    elif portfolio_type == \"H\":\n",
    "        df_u = df[df[\"decile\"] == 4].copy()\n",
    "    elif portfolio_type == \"L\":\n",
    "        df_u = df[df[\"decile\"] == 1].copy()\n",
    "    else:\n",
    "        raise ValueError(\"portfolio_type must be 'HL', 'H', or 'L'\")\n",
    "\n",
    "    df_u = df_u.sort_values([\"month\", \"permno\"])\n",
    "\n",
    "    months = np.sort(df_u[\"month\"].unique())\n",
    "    turnovers = []\n",
    "\n",
    "    for k in range(len(months) - 1):\n",
    "        t, t1 = months[k], months[k + 1]\n",
    "\n",
    "        # Holdings at t\n",
    "        w_t = df_u[df_u[\"month\"] == t].copy()\n",
    "        if w_t.empty:\n",
    "            continue\n",
    "\n",
    "        # Next-month returns\n",
    "        ret_df = (\n",
    "            df[df[\"month\"] == t1][[\"permno\", \"ret_excess\"]]\n",
    "            .rename(columns={\"ret_excess\": \"ret_next\"})\n",
    "        )\n",
    "\n",
    "        w_t = w_t.merge(ret_df, on=\"permno\", how=\"inner\")\n",
    "        if w_t.empty:\n",
    "            continue\n",
    "\n",
    "        # Weights at t\n",
    "        if weight_type == \"equal\":\n",
    "            w = np.repeat(1.0 / len(w_t), len(w_t))\n",
    "        elif weight_type == \"value\":\n",
    "            m = w_t[\"mktcap_lag\"].to_numpy()\n",
    "            if m.sum() <= 0:\n",
    "                turnovers.append(np.nan)\n",
    "                continue\n",
    "            w = m / m.sum()\n",
    "        else:\n",
    "            raise ValueError(\"weight_type must be 'equal' or 'value'\")\n",
    "\n",
    "        # Sign convention\n",
    "        if portfolio_type == \"L\":\n",
    "            w = -w\n",
    "        elif portfolio_type == \"HL\":\n",
    "            w = np.where(w_t[\"decile\"] == 4, w, -w)\n",
    "\n",
    "        w_t[\"w_t\"] = w\n",
    "\n",
    "        # ===== GKX drift =====\n",
    "        num = w_t[\"w_t\"] * (1.0 + w_t[\"ret_next\"])\n",
    "        denom = 1.0 + np.sum(w_t[\"w_t\"] * w_t[\"ret_next\"])\n",
    "\n",
    "        # Failsafe\n",
    "        if (not np.isfinite(denom)) or (abs(denom) < 1e-12):\n",
    "            turnovers.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        w_t[\"w_post\"] = num / denom\n",
    "\n",
    "        # Target weights at t+1\n",
    "        w_t1 = df_u[df_u[\"month\"] == t1].copy()\n",
    "        if w_t1.empty:\n",
    "            turnovers.append(np.abs(w_t[\"w_post\"]).sum())\n",
    "            continue\n",
    "\n",
    "        if weight_type == \"equal\":\n",
    "            w1 = np.repeat(1.0 / len(w_t1), len(w_t1))\n",
    "        else:\n",
    "            m1 = w_t1[\"mktcap_lag\"].to_numpy()\n",
    "            if m1.sum() <= 0:\n",
    "                turnovers.append(np.nan)\n",
    "                continue\n",
    "            w1 = m1 / m1.sum()\n",
    "\n",
    "        if portfolio_type == \"L\":\n",
    "            w1 = -w1\n",
    "        elif portfolio_type == \"HL\":\n",
    "            w1 = np.where(w_t1[\"decile\"] == 4, w1, -w1)\n",
    "\n",
    "        w_t1[\"w_t1\"] = w1\n",
    "\n",
    "        # Align assets\n",
    "        post = w_t.set_index(\"permno\")[\"w_post\"]\n",
    "        target = w_t1.set_index(\"permno\")[\"w_t1\"]\n",
    "\n",
    "        all_ids = post.index.union(target.index)\n",
    "        post = post.reindex(all_ids, fill_value=0.0)\n",
    "        target = target.reindex(all_ids, fill_value=0.0)\n",
    "\n",
    "        turnover_t = np.abs(target - post).sum()\n",
    "        turnovers.append(turnover_t)\n",
    "\n",
    "    # Average across months (ignoring nan)\n",
    "    return float(np.nanmean(turnovers)) if len(turnovers) else np.nan\n",
    "\n",
    "\n",
    "### Risk measures ###\n",
    "\n",
    "def max_drawdown(ret):\n",
    "    cum = (1 + ret).cumprod()\n",
    "    peak = cum.cummax()\n",
    "    dd = (peak - cum) / peak\n",
    "    return float(dd.max() * 100.0)\n",
    "\n",
    "\n",
    "def max_1m_loss(ret):\n",
    "    return -float(ret.min())\n",
    "\n",
    "\n",
    "### FF5 + Momentum alpha (HAC) ###\n",
    "\n",
    "def ff5mom_alpha(port_df, ff_df):\n",
    "    merged = ff_df.merge(port_df, on=\"month\", how=\"inner\")\n",
    "\n",
    "    X = sm.add_constant(\n",
    "        merged[[\"sp500_rf\", \"smb\", \"hml\", \"rmw\", \"cma\", \"mom\"]]\n",
    "    )\n",
    "    y = merged[\"PORT\"]\n",
    "\n",
    "    model = sm.OLS(y, X).fit(\n",
    "        cov_type=\"HAC\",\n",
    "        cov_kwds={\"maxlags\": 12}\n",
    "    )\n",
    "    alpha = float(model.params[\"const\"])\n",
    "    talpha = float(model.tvalues[\"const\"])\n",
    "    r2 = float(model.rsquared)\n",
    "\n",
    "    return alpha, talpha, r2\n",
    "\n",
    "\n",
    "### Information Ratio ###\n",
    "\n",
    "def compute_information_ratio(port, benchmark):\n",
    "    diff = port - benchmark\n",
    "    te = diff.std()\n",
    "    return (diff.mean() / te) if te != 0 else np.nan\n",
    "\n",
    "### Sharpe Ratio ###\n",
    "\n",
    "def compute_sharpe_ratio(ret):\n",
    "    mu = ret.mean()\n",
    "    sigma = ret.std()\n",
    "    return (mu / sigma) * np.sqrt(12) if sigma != 0 else np.nan\n",
    "\n",
    "\n",
    "\n",
    "### Table 8 statistics ###\n",
    "\n",
    "def table8_stats(port_df, df_dec, ff_df, weight_type, portfolio_type):\n",
    "\n",
    "    port = port_df[\"PORT\"]\n",
    "\n",
    "    max_dd = max_drawdown(port)\n",
    "    max_loss = max_1m_loss(port)\n",
    "\n",
    "    # Turnover: GKX formula for the chosen portfolio_type\n",
    "    turnover = compute_gkx_turnover(\n",
    "        df_dec=df_dec,\n",
    "        weight_type=weight_type,\n",
    "        portfolio_type=portfolio_type\n",
    "    )\n",
    "\n",
    "    # IR: ALWAYS vs benchmark (SP500-RF), annualized inside compute_information_ratio\n",
    "    ir = compute_information_ratio(port, ff_df[\"sp500_rf\"])\n",
    "    \n",
    "    # Sharpe Ratio: annualized inside compute_sharpe_ratio\n",
    "    sr_an = compute_sharpe_ratio(port)\n",
    "\n",
    "    mean_ret = float(port.mean() * 100.0)\n",
    "    alpha, talpha, r2 = ff5mom_alpha(port_df, ff_df)\n",
    "\n",
    "    return {\n",
    "        \"Max DD (%)\": max_dd,\n",
    "        \"Max 1M Loss (%)\": max_loss * 100.0,\n",
    "        \"Turnover (%)\": turnover * 100.0 if turnover is not np.nan else np.nan,\n",
    "        \"Mean Ret (%)\": mean_ret,\n",
    "        \"FF5+Mom α (%)\": alpha * 100.0,\n",
    "        \"t(α)\": talpha,\n",
    "        \"R² (%)\": r2 * 100.0,\n",
    "        \"IR\": ir * np.sqrt(12) if ir is not np.nan else np.nan,\n",
    "        \"SR\": sr_an,\n",
    "    }\n",
    "\n",
    "\n",
    "### Full Table 8 computation ###\n",
    "\n",
    "def compute_table8(df, ff_df, pred_ret_excess=\"pred_ret_excess\",\n",
    "                   portfolio_type=\"HL\"):\n",
    "\n",
    "    df_dec = assign_deciles(df, pred_ret_excess)\n",
    "    out = {}\n",
    "\n",
    "    for weight in [\"equal\", \"value\"]:\n",
    "\n",
    "        monthly = compute_monthly_returns(\n",
    "            df_dec, \"ret_excess\", weight\n",
    "        ).reset_index()\n",
    "\n",
    "        port = extract_portfolio_series(monthly, portfolio_type)\n",
    "\n",
    "        port_df = pd.DataFrame({\n",
    "            \"month\": monthly[\"month\"],\n",
    "            \"PORT\": port\n",
    "        })\n",
    "\n",
    "        out[weight] = table8_stats(\n",
    "            port_df, df_dec, ff_df, weight, portfolio_type\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(out).T.rename(\n",
    "        index={\"equal\": \"Equal-weighted\", \"value\": \"Value-weighted\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d028c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get Table 7 and Table 8 for all models ###\n",
    "\n",
    "def evaluate_all_models(model_dict,\n",
    "                        ff_df,\n",
    "                        value_weighted=True,\n",
    "                        portfolio_type=\"HL\"):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_dict : dict\n",
    "        {\n",
    "            \"Model Name\": prediction_df,\n",
    "            ...\n",
    "        }\n",
    "\n",
    "    ff_df : pd.DataFrame\n",
    "        Cleaned Fama-French 5 + Momentum dataframe\n",
    "\n",
    "    value_weighted : bool\n",
    "        If True, Table 7 is value-weighted, else equal-weighted\n",
    "\n",
    "    portfolio_type : str\n",
    "        \"HL\", \"H\", or \"L\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    table7_all : pd.DataFrame\n",
    "        Stacked Table 7 for all models\n",
    "\n",
    "    table8_rotated : pd.DataFrame\n",
    "        Table 8 with statistics as rows and (Model, Weight) as columns\n",
    "    \"\"\"\n",
    "\n",
    "    all_t7 = []\n",
    "    all_t8 = []\n",
    "\n",
    "    for model_name, prediction_df in model_dict.items():\n",
    "\n",
    "        # Table 7\n",
    "\n",
    "        t7 = portfolio_summary(\n",
    "            df=prediction_df,\n",
    "            ret_excess=\"ret_excess\",\n",
    "            pred_ret_excess=\"pred_ret_excess\",\n",
    "            weight=\"value\" if value_weighted else \"equal\",\n",
    "            portfolio_type=\"HL\"\n",
    "        )\n",
    "\n",
    "        t7.index = pd.MultiIndex.from_product(\n",
    "            [[model_name], t7.index],\n",
    "            names=[\"Model\", \"Portfolio\"]\n",
    "        )\n",
    "\n",
    "        all_t7.append(t7)\n",
    "\n",
    "        # Table 8\n",
    "\n",
    "        t8 = compute_table8(\n",
    "            df=prediction_df,\n",
    "            ff_df=ff_df,\n",
    "            pred_ret_excess=\"pred_ret_excess\",\n",
    "            portfolio_type=portfolio_type\n",
    "        )\n",
    "\n",
    "        # t8 index is [\"Equal-weighted\", \"Value-weighted\"]\n",
    "        t8.index = pd.MultiIndex.from_product(\n",
    "            [[model_name], t8.index],\n",
    "            names=[\"Model\", \"Weight\"]\n",
    "        )\n",
    "\n",
    "        all_t8.append(t8)\n",
    "\n",
    "    # Combine all models\n",
    "\n",
    "    table7_all = pd.concat(all_t7)\n",
    "    table8_all = pd.concat(all_t8)\n",
    "\n",
    "    # Rotate Table 8:\n",
    "    # rows    -> statistics\n",
    "    # columns -> (Model, Weight)\n",
    "\n",
    "    table8_rotated = (\n",
    "        table8_all\n",
    "        .stack()                  # stats -> row index\n",
    "        .unstack(level=[0, 1])    # columns = (Model, Weight)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "\n",
    "    return table7_all, table8_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed8c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "table_7_value, _ = evaluate_all_models(model_dict, ff_df, value_weighted=True, portfolio_type=\"H\")\n",
    "table_7_equal, table_8 = evaluate_all_models(model_dict, ff_df, value_weighted=False, portfolio_type=\"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc14d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Value\n",
    "# table_7_value.to_csv(\"table_7_value_size_liq.csv\")\n",
    "\n",
    "# # Equal\n",
    "# table_7_equal.to_csv(\"table_7_equal_size_liq.csv\")\n",
    "# table_8.to_csv(\"table_8_size_liq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decile_portfolios(model_dict,\n",
    "                            ff_df,\n",
    "                            ret_excess_col=\"ret_excess\",\n",
    "                            pred_col=\"pred_ret_excess\",\n",
    "                            outdir=None):\n",
    "    \"\"\"\n",
    "    Computes EW and VW decile portfolios for all models and\n",
    "    returns objects ready for plotting.\n",
    "\n",
    "    Outputs match existing plotting expectations exactly.\n",
    "    \"\"\"\n",
    "\n",
    "    def _compute(weight):\n",
    "\n",
    "        h_dict, l_dict, hl_dict = {}, {}, {}\n",
    "        long_log, short_log = {}, {}\n",
    "\n",
    "        for model_name, df in model_dict.items():\n",
    "\n",
    "            df = df.copy()\n",
    "            df[\"month\"] = pd.to_datetime(df[\"month\"])\n",
    "\n",
    "            df_dec = assign_deciles(df, pred_col)\n",
    "\n",
    "            monthly = compute_monthly_returns(\n",
    "                df=df_dec,\n",
    "                ret_excess=ret_excess_col,\n",
    "                weight=weight\n",
    "            ).sort_index()\n",
    "\n",
    "            h  = monthly[\"decile_4\"]\n",
    "            l  = monthly[\"decile_1\"]\n",
    "            hl = h - l\n",
    "\n",
    "            h_dict[model_name]  = h\n",
    "            l_dict[model_name]  = l\n",
    "            hl_dict[model_name] = hl\n",
    "\n",
    "            long_log[model_name]  = np.log1p(h).cumsum()\n",
    "            short_log[model_name] = np.log1p(l).cumsum()\n",
    "\n",
    "        H  = pd.DataFrame(h_dict)\n",
    "        L  = pd.DataFrame(l_dict)\n",
    "        HL = pd.DataFrame(hl_dict)\n",
    "\n",
    "        LONG  = pd.DataFrame(long_log)\n",
    "        SHORT = pd.DataFrame(short_log)\n",
    "\n",
    "        if outdir is not None:\n",
    "            H.to_csv(f\"{outdir}/H_{weight}.csv\")\n",
    "            L.to_csv(f\"{outdir}/L_{weight}.csv\")\n",
    "            HL.to_csv(f\"{outdir}/HL_{weight}.csv\")\n",
    "\n",
    "        return LONG, SHORT, H, L, HL\n",
    "\n",
    "    # Equal- and value-weighted\n",
    "    long_eq, short_eq, h_eq, l_eq, hl_eq = _compute(\"equal\")\n",
    "    long_vw, short_vw, h_vw, l_vw, hl_vw = _compute(\"value\")\n",
    "\n",
    "    # SP500 log series (unchanged)\n",
    "    sp = ff_df.copy()\n",
    "    sp[\"month\"] = pd.to_datetime(sp[\"month\"])\n",
    "    sp = sp.set_index(\"month\")[\"sp500_rf\"]\n",
    "\n",
    "    sp_eq = np.log1p(sp).cumsum()\n",
    "    sp_vw = sp_eq.copy()   # identical series, kept separate for clarity\n",
    "\n",
    "    return {\n",
    "        # Plot-ready (EXACT names expected later)\n",
    "        \"long_eq\": long_eq,\n",
    "        \"short_eq\": short_eq,\n",
    "        \"sp_eq\": sp_eq,\n",
    "\n",
    "        \"long_vw\": long_vw,\n",
    "        \"short_vw\": short_vw,\n",
    "        \"sp_vw\": sp_vw,\n",
    "\n",
    "        # Monthly returns (for Sharpe / tests)\n",
    "        \"EW_H\":  h_eq,\n",
    "        \"EW_L\":  l_eq,\n",
    "        \"EW_HL\": hl_eq,\n",
    "\n",
    "        \"VW_H\":  h_vw,\n",
    "        \"VW_L\":  l_vw,\n",
    "        \"VW_HL\": hl_vw\n",
    "    }\n",
    "    \n",
    "def prepare_plot_inputs(model_dict,\n",
    "                        ff_df,\n",
    "                        ret_excess_col=\"ret_excess\",\n",
    "                        pred_col=\"pred_ret_excess\",\n",
    "                        outdir=None):\n",
    "    \"\"\"\n",
    "    Wrapper that prepares all objects used by the plotting code.\n",
    "    \"\"\"\n",
    "\n",
    "    ports = build_decile_portfolios(\n",
    "        model_dict=model_dict,\n",
    "        ff_df=ff_df,\n",
    "        ret_excess_col=ret_excess_col,\n",
    "        pred_col=pred_col,\n",
    "        outdir=\"/Users/jonas/Documents/GitHub/Masters_Thesis_2025E/Project/Jonas/Models\"\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        ports[\"long_vw\"], ports[\"short_vw\"], ports[\"sp_vw\"],\n",
    "        ports[\"long_eq\"], ports[\"short_eq\"], ports[\"sp_eq\"],\n",
    "        ports[\"EW_HL\"], ports[\"VW_HL\"]\n",
    "    )\n",
    "\n",
    "\n",
    "long_vw, short_vw, sp_vw, \\\n",
    "long_eq, short_eq, sp_eq, \\\n",
    "EW_HL, VW_HL = prepare_plot_inputs(\n",
    "    model_dict=model_dict,\n",
    "    ff_df=ff_df,\n",
    "    outdir=None   # or None\n",
    ")\n",
    "\n",
    "# consistent model colors\n",
    "# Model dict for apendix\n",
    "\n",
    "base_color = {\n",
    "    \"OLS3\": \"blue\",\n",
    "    \"PLS\": \"red\",\n",
    "    \"NN3\": \"green\",\n",
    "    \"NN4\": \"purple\",\n",
    "}\n",
    "\n",
    "baseline  = [m for m in model_dict if \"Outsider\" not in m and \"Insider\" not in m]\n",
    "outsiders = [m for m in model_dict if \"Outsider\" in m]\n",
    "insiders  = [m for m in model_dict if \"Insider\"  in m]\n",
    "\n",
    "groups = {\"Baseline\": baseline, \"Outsider\": outsiders, \"Insider\": insiders}\n",
    "\n",
    "# ========================= #\n",
    "# Font control center (change only these)\n",
    "FONT_AXIS_LABEL   = 15\n",
    "FONT_AXIS_TICKS   = 12\n",
    "FONT_TITLE        = 18\n",
    "FONT_COLUMN_LABEL = 14\n",
    "FONT_LEGEND       = 16\n",
    "# ========================= # \n",
    "\n",
    "### Load NBER recession indicator (USREC) ###\n",
    "# Use the min/max dates across your cumulative series\n",
    "start_date = min(long_vw.index.min(), long_eq.index.min())\n",
    "end_date   = max(long_vw.index.max(), long_eq.index.max())\n",
    "\n",
    "recession = web.DataReader(\"USREC\", \"fred\", start_date, end_date)\n",
    "recession.index = pd.to_datetime(recession.index)\n",
    "recession = recession.resample(\"ME\").max()\n",
    "\n",
    "# Identify recession intervals\n",
    "rec_periods = []\n",
    "in_rec = False\n",
    "\n",
    "for date, value in recession[\"USREC\"].items():\n",
    "    if value == 1 and not in_rec:\n",
    "        in_rec = True\n",
    "        start = date\n",
    "    elif value == 0 and in_rec:\n",
    "        in_rec = False\n",
    "        end = date\n",
    "        rec_periods.append((start, end))\n",
    "\n",
    "if in_rec:\n",
    "    rec_periods.append((start, recession.index.max()))\n",
    "\n",
    "\n",
    "### Main figure ###\n",
    "fig, axes = plt.subplots(2, 3, figsize=(26,10), sharey=\"row\")\n",
    "\n",
    "for row,(long_df,short_df,sp_log) in enumerate([\n",
    "    (long_vw, short_vw, sp_vw),\n",
    "    (long_eq, short_eq, sp_eq)\n",
    "]):\n",
    "\n",
    "    for col,(title,models) in enumerate(groups.items()):\n",
    "\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        # Add grey recession shading\n",
    "        for start, end in rec_periods:\n",
    "            ax.axvspan(start, end, color=\"grey\", alpha=0.25, zorder=-1)\n",
    "    \n",
    "        # Plot long / short series\n",
    "        for m in models:\n",
    "            name = m.replace(\" Outsider\",\"\").replace(\" Insider\",\"\")\n",
    "            c = base_color.get(name, \"black\")\n",
    "\n",
    "            ax.plot(long_df[m],  color=c, linestyle=\"-\")\n",
    "            # ax.plot(short_df[m], color=c, linestyle=\"--\") # remove to only show long/short\n",
    "\n",
    "        # SP500\n",
    "        ax.plot(sp_log, color=\"black\", linewidth=2, label=\"SP500\")\n",
    "\n",
    "        # Titles\n",
    "        if row == 0 and col == 1:\n",
    "            ax.set_title(\"Value-weighted\", fontsize=FONT_TITLE)\n",
    "        if row == 1 and col == 1:\n",
    "            ax.set_title(\"Equal-weighted\", fontsize=FONT_TITLE)\n",
    "\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(\"Log cumulative return\", fontsize=FONT_AXIS_LABEL)\n",
    "\n",
    "        ax.tick_params(axis=\"both\", labelsize=FONT_AXIS_TICKS)\n",
    "        ax.grid(True, linewidth=0.7, alpha=0.35)\n",
    "\n",
    "\n",
    "# Bottom x-axis labels\n",
    "axes[1,0].set_xlabel(\"Baseline\", fontsize=FONT_COLUMN_LABEL)\n",
    "axes[1,1].set_xlabel(\"Baseline with Outsider Information\", fontsize=FONT_COLUMN_LABEL)\n",
    "axes[1,2].set_xlabel(\"Baseline with Insider Information\", fontsize=FONT_COLUMN_LABEL)\n",
    "\n",
    "\n",
    "### Legend (incl. SP500) ###\n",
    "handles, labels = [], []\n",
    "\n",
    "for name, color in base_color.items():\n",
    "    handles.append(plt.Line2D([0],[0], color=color, linewidth=2))\n",
    "    labels.append(name)\n",
    "\n",
    "handles.append(plt.Line2D([0],[0], color=\"black\", linewidth=2))\n",
    "labels.append(\"SP500-RF\")\n",
    "\n",
    "# handles.extend([\n",
    "#     plt.Line2D([0],[0], color=\"black\", linestyle=\"-\"),\n",
    "#     plt.Line2D([0],[0], color=\"black\", linestyle=\"--\")\n",
    "# ])\n",
    "# labels.extend([\"long (solid)\", \"short (dashed)\"])\n",
    "\n",
    "fig.legend(handles, labels,\n",
    "           loc=\"upper center\",\n",
    "           bbox_to_anchor=(0.5,0.98),\n",
    "           fontsize=FONT_LEGEND,\n",
    "           ncol=len(labels),\n",
    "           frameon=True)\n",
    "\n",
    "plt.tight_layout(rect=[0,0,1,0.93])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
