{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c65295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q numpy pandas tqdm scikit-learn fastparquet seaborn doubleml tensorflow scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e966a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbdb04e",
   "metadata": {},
   "source": [
    "https://docs.doubleml.org/stable/guide/models.html#partially-linear-models-plm\n",
    "Partially linear regression model (PLR)\n",
    "\n",
    "https://docs.doubleml.org/stable/examples/py_double_ml_cate_plr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d764590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import doubleml as dml\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "# Removed make_pipeline and StandardScaler imports as requested\n",
    "\n",
    "# ======================================================\n",
    "# 1. Setup Paths & Load Feature Lists\n",
    "# ======================================================\n",
    "base_path = \"/work/Thesis/Data/2. Insider\"\n",
    "data_path = \"/work/Thesis/Data/2. Insider/with_insider.parquet\"\n",
    "results_path = \"/work/Thesis/Results/\"\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "print(\"Loading feature lists...\")\n",
    "\n",
    "# A. Load Baseline Controls (X)\n",
    "with open(os.path.join(base_path, \"gkx_cols_ex inter.txt\"), \"r\") as f:\n",
    "    baseline_cols = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# B. Load Insider Treatments (D)\n",
    "with open(os.path.join(base_path, \"insider_cols_ex inter.txt\"), \"r\") as f:\n",
    "    insider_cols = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# C. Define Target (Y)\n",
    "target_col = \"ret_excess\"\n",
    "# D. Define Date Column (CHANGE THIS IF YOUR COLUMN IS NAMED DIFFERENTLY)\n",
    "date_col = \"month\" \n",
    "\n",
    "print(f\"-> Baseline features (Controls): {len(baseline_cols)}\")\n",
    "print(f\"-> Insider features (Treatments): {len(insider_cols)}\")\n",
    "print(f\"-> Target Variable: {target_col}\")\n",
    "\n",
    "# ======================================================\n",
    "# 2. Load Data & Filter First 8 Years\n",
    "# ======================================================\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "# 2a. Sanity Checks\n",
    "if target_col not in df.columns:\n",
    "    raise ValueError(f\"CRITICAL ERROR: Target column '{target_col}' not found!\")\n",
    "if date_col not in df.columns:\n",
    "    raise ValueError(f\"CRITICAL ERROR: Date column '{date_col}' not found! Update 'date_col' variable.\")\n",
    "\n",
    "# 2b. Filter for the First 8 Years\n",
    "print(\"Filtering data to the first 8 years...\")\n",
    "\n",
    "# Ensure date format\n",
    "df[date_col] = pd.to_datetime(df[date_col])\n",
    "df = df.sort_values(date_col)\n",
    "\n",
    "start_date = df[date_col].min()\n",
    "cutoff_date = start_date + pd.DateOffset(years=8)\n",
    "\n",
    "# Slice the dataframe\n",
    "df = df[df[date_col] < cutoff_date].copy()\n",
    "\n",
    "print(f\"-> Date Range: {start_date.date()} to {cutoff_date.date()}\")\n",
    "print(f\"-> Data ready: {len(df)} rows.\")\n",
    "\n",
    "# 2c. Drop NaNs (DoubleML will crash if there are NaNs)\n",
    "cols_to_check = [target_col] + baseline_cols\n",
    "df = df.dropna(subset=cols_to_check).reset_index(drop=True)\n",
    "\n",
    "# ======================================================\n",
    "# 3. Define the Learners (Neural Networks)\n",
    "# ======================================================\n",
    "# Removed StandardScaler as requested.\n",
    "# Using raw Neural Network.\n",
    "learner_nn = MLPRegressor(\n",
    "    hidden_layer_sizes=(64), \n",
    "    activation='relu', \n",
    "    solver='adam', \n",
    "    alpha=0.0001, \n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=60, \n",
    "    early_stopping=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ======================================================\n",
    "# 4. Run the DoubleML Horse Race\n",
    "# ======================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"STARTING AUTO-DML HORSE RACE ({len(insider_cols)} Signals)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Signal':<40} | {'Coef':<10} | {'t-stat':<10} | {'P-value':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for treatment_var in insider_cols:\n",
    "    \n",
    "    # 1. Check if treatment exists in DF\n",
    "    if treatment_var not in df.columns:\n",
    "        print(f\"Skipping {treatment_var:<40} (Not found)\")\n",
    "        continue\n",
    "        \n",
    "    # 2. Drop NaNs specific to this treatment\n",
    "    df_sub = df.dropna(subset=[treatment_var])\n",
    "    \n",
    "    if len(df_sub) < 500: # Lowered threshold slightly since we cut sample size\n",
    "        print(f\"Skipping {treatment_var:<40} (Too sparse: {len(df_sub)})\")\n",
    "        continue\n",
    "\n",
    "    # 3. Initialize DoubleML Data Object\n",
    "    dml_data = dml.DoubleMLData(\n",
    "        df_sub,\n",
    "        y_col=target_col,\n",
    "        d_cols=treatment_var,\n",
    "        x_cols=baseline_cols\n",
    "    )\n",
    "    \n",
    "    # 4. Initialize Model (Partially Linear Regression)\n",
    "    dml_plr = dml.DoubleMLPLR(\n",
    "        dml_data,\n",
    "        ml_l=learner_nn, \n",
    "        ml_m=learner_nn, \n",
    "        n_folds=5,       \n",
    "        score='partialling out'\n",
    "    )\n",
    "    \n",
    "    # 5. Fit & Extract\n",
    "    try:\n",
    "        dml_plr.fit()\n",
    "        \n",
    "        summary = dml_plr.summary\n",
    "        coef = summary.loc[treatment_var, \"coef\"]\n",
    "        pval = summary.loc[treatment_var, \"P>|t|\"]\n",
    "        tstat = summary.loc[treatment_var, \"t\"]\n",
    "        \n",
    "        # Live Print\n",
    "        print(f\"{treatment_var:<40} | {coef:.4f}     | {tstat:.4f}     | {pval:.4f}\")\n",
    "        \n",
    "        # Store\n",
    "        results_list.append({\n",
    "            \"Signal\": treatment_var,\n",
    "            \"Coefficient\": coef,\n",
    "            \"t_stat\": tstat,\n",
    "            \"P_value\": pval,\n",
    "            \"Significant\": pval < 0.05\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting {treatment_var}: {e}\")\n",
    "\n",
    "# ======================================================\n",
    "# 5. Save Final Results\n",
    "# ======================================================\n",
    "if results_list:\n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    # Sort by significance (highest absolute t-stat)\n",
    "    df_results = df_results.sort_values(\"t_stat\", ascending=False, key=abs)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL DML RESULTS (Sorted by Significance)\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_results.to_string(index=False))\n",
    "\n",
    "    # Save to CSV\n",
    "    out_file = os.path.join(results_path, \"dml_insider_inference_8years.csv\")\n",
    "    df_results.to_csv(out_file, index=False)\n",
    "    print(f\"\\nSaved detailed results to: {out_file}\")\n",
    "else:\n",
    "    print(\"\\nNo results generated. Check column names or data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746e9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import doubleml as dml\n",
    "\n",
    "# Import Keras components\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L1\n",
    "# Note: You need 'scikeras' installed: pip install scikeras\n",
    "from scikeras.wrappers import KerasRegressor \n",
    "\n",
    "# ======================================================\n",
    "# 0. CPU / Threading Configuration (MAX SPEED)\n",
    "# ======================================================\n",
    "# Set environment variables BEFORE initializing TensorFlow\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"96\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"96\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"96\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"96\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"96\"\n",
    "# TensorFlow specific\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"96\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"8\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # Force CPU\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Configure TensorFlow threading\n",
    "tf.config.threading.set_intra_op_parallelism_threads(96)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(8)\n",
    "\n",
    "print(f\"CPU Threading Configured: 96 Intra-op / 8 Inter-op threads.\")\n",
    "\n",
    "# ======================================================\n",
    "# 1. Setup Paths & Load Feature Lists\n",
    "# ======================================================\n",
    "base_path = \"/work/Thesis/Data/1. Pre Variable Selection/2. Insider\"\n",
    "data_path = \"/work/Thesis/Data/1. Pre Variable Selection/2. Insider/with_insider.parquet\"\n",
    "results_path = \"/work/Thesis/Results/\"\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "print(\"Loading feature lists...\")\n",
    "\n",
    "with open(os.path.join(base_path, \"gkx_cols_ex inter.txt\"), \"r\") as f:\n",
    "    baseline_cols = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "with open(os.path.join(base_path, \"insider_cols_ex inter.txt\"), \"r\") as f:\n",
    "    insider_cols = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "target_col = \"ret_excess\"\n",
    "date_col = \"month\"\n",
    "\n",
    "print(f\"-> Baseline features (Controls): {len(baseline_cols)}\")\n",
    "print(f\"-> Insider features (Treatments): {len(insider_cols)}\")\n",
    "\n",
    "# ======================================================\n",
    "# 2. Load Data & Filter First 8 Years\n",
    "# ======================================================\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "# Filter for First 8 Years\n",
    "df[date_col] = pd.to_datetime(df[date_col])\n",
    "df = df.sort_values(date_col)\n",
    "\n",
    "start_date = df[date_col].min()\n",
    "cutoff_date = start_date + pd.DateOffset(years=8)\n",
    "df = df[df[date_col] < cutoff_date].copy()\n",
    "\n",
    "print(f\"-> Date Range: {start_date.date()} to {cutoff_date.date()}\")\n",
    "print(f\"-> Data ready: {len(df)} rows.\")\n",
    "\n",
    "# Drop NaNs for Baseline/Target\n",
    "cols_to_check = [target_col] + baseline_cols\n",
    "df = df.dropna(subset=cols_to_check).reset_index(drop=True)\n",
    "\n",
    "# ======================================================\n",
    "# 3. Define the Keras Model (With L1 & Batch Norm)\n",
    "# ======================================================\n",
    "def create_keras_model(input_dim, l1_reg=1e-4, lr=0.001):\n",
    "    model = Sequential()\n",
    "    # Layer 1\n",
    "    model.add(Dense(64, activation='relu', input_shape=(input_dim,), \n",
    "                    kernel_regularizer=L1(l1_reg), use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 2\n",
    "    model.add(Dense(32, activation='relu', \n",
    "                    kernel_regularizer=L1(l1_reg), use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Output\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Setup wrapper with Early Stopping\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Initialize the Learner\n",
    "# Note: KerasRegressor will handle the 'input_dim' automatically if not passed, \n",
    "# but passing it explicitly via arguments ensures clarity.\n",
    "input_dim = len(baseline_cols)\n",
    "\n",
    "learner_nn = KerasRegressor(\n",
    "    model=create_keras_model,\n",
    "    model__input_dim=input_dim,\n",
    "    model__l1_reg=0.0001,\n",
    "    model__lr=0.001,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    verbose=0,\n",
    "    callbacks=[early_stop],\n",
    "    validation_split=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Neural Network Learner initialized (L1=0.0001, LR=0.001)\")\n",
    "\n",
    "# ======================================================\n",
    "# 4. Run the DoubleML Horse Race\n",
    "# ======================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"STARTING AUTO-DML HORSE RACE ({len(insider_cols)} Signals)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Signal':<40} | {'Coef':<10} | {'t-stat':<10} | {'P-value':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for treatment_var in insider_cols:\n",
    "    \n",
    "    if treatment_var not in df.columns:\n",
    "        continue\n",
    "        \n",
    "    # Drop NaNs specific to this treatment\n",
    "    df_sub = df.dropna(subset=[treatment_var])\n",
    "    \n",
    "    if len(df_sub) < 500:\n",
    "        continue\n",
    "\n",
    "    # Initialize DoubleML Data\n",
    "    dml_data = dml.DoubleMLData(\n",
    "        df_sub,\n",
    "        y_col=target_col,\n",
    "        d_cols=treatment_var,\n",
    "        x_cols=baseline_cols\n",
    "    )\n",
    "    \n",
    "    # Initialize PLR Model\n",
    "    # Note: DoubleML will handle parallelization of cross-fitting folds internally\n",
    "    # if supported, but typically runs sequential for DL models to avoid VRAM/Resource conflicts.\n",
    "    # Since we set CPU threads to 96, TF will use all cores for EACH model training.\n",
    "    dml_plr = dml.DoubleMLPLR(\n",
    "        dml_data,\n",
    "        ml_l=learner_nn, \n",
    "        ml_m=learner_nn, \n",
    "        n_folds=5,      \n",
    "        score='partialling out'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        dml_plr.fit()\n",
    "        \n",
    "        summary = dml_plr.summary\n",
    "        coef = summary.loc[treatment_var, \"coef\"]\n",
    "        pval = summary.loc[treatment_var, \"P>|t|\"]\n",
    "        tstat = summary.loc[treatment_var, \"t\"]\n",
    "        \n",
    "        print(f\"{treatment_var:<40} | {coef:.4f}     | {tstat:.4f}     | {pval:.4f}\")\n",
    "        \n",
    "        results_list.append({\n",
    "            \"Signal\": treatment_var,\n",
    "            \"Coefficient\": coef,\n",
    "            \"t_stat\": tstat,\n",
    "            \"P_value\": pval,\n",
    "            \"Significant\": pval < 0.05\n",
    "        })\n",
    "        \n",
    "        # Explicit garbage collection to free memory between iterations\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting {treatment_var}: {e}\")\n",
    "\n",
    "# ======================================================\n",
    "# 5. Save Final Results\n",
    "# ======================================================\n",
    "if results_list:\n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    df_results = df_results.sort_values(\"t_stat\", ascending=False, key=abs)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL DML RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_results.to_string(index=False))\n",
    "\n",
    "    out_file = os.path.join(results_path, \"dml_insider_inference_8years.csv\")\n",
    "    df_results.to_csv(out_file, index=False)\n",
    "    print(f\"\\nSaved results to: {out_file}\")\n",
    "else:\n",
    "    print(\"\\nNo results generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
