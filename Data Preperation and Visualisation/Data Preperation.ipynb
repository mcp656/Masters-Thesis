{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e664a54",
   "metadata": {},
   "source": [
    "**Modules**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691dc73c",
   "metadata": {},
   "source": [
    "To run this code, please ensure that you use python 3.10.11 and the tidy-finance environment installed:\n",
    "https://www.tidy-finance.org/python/setting-up-your-environment.html\n",
    "\n",
    "The code runs beautifully using python 3.13.5, but newer versions of python may not work, as pandas has a tendency to deprecate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8908cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Installs that my not be present in your environment\n",
    "# !pip install Pathlib\n",
    "# !pip install sqlite3\n",
    "# !pip install re\n",
    "# !pip install pandas-market-calendars\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import sqlite3\n",
    "import requests\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Path to repository\n",
    "sys.path.append(\"/Users/jonas/Documents/GitHub/Masters_Thesis_2025E\")\n",
    "\n",
    "# Import functions from helpers.py\n",
    "from Project.Jonas.Models.helpers import convert_formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7bee40",
   "metadata": {},
   "source": [
    "Set start and end dates for preprocessing. This is important as ranking depends on the time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d37b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set start and end date\n",
    "START_DATE = \"2005-12-31\"\n",
    "END_DATE = \"2021-12-31\"\n",
    "\n",
    "# Set epoch start\n",
    "EPOCH_START = pd.Timestamp(\"1970-01-01\") # Do not change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6f797e",
   "metadata": {},
   "source": [
    "**Retrieving Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60e7bf8",
   "metadata": {},
   "source": [
    "Download stock-characteristics from Xiu's website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "373dd0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Retrieve data from Xiu's website ###\n",
    "\n",
    "# # Download the ZIP file\n",
    "# url = \"https://dachxiu.chicagobooth.edu/download/datashare.zip\"\n",
    "# response = requests.get(url)\n",
    "\n",
    "# with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "#     with z.open(\"datashare.csv\") as f:\n",
    "#         characteristics = pd.read_csv(f)\n",
    "\n",
    "# # Save the CSV locally\n",
    "# characteristics.to_csv(\"datashare.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "183b8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "### If the file is already downloaded, read it directly from the local file system ###\n",
    "path = \"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/Factors Xiu/datashare.csv\" # Update this path to where you saved the file\n",
    "\n",
    "if os.path.exists(path):\n",
    "    characteristics = pd.read_csv(path)\n",
    "else:\n",
    "    print(\"No path found\")\n",
    "\n",
    "# # Print the column names and formats to verify\n",
    "# print(characteristics.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8565d6",
   "metadata": {},
   "source": [
    "Download CRSP data and macro from the SQLite database provided by Voigt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ef113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Download and process tables from the SQLite database ###\n",
    "# tidy_finance = sqlite3.connect(\n",
    "#     database=\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/SQLite/tidy_finance_r.sqlite\"\n",
    "# )\n",
    "\n",
    "# # Read tables into pandas DataFrames\n",
    "# tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table';\", tidy_finance) \n",
    "\n",
    "# # # print the table names\n",
    "# # print(tables)\n",
    "\n",
    "# # Select tables to export\n",
    "# selected_tables = [\"crsp_monthly\", \"macro_predictors\"]\n",
    "\n",
    "# # Define absolute output folder path\n",
    "# output_folder = \"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/SQLite/Tables\" # Update this path to your desired output folder\n",
    "\n",
    "# # Create output folder if it doesn't exist\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # Export each table to parquet format\n",
    "# for table_name in selected_tables:\n",
    "#     df = pd.read_sql(f\"SELECT * FROM {table_name};\", tidy_finance)\n",
    "#     df.to_parquet(os.path.join(output_folder, f\"{table_name}.parquet\"), index=False)\n",
    "#     print(f\"Exported: {table_name}\")\n",
    "\n",
    "# # Close the connection\n",
    "# tidy_finance.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1e1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### If the tables are already downloaded and processed, read them directly from the local file system ###\n",
    "path = \"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/SQLite/Tables\" # Update this path to where you saved the files\n",
    "\n",
    "if os.path.exists(path):\n",
    "    crsp_monthly = pd.read_parquet(os.path.join(path, \"crsp_monthly.parquet\"))\n",
    "    macro_predictors = pd.read_parquet(os.path.join(path, \"macro_predictors.parquet\"))\n",
    "else:\n",
    "    print(\"No path found\")\n",
    "    \n",
    "# # Print the column names and formats to verify\n",
    "# print(crsp_monthly.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effbeab6",
   "metadata": {},
   "source": [
    "Download linking tables from Ding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b3c5797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Download and process linking tables from GitHub repository ###\n",
    "# # Base URL for the GitHub repository\n",
    "# base_url = \"https://github.com/Wenzhi-Ding/Std_Security_Code/raw/main/other/\"\n",
    "\n",
    "# # List of linking tables to download\n",
    "# linking_tables = [\n",
    "#     \"cik_gvkey.pq\",\n",
    "#     \"cik_ticker.pq\",\n",
    "#     \"gvkey_permco_permno.pq\"\n",
    "# ]\n",
    "\n",
    "# # Local folder to store linking tables\n",
    "# output_folder = \"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/Linking Tables/Link to link\" # Update this path to your desired output folder\n",
    "\n",
    "# # Create output folder if it doesn't exist\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # Download, load, and export each file\n",
    "# for link in linking_tables:\n",
    "#     url = base_url + link\n",
    "#     save_path = os.path.join(output_folder, link)\n",
    "    \n",
    "#     # Download .pq file\n",
    "#     response = requests.get(url)\n",
    "#     with open(save_path, \"wb\") as f:\n",
    "#         f.write(response.content)\n",
    "#     print(f\"Downloaded: {link} -> {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dfdf69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### If the linking tables are already downloaded and processed, read them directly from the local file system ###\n",
    "path = \"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/Linking Tables/Link to link\" # Update this path to where you saved the files\n",
    "\n",
    "if os.path.exists(path):\n",
    "    cik_gvkey = convert_formats(pd.read_parquet(os.path.join(path, \"cik_gvkey.pq\"))) # Call convert_formats as there is misalignment between formats in the files - used here only as need for precision is not as great as for the others\n",
    "    cik_ticker = convert_formats(pd.read_parquet(os.path.join(path, \"cik_ticker.pq\")))\n",
    "    gvkey_permco_permno = convert_formats(pd.read_parquet(os.path.join(path, \"gvkey_permco_permno.pq\")))\n",
    "else:\n",
    "    print(\"No path found\")\n",
    "\n",
    "# # Print the column names and formats to verify\n",
    "# print(cik_gvkey.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc7c4d",
   "metadata": {},
   "source": [
    "Download insider trades from SEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### SEC Data Download and Preperation ###\n",
    "\n",
    "# # Paths\n",
    "# zip_path = Path(r\"C:\\Users\\ufuky\\OneDrive - University of Copenhagen\\Jonas Theodor Østerby Schmidt's files - Kandidat\\Thesis\\2.0 Data\\Insider Trading\\zip\")   # folder with ZIP files\n",
    "# out_path = Path(r\"C:\\Users\\ufuky\\OneDrive - University of Copenhagen\\Jonas Theodor Østerby Schmidt's files - Kandidat\\Thesis\\2.0 Data\\Insider Trading\\Insider Transaction Data 2021-2006\")\n",
    "\n",
    "# out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# print(f\"Looking for ZIP files in: {zip_path.resolve()}\")\n",
    "\n",
    "# # Loop through all ZIP files\n",
    "# zip_files = list(zip_path.glob(\"*.zip\"))\n",
    "# print(f\"Found {len(zip_files)} ZIP files\")\n",
    "\n",
    "# for zip_file in zip_files:\n",
    "#     print(f\"\\nProcessing {zip_file.name}...\")\n",
    "\n",
    "#     try:\n",
    "#         # Extract year + quarter from filename like \"2010q1_form345\"\n",
    "#         match = re.search(r\"(\\d{4})q([1-4])\", zip_file.stem.lower())\n",
    "#         if not match:\n",
    "#             print(f\"Could not parse year/quarter from: {zip_file.stem}\")\n",
    "#             continue\n",
    "\n",
    "#         year, quarter = match.groups()\n",
    "#         quarter_code = f\"{year[-2:]}Q{quarter}\"  # e.g. 2010q1 -> 10Q1\n",
    "\n",
    "#         with zipfile.ZipFile(zip_file, \"r\") as z:\n",
    "#             members = z.namelist()\n",
    "#             print(f\"  Files inside ZIP: {members}\")\n",
    "\n",
    "#             for member in members:\n",
    "#                 # Match the three relevant files\n",
    "#                 if member.endswith(\"SUBMISSION.tsv\"):\n",
    "#                     new_name = f\"{quarter_code}_SUBMISSION.tsv\"\n",
    "#                 elif member.endswith(\"REPORTINGOWNER.tsv\"):\n",
    "#                     new_name = f\"{quarter_code}_REPORTINGOWNER.tsv\"\n",
    "#                 elif member.endswith(\"NONDERIV_TRANS.tsv\"):\n",
    "#                     new_name = f\"{quarter_code}_NONDERIV_TRANS.tsv\"\n",
    "#                 else:\n",
    "#                     continue\n",
    "\n",
    "#                 # Extract file contents into a DataFrame\n",
    "#                 with z.open(member) as f:\n",
    "#                     df = pd.read_csv(f, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "#                 # Save with renamed file\n",
    "#                 save_path = out_path / new_name\n",
    "#                 df.to_csv(save_path, sep=\"\\t\", index=False)\n",
    "#                 print(f\" Saved {save_path.name} ({len(df)} rows)\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {zip_file.name}: {e}\")\n",
    "        \n",
    "# base_path = Path(\n",
    "#     r\"C:\\Users\\ufuky\\OneDrive - University of Copenhagen\\Jonas Theodor Østerby Schmidt's files - Kandidat\\Thesis\\2.0 Data\\Insider Trading\\Insider Transaction Data 2021-2006\"\n",
    "# )\n",
    "\n",
    "# # Columns to keep\n",
    "# keep_cols = [\n",
    "#     \"ACCESSION_NUMBER\",\n",
    "#     \"PERIOD_OF_REPORT\",\n",
    "#     \"FILING_DATE\",\n",
    "#     \"ISSUERCIK\",\n",
    "#     \"ISSUERTRADINGSYMBOL\",\n",
    "#     \"RPTOWNERCIK\",\n",
    "#     \"RPTOWNERNAME\",\n",
    "#     \"RPTOWNER_RELATIONSHIP\",\n",
    "#     \"RPTOWNER_TITLE\",\n",
    "#     \"RPTOWNER_RELATIONSHIP\",\n",
    "#     \"RPTOWNER_STATE\",\n",
    "#     \"SECURITY_TITLE\",\n",
    "#     \"TRANS_DATE\",\n",
    "#     \"TRANS_CODE\",\n",
    "#     \"TRANS_ACQUIRED_DISP_CD\",\n",
    "#     \"TRANS_SHARES\",\n",
    "#     \"TRANS_PRICEPERSHARE\",\n",
    "#     \"SHRS_OWND_FOLWNG_TRANS\",\n",
    "# ]\n",
    "\n",
    "# def process_quarter(year, quarter):\n",
    "#     \"\"\"Load, merge, and clean insider trades for one quarter.\"\"\"\n",
    "#     try:\n",
    "#         # Load files\n",
    "#         df_sub = pd.read_csv(base_path / f\"{str(year)[-2:]}Q{quarter}_SUBMISSION.tsv\", sep=\"\\t\")\n",
    "#         df_rep = pd.read_csv(base_path / f\"{str(year)[-2:]}Q{quarter}_REPORTINGOWNER.tsv\", sep=\"\\t\")\n",
    "#         df_trans = pd.read_csv(base_path / f\"{str(year)[-2:]}Q{quarter}_NONDERIV_TRANS.tsv\", sep=\"\\t\")\n",
    "\n",
    "#         # Merge\n",
    "#         df_tmp = pd.merge(df_sub, df_rep, on=\"ACCESSION_NUMBER\", how=\"inner\")\n",
    "#         df_merged = pd.merge(df_tmp, df_trans, on=\"ACCESSION_NUMBER\", how=\"inner\")\n",
    "\n",
    "#         # Keep only selected columns\n",
    "#         df_clean = df_merged[df_merged.columns.intersection(keep_cols)].copy()\n",
    "\n",
    "#         # Add transaction value\n",
    "#         df_clean[\"TOTAL\"] = (\n",
    "#             pd.to_numeric(df_clean[\"TRANS_SHARES\"], errors=\"coerce\") *\n",
    "#             pd.to_numeric(df_clean[\"TRANS_PRICEPERSHARE\"], errors=\"coerce\")\n",
    "#         )\n",
    "\n",
    "#         # Filter by P and S only\n",
    "#         df_clean = df_clean[df_clean[\"TRANS_CODE\"].isin([\"P\", \"S\"])]\n",
    "\n",
    "#         # Remove mixed P/S accessions\n",
    "#         check_codes = (\n",
    "#             df_clean.groupby(\"ACCESSION_NUMBER\")[\"TRANS_CODE\"]\n",
    "#             .nunique()\n",
    "#             .reset_index(name=\"unique_trans_codes\")\n",
    "#         )\n",
    "#         bad_accessions = check_codes[check_codes[\"unique_trans_codes\"] > 1][\"ACCESSION_NUMBER\"]\n",
    "\n",
    "#         df_uniform = df_clean[~df_clean[\"ACCESSION_NUMBER\"].isin(bad_accessions)].copy()\n",
    "\n",
    "#         print(f\"{year}Q{quarter}: {len(df_uniform)} rows kept\")\n",
    "#         return df_uniform\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Missing file for {year}Q{quarter}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "# # Collect everything into one big DataFrame\n",
    "# dfs_all = []\n",
    "\n",
    "# for year in range(2006, 2022):  # inclusive 2021\n",
    "#     for quarter in range(1, 5):\n",
    "#         df_q = process_quarter(year, quarter)\n",
    "#         if not df_q.empty:\n",
    "#             dfs_all.append(df_q)\n",
    "\n",
    "# # Save one combined file\n",
    "# if dfs_all:\n",
    "#     df_all = pd.concat(dfs_all, ignore_index=True)\n",
    "#     out_file_all = base_path / \"trades_2006-2021.txt\"\n",
    "#     df_all.to_csv(out_file_all, sep=\"\\t\", index=False)\n",
    "#     print(f\"Saved {len(df_all)} total rows across 2006–2021 → {out_file_all}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40d37231",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The insider trades have been downloaded, see seperate file ###\n",
    "path = \"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/Insider Trading/Insider Transaction Data 2021-2006/trades_2006-2021.txt\"\n",
    "\n",
    "if os.path.exists(path):\n",
    "    insider_trades = pd.read_csv(path, sep=\"\\t\")\n",
    "else: \n",
    "    print(\"No path found\")\n",
    "    \n",
    "# # Print the column names and formats to verify\n",
    "# print(insider_trades.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c9a25",
   "metadata": {},
   "source": [
    "**Preparing GKX**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5a934",
   "metadata": {},
   "source": [
    "Prepare stock characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c79cff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare stock characteristics ###\n",
    "\n",
    "# Print the first couple of rows\n",
    "# # print(characteristics.head())\n",
    "\n",
    "# Rename DATE to month\n",
    "characteristics = characteristics.rename(columns={\"DATE\": \"month\"})\n",
    "\n",
    "# Convert to datetime\n",
    "characteristics[\"month\"] = (\n",
    "    pd.to_datetime(characteristics[\"month\"].astype(str), format=\"%Y%m%d\", errors=\"coerce\")\n",
    "      .dt.to_period(\"M\")\n",
    "      .dt.to_timestamp(\"M\")\n",
    ")\n",
    "\n",
    "# Change period\n",
    "characteristics = characteristics[(characteristics[\"month\"] >= pd.to_datetime(START_DATE)) & (characteristics[\"month\"] <= pd.to_datetime(END_DATE))]\n",
    "        \n",
    "# For each characteristic change name to char_value\n",
    "characteristics = characteristics.rename(columns=lambda x: \"char_\" + x if x not in [\"permno\", \"month\", \"sic2\"] else x)\n",
    "\n",
    "# Function to impute missing sic2 values\n",
    "def impute_sic2(df):\n",
    "    \n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by permno and month\n",
    "    df = df.sort_values([\"permno\", \"month\"])\n",
    "\n",
    "    # Backfill sic2 within each permno\n",
    "    df[\"sic2_bfill\"] = df.groupby(\"permno\")[\"sic2\"].bfill()\n",
    "\n",
    "    # Compute sic2 mode per permno (or 0 if all missing)\n",
    "    sic2_final = (\n",
    "        df.groupby(\"permno\")[\"sic2_bfill\"]\n",
    "        .agg(lambda x: x.mode().iloc[0] if not x.dropna().empty else 0)\n",
    "    )\n",
    "\n",
    "    # Fill all missing sic2 values using the per-permno mapping\n",
    "    df[\"sic2\"] = df[\"sic2\"].fillna(df[\"permno\"].map(sic2_final))\n",
    "\n",
    "    # Clean up helper column\n",
    "    df = df.drop(columns=[\"sic2_bfill\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the imputation function \n",
    "characteristics = impute_sic2(characteristics)\n",
    "\n",
    "# # Display the first couple of rows\n",
    "# display(characteristics.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca4888",
   "metadata": {},
   "source": [
    "Prepare CRSP monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29828451",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare CRSP monthly data ###\n",
    "\n",
    "# # Print the first couple of rows\n",
    "# print(crsp_monthly.head())\n",
    "\n",
    "# Rename date to month\n",
    "crsp_monthly = crsp_monthly.rename(columns={\"date\": \"month\"})\n",
    "\n",
    "# Convert date type, and set to month to months end\n",
    "crsp_monthly[\"month\"] = EPOCH_START + pd.to_timedelta(crsp_monthly[\"month\"], unit=\"D\")\n",
    "crsp_monthly[\"month\"] = crsp_monthly[\"month\"].dt.to_period(\"M\").dt.to_timestamp(\"M\")\n",
    "\n",
    "# Change period\n",
    "crsp_monthly = crsp_monthly[(crsp_monthly[\"month\"] >= pd.to_datetime(START_DATE)) & (crsp_monthly[\"month\"] <= pd.to_datetime(END_DATE))]\n",
    "\n",
    "# Only use selected columns\n",
    "crsp_monthly = crsp_monthly[[\"month\", \"permno\", \"ret_excess\", \"prc\", \"shrout\", \"mktcap\", \"mktcap_lag\"]]\n",
    "\n",
    "# # Display the first couple of rows\n",
    "# display(crsp_monthly.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6950b6",
   "metadata": {},
   "source": [
    "Prepare macropredictors by Goyal.\n",
    "\n",
    "Download raw predictors via: https://docs.google.com/spreadsheets/d/1bM7vCWd3WOt95Sf9qjLPZjoiafgF_8EG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d090634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare macropredictors data ###\n",
    "\n",
    "# # Print the first couple of rows\n",
    "# print(macropredictors.head())\n",
    "\n",
    "# Rename date to month\n",
    "macro_predictors = macro_predictors.rename(columns={\"date\": \"month\"})\n",
    "\n",
    "# Convert date type\n",
    "macro_predictors[\"month\"] = EPOCH_START + pd.to_timedelta(macro_predictors[\"month\"], unit=\"D\")\n",
    "macro_predictors[\"month\"] = macro_predictors[\"month\"].dt.to_period(\"M\").dt.to_timestamp(\"M\")\n",
    "\n",
    "# Lag one month\n",
    "macro_predictors[\"month\"] = macro_predictors[\"month\"] + pd.offsets.MonthEnd(1)\n",
    "\n",
    "# Change period\n",
    "macro_predictors = macro_predictors[(macro_predictors[\"month\"] >= pd.to_datetime(START_DATE)) & (macro_predictors[\"month\"] <= pd.to_datetime(END_DATE))]\n",
    "\n",
    "# Only use selected columns\n",
    "macro_predictors = macro_predictors[[\"month\", \"dp\", \"ep\", \"bm\", \"ntis\", \"tbl\", \"tms\", \"dfy\", \"svar\"]]\n",
    "\n",
    "# For each predictor change name to macro_predictor\n",
    "macro_predictors = macro_predictors.rename(columns=lambda x: f\"macro_{x}\" if x != \"month\" else x)\n",
    "\n",
    "# # Display the first couple of rows\n",
    "# display(macro_predictors.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8bf33d",
   "metadata": {},
   "source": [
    "Link tables: cik → permno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30e5ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get all cik_gvkeys where sec_end_date is larger than December 31, 2005 and sec_start_date is smaller than January 31, 2021 ###\n",
    "\n",
    "# Ensure datetime format for sec_start_date and sec_end_date\n",
    "cik_gvkey[\"sec_start_date\"] = (pd.to_datetime(cik_gvkey[\"sec_start_date\"], format=\"%Y%m%d\", errors=\"coerce\"))\n",
    "cik_gvkey[\"sec_end_date\"] = (pd.to_datetime(cik_gvkey[\"sec_end_date\"], format=\"%Y%m%d\", errors=\"coerce\"))\n",
    "\n",
    "# Print \"Not all sec_x_date have values\" if there are any NA values in sec_start_date or sec_end_date\n",
    "assert cik_gvkey[\"sec_start_date\"].isna().sum() == 0, \"Not all sec_start_date have values\"\n",
    "assert cik_gvkey[\"sec_end_date\"].isna().sum() == 0, \"Not all sec_end_date have values\"\n",
    "\n",
    "# Filter cik_gvkey for sec_end_date larger than December 31, 2005 and sec_start_date smaller than January 31, 2021\n",
    "cik_gvkey = cik_gvkey[cik_gvkey[\"sec_start_date\"] <= pd.to_datetime(END_DATE)]\n",
    "cik_gvkey = cik_gvkey[cik_gvkey[\"sec_end_date\"] >= pd.to_datetime(START_DATE)].reset_index(drop=True)\n",
    "\n",
    "### Get all gvkey_permco_permno where linkdt is smaller than January 31, 2021 - Issues with linkenddt  ###\n",
    "\n",
    "# Ensure datetime format for linkdt\n",
    "gvkey_permco_permno[\"linkdt\"] = pd.to_datetime(\n",
    "    gvkey_permco_permno[\"linkdt\"].astype(\"Int64\").astype(str),\n",
    "    format=\"%Y%m%d\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Print if there are any NA values in linkdt\n",
    "assert gvkey_permco_permno[\"linkdt\"].isna().sum() == 0, \"Not all linkdt have values\"\n",
    "\n",
    "# Filter gvkey_permco_permno for linkdt smaller than January 31, 2021\n",
    "gvkey_permco_permno = gvkey_permco_permno[gvkey_permco_permno[\"linkdt\"] <= pd.to_datetime(END_DATE)].reset_index(drop=True)\n",
    "\n",
    "### Merge cik_gvkey with gvkey_permco_permno to get cik_permno ###\n",
    "\n",
    "# Merge on gvkey and keep only relevant columns\n",
    "cik_permno_linking_table = (\n",
    "    pd.merge(cik_gvkey, gvkey_permco_permno, on=\"gvkey\", how=\"inner\")\n",
    "      .loc[:, [\"cik\", \"lpermno\"]]\n",
    "      .rename(columns={\"lpermno\": \"permno\"})\n",
    "      .dropna(subset=[\"permno\"])\n",
    "      .drop_duplicates()\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "### Remove permnos with multiple ciks and vice versa to ensure one-to-one mapping ###\n",
    "\n",
    "# Count unique ciks per permno and vice versa\n",
    "unique_cik_per_permno = cik_permno_linking_table.groupby(\"permno\")[\"cik\"].nunique()\n",
    "unique_permno_per_cik = cik_permno_linking_table.groupby(\"cik\")[\"permno\"].nunique()\n",
    "\n",
    "# Find ciks and permnos where there are multiple ciks per permno and vice versa\n",
    "ambiguous_permnos = unique_cik_per_permno[unique_cik_per_permno > 1].index\n",
    "ambiguous_ciks = unique_permno_per_cik[unique_permno_per_cik > 1].index\n",
    "\n",
    "# Filter out ambiguous ciks and permnos\n",
    "cik_permno_linking_table = cik_permno_linking_table[~cik_permno_linking_table[\"permno\"].isin(ambiguous_permnos)]\n",
    "cik_permno_linking_table = cik_permno_linking_table[~cik_permno_linking_table[\"cik\"].isin(ambiguous_ciks)]\n",
    "\n",
    "# Reset index of cik_permno_linking_table\n",
    "cik_permno_linking_table = cik_permno_linking_table.reset_index(drop=True)\n",
    "\n",
    "# # Finalized cik_permno linking table\n",
    "# display(cik_permno_linking_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1abcca05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cik_permno linking table has 12977 unique cik_permno pairs.\n"
     ]
    }
   ],
   "source": [
    "# Print report\n",
    "print(f\"Final cik_permno linking table has {cik_permno_linking_table.shape[0]} unique cik_permno pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1875bcea",
   "metadata": {},
   "source": [
    "**Merging GKX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbd0ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert permno for characteristics and crsp_monthly to the same format ###\n",
    "# characteristics\n",
    "characteristics[\"permno\"] = characteristics[\"permno\"].astype(str).str.lstrip(\"0\")\n",
    "characteristics[\"permno\"] = pd.to_numeric(characteristics[\"permno\"], errors=\"coerce\").astype(\"Int32\") # Use \"Int32\" to allow for NA values\n",
    "\n",
    "# crsp_monthly\n",
    "crsp_monthly[\"permno\"] = crsp_monthly[\"permno\"].astype(str).str.lstrip(\"0\")\n",
    "crsp_monthly[\"permno\"] = pd.to_numeric(crsp_monthly[\"permno\"], errors=\"coerce\").astype(\"Int32\")\n",
    "\n",
    "### Merge all data ###\n",
    "# Merge characteristics with crsp_monthly on month and permno\n",
    "merged_df = pd.merge(characteristics, crsp_monthly, on=[\"month\", \"permno\"], how=\"inner\")\n",
    "\n",
    "# Merge the result with cik_permno on permno to get cik\n",
    "merged_df = pd.merge(merged_df, cik_permno_linking_table, on=\"permno\", how=\"inner\")\n",
    "\n",
    "# Merge the merged_df with macro_predictors on month\n",
    "merged_df = pd.merge(merged_df, macro_predictors, on=\"month\", how=\"inner\")\n",
    "\n",
    "# Rearrange columns\n",
    "cols = merged_df.columns.tolist()\n",
    "cols = [\"month\", \"cik\", \"permno\", \"ret_excess\", \"prc\", \"shrout\", \"mktcap\", \"mktcap_lag\"] + [col for col in cols if col not in [\"month\", \"cik\", \"permno\", \"ret_excess\", \"prc\", \"shrout\", \"mktcap\", \"mktcap_lag\"]]\n",
    "merged_df = merged_df[cols].reset_index(drop=True)\n",
    "\n",
    "# # Display the first couple of rows\n",
    "# display(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24edf526",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create stock universe for breakpoints ###\n",
    "stock_universe = merged_df[[\"month\", \"cik\", \"permno\", \"ret_excess\", \"prc\", \"shrout\", \"mktcap\", \"mktcap_lag\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Save as stock_universe.csv\n",
    "stock_universe.to_csv(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/Stock Universe/stock_universe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94292767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique ciks: 6760, No. of unique permnos: 6760\n"
     ]
    }
   ],
   "source": [
    "### Create cik_permno linking table with only cik and permno from merged_df. Why? Because insider trades have to be linked to cik and permno with returns data later on. ###\n",
    "insider_cik_permno_linking_table = merged_df[[\"month\", \"cik\", \"permno\", \"mktcap\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Print for later use \n",
    "print(f\"No. of unique ciks: {insider_cik_permno_linking_table.cik.nunique()}, No. of unique permnos: {insider_cik_permno_linking_table.permno.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b66cd",
   "metadata": {},
   "source": [
    "**Preparing insider trading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f37edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of insider trades: 3754776\n"
     ]
    }
   ],
   "source": [
    "### The insider trades have been downloaded, see seperate file ###\n",
    "path = \"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/Insider Trading/Insider Transaction Data 2021-2006/trades_2006-2021.txt\"\n",
    "\n",
    "if os.path.exists(path):\n",
    "    insider_trades = pd.read_csv(path, sep=\"\\t\")\n",
    "else: \n",
    "    print(\"No path found\")\n",
    "    \n",
    "# # Print the column names and formats to verify\n",
    "# print(insider_trades.dtypes)\n",
    "\n",
    "### Prepare insider trades data ###\n",
    "insider_trades.columns = insider_trades.columns.str.lower()\n",
    "\n",
    "# # Print list of column names\n",
    "# print(insider_trades.columns.tolist())\n",
    "\n",
    "# Drop columns that are not needed for the analysis\n",
    "cols_to_keep = ['filing_date', \n",
    "                'issuercik', \n",
    "                'issuertradingsymbol', \n",
    "                'rptownercik', \n",
    "                'rptownername', \n",
    "                'rptowner_relationship', \n",
    "                'rptowner_title',\n",
    "                'security_title', \n",
    "                'trans_date', \n",
    "                'trans_code', \n",
    "                'trans_shares', \n",
    "                'trans_pricepershare', \n",
    "                'total']\n",
    "\n",
    "insider_trades = insider_trades[cols_to_keep]\n",
    "\n",
    "# Rename columns for clarity\n",
    "insider_trades = insider_trades.rename(columns={\n",
    "    \"issuercik\": \"cik\",\n",
    "    \"issuertradingsymbol\": \"ticker\",\n",
    "    \"rptownercik\": \"reporting_cik\",\n",
    "    \"rptownername\": \"name\",\n",
    "    \"rptowner_relationship\": \"relationship\",\n",
    "    \"rptowner_title\": \"officer_title\",\n",
    "    \"security_title\": \"security\",\n",
    "    \"trans_date\": \"transaction_date\",\n",
    "    \"trans_code\": \"transaction_type\",\n",
    "    \"trans_shares\": \"shares\",\n",
    "    \"trans_pricepershare\": \"price_per_share\",\n",
    "    \"total\": \"total\",\n",
    "})\n",
    "\n",
    "# # Print names and formats\n",
    "# print(insider_trades.dtypes)\n",
    "\n",
    "# Print len of insider_trades\n",
    "print(f\"No. of insider trades: {len(insider_trades)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76ff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of insider trades: 3367525\n"
     ]
    }
   ],
   "source": [
    "### Dates ###\n",
    "\n",
    "# Columns to Datetime\n",
    "cols_to_dt = [\"filing_date\", \"transaction_date\"]\n",
    "\n",
    "# Convert using the appropriate datetime format\n",
    "insider_trades[cols_to_dt] = insider_trades[cols_to_dt].apply(\n",
    "    lambda x: pd.to_datetime(x, format=\"%d-%b-%Y\", errors=\"coerce\").dt.normalize()\n",
    ")\n",
    "\n",
    "# Filter insider trades to be within the analysis period\n",
    "insider_trades = insider_trades[\n",
    "    insider_trades[cols_to_dt].apply(lambda x: x.between((pd.to_datetime(START_DATE)+pd.to_timedelta(1, unit=\"D\")), pd.to_datetime(END_DATE))).all(axis=1)\n",
    "]\n",
    "\n",
    "### Set condition ###\n",
    "outsider = True # True if outsider trades, False if insider trades\n",
    "\n",
    "# Filter based on outsider or insider trades\n",
    "if outsider:\n",
    "    # Remove rows where filing_date and transaction_date are not the same month\n",
    "    insider_trades = insider_trades[\n",
    "        insider_trades[\"filing_date\"].dt.to_period(\"M\") == insider_trades[\"transaction_date\"].dt.to_period(\"M\")\n",
    "    ].reset_index(drop=True)\n",
    "else:\n",
    "    # Keep all insider trades\n",
    "    pass\n",
    "\n",
    "# Create a month column based on transaction_date\n",
    "insider_trades[\"month\"] = insider_trades[\"transaction_date\"].dt.to_period(\"M\").dt.to_timestamp(\"M\")\n",
    "    \n",
    "# Print len of insider_trades\n",
    "print(f\"No. of insider trades: {len(insider_trades)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c97b3",
   "metadata": {},
   "source": [
    "With outsider == False, 3,742,752 transactions. \n",
    "\n",
    "\n",
    "With outsider == True, 3,367,525 transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1befa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate list of market days (NYSE trading days) for clustering ###\n",
    "market_cal = mcal.get_calendar(\"NYSE\")\n",
    "\n",
    "# Get the NYSE schedule (open/close times)\n",
    "schedule = market_cal.schedule(start_date=START_DATE, end_date=END_DATE)\n",
    "\n",
    "# Convert schedule into a list of actual trading days\n",
    "market_days = mcal.date_range(schedule, frequency=\"1D\")\n",
    "\n",
    "# Ensure the result is a clean DatetimeIndex without time-zone and normalized to midnight\n",
    "market_days = pd.to_datetime(market_days).tz_localize(None).normalize()\n",
    "\n",
    "# Assert market dates are monotoniccally increasing\n",
    "assert market_days.is_monotonic_increasing, \"Market days are not monotonically increasing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bfaeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of insider trades: 3367525\n",
      "Number of transactions with invalid transaction dates: 2929\n",
      "No. of insider trades after removing invalid dates: 3364596\n"
     ]
    }
   ],
   "source": [
    "### Print number of transactions ###\n",
    "print(f\"No. of insider trades: {len(insider_trades)}\")\n",
    "\n",
    "# Check for any transaction dates that are not market days\n",
    "invalid_dates = insider_trades[~insider_trades[\"transaction_date\"].isin(market_days)]\n",
    "\n",
    "# Print the number of invalid dates found\n",
    "print(f\"Number of transactions with invalid transaction dates: {len(invalid_dates)}\")\n",
    "\n",
    "# As 1000 rows are negligible compared to the full sample, they are dropped\n",
    "insider_trades = insider_trades[insider_trades[\"transaction_date\"].isin(market_days)].reset_index(drop=True)\n",
    "\n",
    "# Print number of transactions after removing invalid dates\n",
    "print(f\"No. of insider trades after removing invalid dates: {len(insider_trades)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6bac02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of insider trades: 2201319\n"
     ]
    }
   ],
   "source": [
    "### Merge with cik_permno linking table to get permno ###\n",
    "\n",
    "# Merge insider trades with cik_permno linking table on cik and month\n",
    "insider_trades = insider_trades.merge(\n",
    "    insider_cik_permno_linking_table,\n",
    "    on=[\"cik\", \"month\"],\n",
    "    how=\"inner\"\n",
    ").sort_values(by=[\"cik\", \"transaction_date\"]).reset_index(drop=True)\n",
    "\n",
    "# Drop month column as it is no longer needed\n",
    "insider_trades = insider_trades.drop(columns=[\"month\"])\n",
    "\n",
    "# Set permno after cik for easier viewing\n",
    "cols_order = insider_trades.columns.tolist()\n",
    "cols = [\"cik\", \"permno\"] + [col for col in cols_order if col not in [\"cik\", \"permno\"]]\n",
    "insider_trades = insider_trades[cols]\n",
    "\n",
    "# Print len of insider_trades\n",
    "print(f\"No. of insider trades: {len(insider_trades)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede54f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before cleaning: 2,201,319\n",
      "\n",
      "Removed by invalid ticker: 165\n",
      "Removed by invalid/missing total: 696,785\n",
      "Removed by small/missing trades (<100 shares): 3,782\n",
      "Removed by market cap filter (>5% of firm value): 10,668\n",
      "Removed exact duplicates: 26,535\n",
      "Removed near-duplicates (name/reporting_cik differences): 249,316\n",
      "\n",
      "Rows after cleaning: 1,214,068\n",
      "Total removed: 987,251\n"
     ]
    }
   ],
   "source": [
    "### Cleaning ###\n",
    "\n",
    "# Rows before cleaning\n",
    "before = len(insider_trades)\n",
    "print(f\"Rows before cleaning: {before:,}\\n\")\n",
    "\n",
    "# Copy df\n",
    "df0 = insider_trades.copy()\n",
    "\n",
    "# Valid ticker\n",
    "df0[\"ticker\"] = df0[\"ticker\"].astype(str).str.upper() # Ensure ticker is uppercase string for consistent filtering\n",
    "mask0 = ~df0[\"ticker\"].isin([\"NONE\", \"N/A\", \"NA\"])\n",
    "print(f\"Removed by invalid ticker: {len(df0) - mask0.sum():,}\")\n",
    "df0 = df0[mask0]\n",
    "\n",
    "# total > 10_000 and not missing\n",
    "mask1 = df0[\"total\"].notna() & (df0[\"total\"] > 10_000)\n",
    "print(f\"Removed by invalid/missing total: {len(df0) - mask1.sum():,}\")\n",
    "df1 = df0[mask1].copy()\n",
    "\n",
    "# At least 100 shares\n",
    "mask2 = (df1[\"shares\"].notna() & (df1[\"shares\"] >= 100))\n",
    "print(f\"Removed by small/missing trades (<100 shares): {len(df1) - mask2.sum():,}\")\n",
    "df2 = df1[mask2]\n",
    "\n",
    "# Market cap filter (5% of firm value)\n",
    "scale = 1_000_000  # market cap is in millions\n",
    "mask3 = df2[\"mktcap\"].notna() & (df2[\"total\"] <= 0.05 * (df2[\"mktcap\"] * scale))\n",
    "print(f\"Removed by market cap filter (>5% of firm value): {len(df2) - mask3.sum():,}\")\n",
    "df3 = df2[mask3]\n",
    "\n",
    "# Remove exact duplicates\n",
    "before_dupes = len(df3)\n",
    "df3 = df3.drop_duplicates()\n",
    "after_dupes = len(df3)\n",
    "print(f\"Removed exact duplicates: {before_dupes - after_dupes:,}\")\n",
    "\n",
    "# Remove duplicates where only name/reporting_cik differs\n",
    "if \"name\" in df3.columns and \"reporting_cik\" in df3.columns:\n",
    "    before_near_dupes = len(df3)\n",
    "    df3 = df3.drop_duplicates(\n",
    "        subset=[col for col in df3.columns if col not in [\"name\", \"reporting_cik\"]]\n",
    "    )\n",
    "    after_near_dupes = len(df3)\n",
    "    print(f\"Removed near-duplicates (name/reporting_cik differences): {before_near_dupes - after_near_dupes:,}\")\n",
    "\n",
    "# Drop mktcap column as it is no longer needed\n",
    "df3 = df3.drop(columns=[\"mktcap\"])\n",
    "\n",
    "# Rows after cleaning\n",
    "after = len(df3)\n",
    "print(f\"\\nRows after cleaning: {after:,}\")\n",
    "print(f\"Total removed: {before - after:,}\")\n",
    "\n",
    "# Final cleaned DataFrame\n",
    "insider_trades = df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d56ed",
   "metadata": {},
   "source": [
    "Insider = 1,313,093\n",
    "\n",
    "Outsider = 1,214,068"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf7aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories: ['other_officer' 'vice_president' 'cfo' 'ceo' 'coo' 'director'\n",
      " 'ten_percent_owner']\n",
      "title\n",
      "other_officer        493961\n",
      "ceo                  246018\n",
      "vice_president       163495\n",
      "ten_percent_owner    130914\n",
      "cfo                   88898\n",
      "coo                   49685\n",
      "director              41097\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### Change Titles ### \n",
    "\n",
    "# Abbreviation normalization map\n",
    "_ABBREV_MAP = {\n",
    "    r\"\\b&\\b\": \" and \",\n",
    "    r\"\\bevps?\\b\": \" executive vice president \",\n",
    "    r\"\\bsvp\\b\": \" senior vice president \",\n",
    "    r\"\\bsr\\.\\b\": \" senior \",\n",
    "    r\"\\bsr\\b\": \" senior \",\n",
    "    r\"\\bavp\\b\": \" assistant vice president \",\n",
    "    r\"\\bvp\\b\": \" vice president \",\n",
    "    r\"\\bceo\\b\": \" chief executive officer \",\n",
    "    r\"\\bcfo\\b\": \" chief financial officer \",\n",
    "    r\"\\bcoo\\b\": \" chief operating officer \",\n",
    "    r\"\\bcto\\b\": \" chief technology officer \",\n",
    "}\n",
    "_STOP_PUNCT = r\"[^a-z0-9/&\\- ]\"\n",
    "\n",
    "# Normalization function\n",
    "def normalize_title(s: str) -> str:\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    t = str(s).lower().strip()\n",
    "    t = re.sub(_STOP_PUNCT, \" \", t)\n",
    "    for pat, repl in _ABBREV_MAP.items():\n",
    "        t = re.sub(pat, repl, t)\n",
    "    t = t.replace(\"/\", \" \").replace(\"-\", \" \")\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "### Change below to all lower.string? \n",
    "\n",
    "# Categorization rules (updated hierarchy)\n",
    "def categorize_simple(t: str) -> str:\n",
    "    if t is None:\n",
    "        return None\n",
    "\n",
    "    # CEO\n",
    "    if \"chief executive officer\" in t or \"ceo\" in t:\n",
    "        return \"ceo\"\n",
    "    # CFO\n",
    "    elif any(w in t for w in [\"chief financial officer\", \"cfo\", \"treasurer\", \"controller\", \"finance\"]):\n",
    "        return \"cfo\"\n",
    "    # COO\n",
    "    elif any(w in t for w in [\"chief operating officer\", \"coo\", \"operations\"]):\n",
    "        return \"coo\"\n",
    "    # Other Officers (all other \"Chiefs\" such as CTO, CLO, etc.)\n",
    "    elif any(w in t for w in [\n",
    "        \"chief\", \"cto\", \"legal\", \"marketing\", \"information\", \"technology\",\n",
    "        \"strategy\", \"compliance\", \"administration\", \"hr\"\n",
    "    ]):\n",
    "        return \"other_officer\"\n",
    "    # Vice Presidents\n",
    "    elif any(w in t for w in [\"vice president\", \"executive vice president\", \"senior vice president\"]):\n",
    "        return \"vice_president\"\n",
    "    # Directors / Board members\n",
    "    elif any(w in t for w in [\"director\", \"board\", \"chairman\", \"managing director\"]):\n",
    "        return \"director\"\n",
    "    # Owners (≥10%)\n",
    "    elif \"owner\" in t:\n",
    "        return \"ten_percent_owner\"\n",
    "    # Default fallback\n",
    "    else:\n",
    "        return \"other_officer\"\n",
    "\n",
    "# Apply normalization and categorization\n",
    "insider_trades[\"title\"] = insider_trades[\"officer_title\"].apply(normalize_title)\n",
    "insider_trades[\"title\"] = insider_trades[\"title\"].apply(categorize_simple)\n",
    "\n",
    "# Backfill from relationship if available\n",
    "if \"relationship\" in insider_trades.columns: \n",
    "    insider_trades[\"title\"] = insider_trades[\"title\"].fillna(\n",
    "        insider_trades[\"relationship\"].apply(normalize_title).apply(categorize_simple)\n",
    "    )\n",
    "\n",
    "# Collapse into 7 categories (enforcing clean labels)\n",
    "VALID_CATS = [\"ceo\", \"cfo\", \"coo\", \"other_officer\", \"vice_president\", \"director\", \"ten_percent_owner\"]\n",
    "\n",
    "def enforce_categories(s: str) -> str:\n",
    "    if pd.isna(s):\n",
    "        return \"other_officer\"\n",
    "    if s in VALID_CATS:\n",
    "        return s\n",
    "    if \"vice\" in s.lower():\n",
    "        return \"vice_president\"\n",
    "    if \"director\" in s.lower():\n",
    "        return \"director\"\n",
    "    if \"owner\" in s.lower():\n",
    "        return \"ten_percent_owner\"\n",
    "    return \"other_officer\"\n",
    "\n",
    "insider_trades[\"title\"] = insider_trades[\"title\"].apply(enforce_categories)\n",
    "\n",
    "# Move title next to relationship if it exists\n",
    "if \"relationship\" in insider_trades.columns:\n",
    "    cols = list(insider_trades.columns)\n",
    "    rel_idx = cols.index(\"relationship\")\n",
    "    cols.insert(rel_idx + 1, cols.pop(cols.index(\"title\")))\n",
    "    insider_trades = insider_trades[cols]\n",
    "\n",
    "print(\"Unique categories:\", insider_trades[\"title\"].unique())\n",
    "print(insider_trades[\"title\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacda9c0",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e98350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dummy Variables for Officer Titles ###\n",
    "# Create dummies \n",
    "insider_trades = pd.get_dummies(\n",
    "    insider_trades,\n",
    "    columns=[\"title\"],\n",
    "    prefix=\"is_tit\",       \n",
    "    drop_first=False,      # Keep all categories\n",
    "    dtype=\"float32\",\n",
    "    sparse=False\n",
    ")\n",
    "\n",
    "# # Print column names and types\n",
    "# print(insider_trades.dtypes)\n",
    "\n",
    "### Dummy Variables for Transaction Type ###\n",
    "# Standardize transaction types\n",
    "insider_trades[\"transaction_type\"] = (\n",
    "    insider_trades[\"transaction_type\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower() # Ensure consistent casing\n",
    ")\n",
    "\n",
    "# Create dummies\n",
    "insider_trades = pd.get_dummies(insider_trades,\n",
    "                                columns=[\"transaction_type\"],\n",
    "                                prefix=\"is_txn\",         \n",
    "                                drop_first=False,       # Keep all categories\n",
    "                                dtype=\"float32\",\n",
    "                                sparse=False\n",
    ")\n",
    "\n",
    "# Rename for readability\n",
    "insider_trades = insider_trades.rename(columns={\n",
    "    \"is_txn_p\": \"is_txn_purchase\",\n",
    "    \"is_txn_s\": \"is_txn_sell\",\n",
    "})\n",
    "\n",
    "# # Print column names and types\n",
    "# print(insider_trades.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84142e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interaction is_purchase and is_sell with officer titles ###\n",
    "# List of officer title dummies\n",
    "officer_titles = [col for col in insider_trades.columns if col.startswith(\"is_tit_\")]\n",
    "\n",
    "# For loop\n",
    "for txn in [\"is_txn_purchase\", \"is_txn_sell\"]:\n",
    "    for title in officer_titles:\n",
    "        col_name = f\"{txn}_x_{title}\"\n",
    "        insider_trades[col_name] = (\n",
    "            insider_trades[txn] * insider_trades[title]\n",
    "            ).astype(\"float32\")\n",
    "        \n",
    "# Drop titles from dataframe\n",
    "insider_trades = insider_trades.drop(columns=officer_titles)\n",
    "\n",
    "# # Print column names and types\n",
    "# print(insider_trades.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b9460",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct is_vol_purchase and is_vol_sell ###\n",
    "insider_trades[\"is_vol_purchase\"] = (insider_trades[\"is_txn_purchase\"] * insider_trades[\"total\"]).astype(\"float32\")\n",
    "insider_trades[\"is_vol_sell\"] = (insider_trades[\"is_txn_sell\"] * insider_trades[\"total\"]).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39e78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k3/9jyk10k50tg1bg0x762zj6200000gn/T/ipykernel_41890/962994462.py:96: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: clusters_per_type(x, \"is_txn_purchase\"))\n",
      "/var/folders/k3/9jyk10k50tg1bg0x762zj6200000gn/T/ipykernel_41890/962994462.py:102: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: clusters_per_type(x, \"is_txn_sell\"))\n"
     ]
    }
   ],
   "source": [
    "# ### Function to Construct Cluster Features: Detect cluster episodes for one firm and one transaction type, returns column with 1 on the LAST day of each cluster episode ###\n",
    "def clusters_per_type(group, txn_col):\n",
    "\n",
    "    # Create a copy of the group to avoid modifying the original DataFrame\n",
    "    group = group.copy()\n",
    "    \n",
    "    # Ensure datetime format for transaction_date\n",
    "    group[\"transaction_date\"] = pd.to_datetime(group[\"transaction_date\"]).dt.normalize()\n",
    "    \n",
    "    # Initialize cluster column to 0\n",
    "    group[f\"{txn_col}_cluster\"] = 0\n",
    "\n",
    "    # Filter to transaction true\n",
    "    txn_true = group[group[txn_col] == 1].copy()\n",
    "\n",
    "    # If no trades of this type -> no clusters\n",
    "    if txn_true.empty:\n",
    "        return group\n",
    "\n",
    "    # Sort by transaction date\n",
    "    txn_true = txn_true.sort_values(\"transaction_date\")\n",
    "\n",
    "    # Extract unique dates\n",
    "    dates = (\n",
    "        txn_true[\"transaction_date\"]\n",
    "        .drop_duplicates()\n",
    "        .sort_values()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Ensure that len(dates) >= 2 for cluster detection\n",
    "    if len(dates) < 2:\n",
    "        return group\n",
    "\n",
    "    # Find all cluster-participating days\n",
    "    cluster_days = []\n",
    "\n",
    "    for i in range(len(dates) - 1):\n",
    "        d1, d2 = dates[i], dates[i + 1]\n",
    "\n",
    "        # Must be consecutive market days\n",
    "        if d2 == market_days[market_days.get_loc(d1) + 1]:\n",
    "\n",
    "            # Count unique insiders over d1+d2\n",
    "            insiders = (\n",
    "                txn_true.loc[txn_true[\"transaction_date\"].isin([d1, d2]), \"reporting_cik\"]\n",
    "                .nunique()\n",
    "            )\n",
    "\n",
    "            if insiders >= 2:\n",
    "                cluster_days.append(d1)\n",
    "                cluster_days.append(d2)\n",
    "\n",
    "    # If no cluster days found return group\n",
    "    if not cluster_days:\n",
    "        return group\n",
    "\n",
    "    # Collapse cluster-days into cluster episodes\n",
    "    cluster_days = pd.Series(sorted(set(cluster_days)))\n",
    "\n",
    "    # Convert cluster_days to market-day index positions\n",
    "    market_days_idx = pd.Index(market_days)\n",
    "    cluster_pos = cluster_days.map(market_days_idx.get_loc)\n",
    "\n",
    "    # Identify new blocks where the gap is > 1 market day\n",
    "    blocks = cluster_pos.diff().gt(1).cumsum()\n",
    "\n",
    "    # The END of each block is the last cluster day in that block\n",
    "    cluster_ends = cluster_days.groupby(blocks).max()\n",
    "\n",
    "    # Mark the last row per cluster episode\n",
    "    for end_day in cluster_ends:\n",
    "        \n",
    "        # All trade rows on that day of this transaction\n",
    "        rows_on_day = txn_true[txn_true[\"transaction_date\"] == end_day]\n",
    "\n",
    "        # If multiple rows, use the last one\n",
    "        if not rows_on_day.empty:\n",
    "            last_idx = rows_on_day.index[-1]\n",
    "            group.loc[last_idx, f\"{txn_col}_cluster\"] = 1\n",
    "\n",
    "    return group\n",
    "\n",
    "### Apply cluster logic to entire dataset ###\n",
    "def compute_purchase_sell_clusters(df):\n",
    "\n",
    "    # Copy to avoid modifying original dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by cik and transaction_date\n",
    "    df = df.sort_values([\"cik\", \"transaction_date\"])\n",
    "\n",
    "    # Purchase clusters\n",
    "    df = (\n",
    "        df.groupby(\"cik\", group_keys=False)\n",
    "          .apply(lambda x: clusters_per_type(x, \"is_txn_purchase\"))\n",
    "    )\n",
    "\n",
    "    # sell clusters\n",
    "    df = (\n",
    "        df.groupby(\"cik\", group_keys=False)\n",
    "          .apply(lambda x: clusters_per_type(x, \"is_txn_sell\"))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "### Call Function to Compute Clusters ###\n",
    "insider_trades = compute_purchase_sell_clusters(insider_trades) # Ensure no pandas version > 2.2.3 - groupby issues may arise for later versions (deprecation of group_keys)\n",
    "\n",
    "# Ensure cluster columns are float32\n",
    "insider_trades[\"is_txn_purchase_cluster\"] = insider_trades[\"is_txn_purchase_cluster\"].astype(\"float32\")\n",
    "insider_trades[\"is_txn_sell_cluster\"] = insider_trades[\"is_txn_sell_cluster\"].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a61c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insider Trader Types Monthly Summary:\n",
      "is_opp_buy     29580.0\n",
      "is_opp_sell    99349.0\n",
      "is_rtn_buy      3978.0\n",
      "is_rtn_sell    29913.0\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "### Classify Routine vs. Opportunistic Insiders ###\n",
    "def classify_routine_opportunistic_monthly(df_trades_clean, min_year=2009):\n",
    "    \"\"\"\n",
    "    Classify insiders as routine or opportunistic (CMP 2012) based on transaction_date.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy to avoid modifying original dataframe\n",
    "    df = df_trades_clean.copy()\n",
    "    \n",
    "    # Ensure transaction_date is datetime\n",
    "    df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"]).dt.normalize()\n",
    "\n",
    "    # Dates\n",
    "    df[\"year\"] = df[\"transaction_date\"].dt.year\n",
    "    df[\"month\"] = df[\"transaction_date\"].dt.month\n",
    "    df[\"month_period\"] = df[\"transaction_date\"].dt.to_period(\"M\")\n",
    "\n",
    "    # Find routine insiders (3-year same-month pattern)\n",
    "    months_per_year = (\n",
    "        df.groupby([\"reporting_cik\", \"year\"])[\"month\"]\n",
    "          .agg(lambda x: set(x))\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # Find start year of routine trader\n",
    "    routine_start = {}\n",
    "    for insider, sub in months_per_year.groupby(\"reporting_cik\"):\n",
    "        sub = sub.sort_values(\"year\")\n",
    "        years = sub[\"year\"].to_numpy()\n",
    "        months = sub[\"month\"].to_list()\n",
    "\n",
    "        for j in range(len(years) - 2):\n",
    "            y1, y2, y3 = years[j], years[j+1], years[j+2]\n",
    "\n",
    "            # Consecutive years\n",
    "            if y2 == y1 + 1 and y3 == y1 + 2:\n",
    "                common = months[j] & months[j+1] & months[j+2]\n",
    "                if common:\n",
    "                    # Routine starts in year T+1\n",
    "                    routine_start[insider] = y3 + 1\n",
    "                    break\n",
    "\n",
    "    # Create DataFrame of routine_starts\n",
    "    df_routine = pd.DataFrame(\n",
    "        list(routine_start.items()),\n",
    "        columns=[\"reporting_cik\", \"routine_start_year\"]\n",
    "    )\n",
    "\n",
    "    df = df.merge(df_routine, on=\"reporting_cik\", how=\"left\")\n",
    "\n",
    "    # Classify trades\n",
    "    df[\"trader_type\"] = np.where(\n",
    "        df[\"routine_start_year\"].notna() &\n",
    "        (df[\"year\"] >= df[\"routine_start_year\"]),\n",
    "        \"routine\",\n",
    "        \"opportunistic\"\n",
    "    )\n",
    "\n",
    "    # Optional trimming\n",
    "    if min_year is not None:\n",
    "        df = df[df[\"year\"] >= min_year].copy()\n",
    "\n",
    "    # Get dummies for trader type and transaction type\n",
    "    df[\"is_opp_buy\"]  = ((df[\"trader_type\"] == \"opportunistic\") & df[\"is_txn_purchase\"]).astype(\"float32\")\n",
    "    df[\"is_opp_sell\"] = ((df[\"trader_type\"] == \"opportunistic\") & df[\"is_txn_sell\"]).astype(\"float32\")\n",
    "    df[\"is_rtn_buy\"]  = ((df[\"trader_type\"] == \"routine\")       & df[\"is_txn_purchase\"]).astype(\"float32\")\n",
    "    df[\"is_rtn_sell\"] = ((df[\"trader_type\"] == \"routine\")       & df[\"is_txn_sell\"]).astype(\"float32\")\n",
    "\n",
    "    # Aggregate to firm–month (only months with trades)\n",
    "    monthly = (\n",
    "        df.groupby([\"cik\", \"month_period\"])[\n",
    "            [\"is_opp_buy\", \"is_opp_sell\", \"is_rtn_buy\", \"is_rtn_sell\"]\n",
    "        ]\n",
    "        .max()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"month_period\": \"month\"})\n",
    "    )\n",
    "\n",
    "    return monthly[[\"cik\", \"month\", \"is_opp_buy\", \"is_opp_sell\", \"is_rtn_buy\", \"is_rtn_sell\"]]\n",
    "\n",
    "### Call Function to Classify Routine vs. Opportunistic Insiders ###\n",
    "insider_trader_types_monthly = classify_routine_opportunistic_monthly(insider_trades, min_year=2009)\n",
    "\n",
    "### Ensure month is datetime and end of month ###\n",
    "\n",
    "# Set month to timestamp\n",
    "insider_trader_types_monthly[\"month\"] = insider_trader_types_monthly[\"month\"].dt.to_timestamp(\"M\")\n",
    "\n",
    "# Ensure that period is within analysis window\n",
    "insider_trader_types_monthly = insider_trader_types_monthly[\n",
    "    (insider_trader_types_monthly[\"month\"] >= pd.to_datetime(START_DATE)) &\n",
    "    (insider_trader_types_monthly[\"month\"] <= pd.to_datetime(END_DATE))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Print report\n",
    "print(\"Insider Trader Types Monthly Summary:\")\n",
    "print(insider_trader_types_monthly[[\"is_opp_buy\", \"is_opp_sell\", \"is_rtn_buy\", \"is_rtn_sell\"]].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8efc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to Classify Insider Silence Events (PPN, SSN): Return Only firm-month where an event occurs.  ###\n",
    "def classify_insider_silence_fast(df):\n",
    "    \"\"\"\n",
    "    Classify insider silence events (PPN, SSN) based on transaction_date.\n",
    "    PPN: purchase-purchase-no trade\n",
    "    SSN: sell-sell-no trade\"\"\"\n",
    "\n",
    "    # Copy data to avoid modifying the original\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure transaction_date is datetime\n",
    "    df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"]).dt.normalize()\n",
    "\n",
    "    # Prepare date columns\n",
    "    df[\"year\"] = df[\"transaction_date\"].dt.year\n",
    "    df[\"month\"] = df[\"transaction_date\"].dt.month\n",
    "\n",
    "    # Monthly insider activity\n",
    "    g = (\n",
    "        df.groupby([\"cik\", \"reporting_cik\", \"year\", \"month\"])[[\"is_txn_purchase\", \"is_txn_sell\"]]\n",
    "          .any()\n",
    "          .reset_index()\n",
    "    )\n",
    "    g[\"has_any\"] = g[\"is_txn_purchase\"] | g[\"is_txn_sell\"]\n",
    "    \n",
    "    # Months where insiders bought/sold\n",
    "    purchase = g[g[\"is_txn_purchase\"]][[\"cik\", \"reporting_cik\", \"month\", \"year\"]]\n",
    "    sell = g[g[\"is_txn_sell\"]][[\"cik\", \"reporting_cik\", \"month\", \"year\"]]\n",
    "\n",
    "    # Build T-year candidates (PPN purchases)\n",
    "    purchase_tminus2 = purchase.assign(year=lambda x: x[\"year\"] + 2)\n",
    "    purchase_tminus1 = purchase.assign(year=lambda x: x[\"year\"] + 1)\n",
    "\n",
    "    cand_ppn_T = (\n",
    "        purchase_tminus2.merge(\n",
    "            purchase_tminus1,\n",
    "            on=[\"cik\", \"reporting_cik\", \"month\", \"year\"],\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        [[\"cik\", \"reporting_cik\", \"month\", \"year\"]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    # Build T-year candidates (SSN sells)\n",
    "    sell_tminus2 = sell.assign(year=lambda x: x[\"year\"] + 2)\n",
    "    sell_tminus1 = sell.assign(year=lambda x: x[\"year\"] + 1)\n",
    "\n",
    "    cand_ssn_T = (\n",
    "        sell_tminus2.merge(\n",
    "            sell_tminus1,\n",
    "            on=[\"cik\", \"reporting_cik\", \"month\", \"year\"],\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        [[\"cik\", \"reporting_cik\", \"month\", \"year\"]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    # Insider traded in T (used to filter out non-silence)\n",
    "    any_T = g[g[\"has_any\"]][[\"cik\", \"reporting_cik\", \"month\", \"year\"]]\n",
    "\n",
    "    # PPN: purchase-purchase-no trade\n",
    "    ppn_T = (\n",
    "        cand_ppn_T.merge(any_T, on=[\"cik\", \"reporting_cik\", \"month\", \"year\"],\n",
    "                         how=\"left\", indicator=True)\n",
    "                 .loc[lambda x: x[\"_merge\"] == \"left_only\"]\n",
    "                 .drop(columns=\"_merge\")\n",
    "    )\n",
    "\n",
    "    # SSN: sell-sell-no trade\n",
    "    ssn_T = (\n",
    "        cand_ssn_T.merge(any_T, on=[\"cik\", \"reporting_cik\", \"month\", \"year\"],\n",
    "                         how=\"left\", indicator=True)\n",
    "                 .loc[lambda x: x[\"_merge\"] == \"left_only\"]\n",
    "                 .drop(columns=\"_merge\")\n",
    "    )\n",
    "\n",
    "    # Convert to firm–month Period[M]\n",
    "    def to_month(df_sub):\n",
    "        out = df_sub.copy()\n",
    "        out[\"month\"] = pd.PeriodIndex(\n",
    "            pd.to_datetime(dict(year=out[\"year\"], month=out[\"month\"], day=1)),\n",
    "            freq=\"M\"\n",
    "        )\n",
    "        return out[[\"cik\", \"month\"]]\n",
    "\n",
    "    ppn_e = to_month(ppn_T).assign(is_ppn=1.0)\n",
    "    ssn_e = to_month(ssn_T).assign(is_ssn=1.0)\n",
    "\n",
    "    # Aggregate to firm-month, convert to float32 for ML\n",
    "    firm_m = (\n",
    "        ppn_e.merge(ssn_e, on=[\"cik\", \"month\"], how=\"outer\")\n",
    "             .fillna(0)\n",
    "             .groupby([\"cik\", \"month\"], as_index=False)\n",
    "             .max()\n",
    "             .astype({\"is_ppn\": \"float32\", \"is_ssn\": \"float32\"})\n",
    "    )\n",
    "\n",
    "    return firm_m[[\"cik\", \"month\", \"is_ppn\", \"is_ssn\"]]\n",
    "\n",
    "### Call Function to Classify Insider Silence Events ###\n",
    "insider_silence_monthly = classify_insider_silence_fast(insider_trades)\n",
    "\n",
    "### Ensure month is datetime and end of month ###\n",
    "\n",
    "# Set month to timestamp\n",
    "insider_silence_monthly[\"month\"] = insider_silence_monthly[\"month\"].dt.to_timestamp(\"M\")\n",
    "\n",
    "# Ensure that period is within analysis window\n",
    "insider_silence_monthly = insider_silence_monthly[\n",
    "    (insider_silence_monthly[\"month\"] >= pd.to_datetime(START_DATE)) &\n",
    "    (insider_silence_monthly[\"month\"] <= pd.to_datetime(END_DATE))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66463f5a",
   "metadata": {},
   "source": [
    "**Merging Features to Insider Trades**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620445b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insider trades with features ###\n",
    "\n",
    "# Create final insider trades dataframe copy\n",
    "insider_trades_final = insider_trades.copy()\n",
    "\n",
    "# Drop filing_date, and rename transaction_date to month\n",
    "insider_trades_final = insider_trades_final.drop(columns=[\"filing_date\"]).rename(columns={\"transaction_date\": \"month\"})\n",
    "\n",
    "# Set month to month end timestamp\n",
    "insider_trades_final[\"month\"] = insider_trades_final[\"month\"].dt.to_period(\"M\").dt.to_timestamp(\"M\")\n",
    "\n",
    "# Merge with insider_trader_types_monthly to get trader types\n",
    "insider_trades_final = insider_trades_final.merge(\n",
    "    insider_trader_types_monthly,\n",
    "    on=[\"cik\", \"month\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Create columns named is_count_purchase and is_count_sell to sum the number of purchase/sell transactions\n",
    "insider_trades_final[\"is_count_purchase\"] = insider_trades_final[\"is_txn_purchase\"].astype(\"float32\")\n",
    "insider_trades_final[\"is_count_sell\"] = insider_trades_final[\"is_txn_sell\"].astype(\"float32\")\n",
    "\n",
    "# Aggregate to permno/transaction_month\n",
    "insider_trades_final = (\n",
    "    insider_trades_final\n",
    "    .groupby([\"cik\", \"month\"], as_index=False)\n",
    "    .agg({      \n",
    "        # Permno, take last\n",
    "        \"permno\": \"last\", \n",
    "        \n",
    "        # Binary indicators, use max\n",
    "        \"is_txn_purchase\": \"max\",\n",
    "        \"is_txn_sell\": \"max\",\n",
    "        \n",
    "        \"is_txn_purchase_x_is_tit_ceo\": \"max\",\n",
    "        \"is_txn_purchase_x_is_tit_cfo\": \"max\",\n",
    "        \"is_txn_purchase_x_is_tit_coo\": \"max\",\n",
    "        \"is_txn_purchase_x_is_tit_director\": \"max\",\n",
    "        \"is_txn_purchase_x_is_tit_other_officer\": \"max\",\n",
    "        \"is_txn_purchase_x_is_tit_ten_percent_owner\": \"max\",\n",
    "        \"is_txn_purchase_x_is_tit_vice_president\": \"max\",\n",
    "        \n",
    "        \"is_txn_sell_x_is_tit_ceo\": \"max\",\n",
    "        \"is_txn_sell_x_is_tit_cfo\": \"max\",\n",
    "        \"is_txn_sell_x_is_tit_coo\": \"max\",\n",
    "        \"is_txn_sell_x_is_tit_director\": \"max\",\n",
    "        \"is_txn_sell_x_is_tit_other_officer\": \"max\",\n",
    "        \"is_txn_sell_x_is_tit_ten_percent_owner\": \"max\",\n",
    "        \"is_txn_sell_x_is_tit_vice_president\": \"max\",\n",
    "        \n",
    "        \"is_opp_buy\": \"max\",\n",
    "        \"is_opp_sell\": \"max\",\n",
    "        \"is_rtn_buy\": \"max\",\n",
    "        \"is_rtn_sell\": \"max\",\n",
    "\n",
    "        # Volume, use sum \n",
    "        \"is_vol_purchase\": \"sum\",\n",
    "        \"is_vol_sell\": \"sum\",\n",
    "        \n",
    "        # Count, use sum \n",
    "        \"is_count_purchase\": \"sum\",\n",
    "        \"is_count_sell\": \"sum\",\n",
    "        \"is_txn_purchase_cluster\": \"sum\",\n",
    "        \"is_txn_sell_cluster\": \"sum\",\n",
    "    })\n",
    ")\n",
    "\n",
    "# # Display the first few rows of the aggregated dataframe\n",
    "# insider_trades_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2612802",
   "metadata": {},
   "outputs": [],
   "source": [
    "### is_ppn and is_ssn ###\n",
    "\n",
    "# Aggregate to cik/month\n",
    "insider_silence_monthly = (\n",
    "    insider_silence_monthly\n",
    "    .groupby([\"cik\", \"month\"], as_index=False)\n",
    "    .agg({      \n",
    "        \"is_ppn\": \"max\",\n",
    "        \"is_ssn\": \"max\"\n",
    "    })\n",
    ")\n",
    "\n",
    "### Lag 1 month ###\n",
    "\n",
    "# Lag month such that 2021-01 becomes 2021-02\n",
    "insider_silence_monthly[\"month\"] = insider_silence_monthly[\"month\"] + pd.offsets.MonthEnd(1)\n",
    "\n",
    "# Ensure that month falls between START_DATE and END_DATE\n",
    "insider_silence_monthly = insider_silence_monthly[\n",
    "    (insider_silence_monthly[\"month\"] >= START_DATE) & (insider_silence_monthly[\"month\"] <= END_DATE)\n",
    "]\n",
    "\n",
    "# Reset index\n",
    "insider_silence_monthly = insider_silence_monthly.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f9868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k3/9jyk10k50tg1bg0x762zj6200000gn/T/ipykernel_41890/4188858623.py:27: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: rolling_window(x, col))\n",
      "/var/folders/k3/9jyk10k50tg1bg0x762zj6200000gn/T/ipykernel_41890/4188858623.py:27: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: rolling_window(x, col))\n",
      "/var/folders/k3/9jyk10k50tg1bg0x762zj6200000gn/T/ipykernel_41890/4188858623.py:27: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: rolling_window(x, col))\n",
      "/var/folders/k3/9jyk10k50tg1bg0x762zj6200000gn/T/ipykernel_41890/4188858623.py:27: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: rolling_window(x, col))\n"
     ]
    }
   ],
   "source": [
    "### Create NPR ###\n",
    "\n",
    "# Create monthly DataFrame and sort by cik then month\n",
    "insider_trades_final = insider_trades_final.sort_values([\"cik\", \"month\"])\n",
    "\n",
    "# Construct rolling_window function\n",
    "def rolling_window(df, col):\n",
    "    out = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # 6-month trailing window using month-end logic\n",
    "        start = row[\"month\"] + pd.offsets.MonthEnd(-6)\n",
    "        \n",
    "        # mask selecting all rows within the window\n",
    "        mask = (df[\"month\"] > start) & (df[\"month\"] <= row[\"month\"])\n",
    "        \n",
    "        # sum the column over the window\n",
    "        out.append(df.loc[mask, col].sum())\n",
    "        \n",
    "    # return as a Series with the original index\n",
    "    return pd.Series(out, index=df.index)\n",
    "\n",
    "# Apply to each variable and each issuer\n",
    "for col in [\"is_count_purchase\", \"is_count_sell\", \"is_vol_purchase\", \"is_vol_sell\"]:\n",
    "    insider_trades_final[f\"{col}_6m\"] = (\n",
    "        insider_trades_final.groupby(\"cik\", group_keys=False)\n",
    "               .apply(lambda x: rolling_window(x, col))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665774c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saferatio(num, den):\n",
    "    # Create output Series initialized to 0.0, with same index as num\n",
    "    out = pd.Series(0.0, index=num.index)\n",
    "\n",
    "    # mask where denominator is NOT zero\n",
    "    mask = den != 0\n",
    "\n",
    "    # compute ratio only where safe (den != 0)\n",
    "    out[mask] = num[mask] / den[mask]\n",
    "    \n",
    "    return out\n",
    "\n",
    "# NPR calculations\n",
    "insider_trades_final[\"is_npr_count\"] = saferatio(\n",
    "        insider_trades_final[\"is_count_purchase_6m\"] - insider_trades_final[\"is_count_sell_6m\"],\n",
    "        insider_trades_final[\"is_count_purchase_6m\"] + insider_trades_final[\"is_count_sell_6m\"],\n",
    "    )\n",
    "\n",
    "insider_trades_final[\"is_npr_volume\"] = saferatio(\n",
    "        insider_trades_final[\"is_vol_purchase_6m\"] - insider_trades_final[\"is_vol_sell_6m\"],\n",
    "        insider_trades_final[\"is_vol_purchase_6m\"] + insider_trades_final[\"is_vol_sell_6m\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ef1692",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Net Clustering ###\n",
    "insider_trades_final[\"is_net_cluster\"] = (\n",
    "    insider_trades_final[\"is_txn_purchase_cluster\"] - insider_trades_final[\"is_txn_sell_cluster\"]\n",
    ").astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea38455",
   "metadata": {},
   "source": [
    "**Extract features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda478ae",
   "metadata": {},
   "source": [
    "**Lags and dropping unnecessary columns** \n",
    "\n",
    "Relies on monthly data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b9cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lag 1 month ###\n",
    "\n",
    "# Lag month such that 2021-01 becomes 2021-02\n",
    "insider_trades_final[\"month\"] = insider_trades_final[\"month\"] + pd.offsets.MonthEnd(1)\n",
    "\n",
    "# Ensure that month falls between START_DATE and END_DATE\n",
    "insider_trades_final = insider_trades_final[\n",
    "    (insider_trades_final[\"month\"] >= START_DATE) & (insider_trades_final[\"month\"] <= END_DATE)\n",
    "]\n",
    "\n",
    "# Reset index\n",
    "insider_trades_final = insider_trades_final.reset_index(drop=True)\n",
    "\n",
    "# # Display the first few rows of the final dataframe\n",
    "# insider_trades_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21768ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping features that are not used anymore ###\n",
    "insider_trades_final = insider_trades_final.drop(columns=[\"is_txn_purchase\", \"is_txn_sell\", \"is_vol_purchase\", \n",
    "                                                          \"is_vol_sell\", \"is_count_purchase\", \"is_count_sell\",\n",
    "                                                          \"is_count_purchase_6m\", \"is_count_sell_6m\",\n",
    "                                                          \"is_vol_purchase_6m\", \"is_vol_sell_6m\", \n",
    "                                                          \"is_txn_purchase_cluster\", \"is_txn_sell_cluster\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a9b92",
   "metadata": {},
   "source": [
    "**Merge insider trades with GKX data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cda878",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge all but is_ppn and is_ssn ### \n",
    "\n",
    "# Ensure datetime normalized and month end for both month columns in both dataframes\n",
    "merged_df[\"month\"] = pd.to_datetime(merged_df[\"month\"]).dt.normalize()\n",
    "insider_trades_final[\"month\"] = pd.to_datetime(insider_trades_final[\"month\"]).dt.normalize()\n",
    "\n",
    "# Merge\n",
    "merged_df_final = pd.merge(\n",
    "    merged_df,\n",
    "    insider_trades_final,\n",
    "    on=[\"permno\", \"cik\", \"month\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Create an assertion \n",
    "assert len(merged_df_final) == len(merged_df), \"Merged DataFrame length mismatch!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d63c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge is_ppn and is_ssn ###\n",
    "\n",
    "# Ensure datetime normalized and month end for both month columns in both dataframes\n",
    "merged_df_final[\"month\"] = pd.to_datetime(merged_df_final[\"month\"]).dt.normalize()\n",
    "insider_silence_monthly[\"month\"] = pd.to_datetime(insider_silence_monthly[\"month\"]).dt.normalize()\n",
    "\n",
    "# Merge\n",
    "merged_df_final = pd.merge(\n",
    "    merged_df_final,\n",
    "    insider_silence_monthly,\n",
    "    on=[\"cik\", \"month\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Create an assertion \n",
    "assert len(merged_df_final) == len(merged_df), \"Merged DataFrame length mismatch!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d539ae4a",
   "metadata": {},
   "source": [
    "**Rank transformation of stock characteristics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbdae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill na for dummies\n",
    "dummy_cols = [\n",
    "    \"is_opp_buy\", \"is_opp_sell\", \n",
    "    \"is_rtn_buy\", \"is_rtn_sell\", \n",
    "    \"is_ppn\", \"is_ssn\",\n",
    "    \n",
    "    \"is_txn_purchase_x_is_tit_ceo\",\n",
    "    \"is_txn_purchase_x_is_tit_cfo\",\n",
    "    \"is_txn_purchase_x_is_tit_coo\",\n",
    "    \"is_txn_purchase_x_is_tit_director\",\n",
    "    \"is_txn_purchase_x_is_tit_other_officer\",\n",
    "    \"is_txn_purchase_x_is_tit_ten_percent_owner\",\n",
    "    \"is_txn_purchase_x_is_tit_vice_president\",\n",
    "    \n",
    "    \"is_txn_sell_x_is_tit_ceo\",\n",
    "    \"is_txn_sell_x_is_tit_cfo\",\n",
    "    \"is_txn_sell_x_is_tit_coo\",\n",
    "    \"is_txn_sell_x_is_tit_director\",\n",
    "    \"is_txn_sell_x_is_tit_other_officer\",\n",
    "    \"is_txn_sell_x_is_tit_ten_percent_owner\",\n",
    "    \"is_txn_sell_x_is_tit_vice_president\",\n",
    "]\n",
    "\n",
    "# Rename dummies\n",
    "for col in dummy_cols:\n",
    "    if col in merged_df_final.columns:\n",
    "        merged_df_final[col] = merged_df_final[col].fillna(0).astype(\"float32\")\n",
    "        \n",
    "# Assert that dummy cols does not have any missing values\n",
    "for col in dummy_cols:\n",
    "    if col in merged_df_final.columns:\n",
    "        assert merged_df_final[col].isna().sum() == 0, f\"Missing values found in column {col}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e67ed87",
   "metadata": {},
   "source": [
    "Rank transform stock characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a72ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rank transform ###\n",
    "\n",
    "# Function for ranking\n",
    "def rank_transform(x):\n",
    "    \"\"\"\n",
    "    Rank transform function equivalent to the R version.\n",
    "    Scales values to range from -1 to 1.\n",
    "    \"\"\"\n",
    "    # Get ranks\n",
    "    rank_x = x.rank(axis=0, method='average', na_option='keep') # R's \"average\" method is equivalent to pandas' \"average\"; keep ensures that NA value are not ranked\n",
    "\n",
    "    # Count non-NA values\n",
    "    max_rank = x.notna().sum() # Counts number of non-NA values\n",
    "    min_rank = 1\n",
    "\n",
    "    if max_rank <= 1:\n",
    "        return pd.Series(np.nan, index=x.index) # All values are NA\n",
    "    else:\n",
    "        return 2 * (((rank_x - min_rank) / (max_rank - min_rank)) - 0.5) # Scale to [-1, 1]\n",
    "\n",
    "# List of characteristic columns\n",
    "char_cols = [col for col in merged_df_final.columns if col.startswith(\"char_\")] + [col for col in merged_df_final.columns if (col.startswith(\"is_npr_\") or col.startswith(\"is_net_cluster\"))]\n",
    "\n",
    "# Apply rank transformation by month\n",
    "merged_df_final[char_cols] = merged_df_final.groupby(\"month\")[char_cols].transform(rank_transform)\n",
    "\n",
    "# # Display the first couple of rows\n",
    "# display(merged_df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02abe727",
   "metadata": {},
   "source": [
    "Two-step procedure: \n",
    "1. Replace missing values with cross-sectional median. \n",
    "2. If cross-sectional median is undefined → replace with zero which for large and diverse datasets goes to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7883925",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Handle missing values ### \n",
    "\n",
    "# Select characteristic columns\n",
    "char_cols = [col for col in merged_df_final.columns if col.startswith(\"char_\")] + [col for col in merged_df_final.columns if (col.startswith(\"is_npr_\") or col.startswith(\"is_net_cluster\"))]\n",
    "\n",
    "# Step 1: fill NAs with within-month median\n",
    "merged_df_final[char_cols] = (\n",
    "    merged_df_final.groupby(\"month\")[char_cols]\n",
    "    .transform(lambda x: x.fillna(x.median() if x.notna().any() else np.nan))\n",
    ")\n",
    "\n",
    "# Step 2: replace any leftover NAs with 0\n",
    "merged_df_final[char_cols] = merged_df_final[char_cols].fillna(0)\n",
    "\n",
    "# # Display the first couple of rows\n",
    "# display(merged_df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5217971",
   "metadata": {},
   "source": [
    "**Create interaction terms and one-hot encode industry classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057af4c7",
   "metadata": {},
   "source": [
    "Create interaction terms between macro predictors and firm characteristics. \n",
    "\n",
    "Note that the macro predictors are not standardized:\n",
    "\n",
    "If you choose to standardize the macro predictors before creating interaction terms, this must be done using the training data only to avoid look-ahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d975ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create interaction terms ###\n",
    "\n",
    "# Identify characteristic and macro columns\n",
    "char_cols = [col for col in merged_df_final.columns if col.startswith(\"char_\")] + [col for col in merged_df_final.columns if (col.startswith(\"is_npr_\") or col.startswith(\"is_net_cluster\"))]\n",
    "macro_cols = [col for col in merged_df_final.columns if col.startswith(\"macro_\")]\n",
    "\n",
    "# Create interaction terms using dictionary comprehension\n",
    "interaction_terms = {\n",
    "    f\"inter_{char}_x_{macro}\": merged_df_final[char].values * merged_df_final[macro].values\n",
    "    for char in char_cols\n",
    "    for macro in macro_cols\n",
    "}\n",
    "\n",
    "# Turn interaction terms into a DataFrame\n",
    "interaction_df = pd.DataFrame(interaction_terms, index=merged_df_final.index)\n",
    "\n",
    "# Concat interaction terms to the original DataFrame\n",
    "merged_df_final = pd.concat([merged_df_final, interaction_df], axis=1)\n",
    "\n",
    "# # Drop macro predictors as they are now represented in the interaction terms\n",
    "# merged_df_final = merged_df_final.drop(columns=macro_cols)\n",
    "\n",
    "# # Display the first couple of rows\n",
    "# display(merged_df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5653a872",
   "metadata": {},
   "source": [
    "One-hot encode industry classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce091796",
   "metadata": {},
   "outputs": [],
   "source": [
    "### One-hot encode sic2 using float32 ###\n",
    "\n",
    "# Downcast if pd allows it\n",
    "merged_df_final = convert_formats(merged_df_final)\n",
    "\n",
    "# One-hot encode sic2 using float32\n",
    "merged_df_final = pd.get_dummies(\n",
    "    merged_df_final,\n",
    "    columns=[\"sic2\"],\n",
    "    prefix=\"sic2\",           # Prefix to seperate from dummies not adhering to insider trading\n",
    "    drop_first=True,         # Drop first to avoid dummy variable trap - the dummies are mutually exclusive\n",
    "    dtype=\"float32\",\n",
    "    sparse=False             # Use dense format to save as parquet file\n",
    ")\n",
    "\n",
    "# # Display the first couple of rows\n",
    "# display(merged_df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acbc082",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set insider trading variables to last ###\n",
    "\n",
    "# Get columns: other and insider\n",
    "insider_cols = [col for col in merged_df_final.columns if col.startswith(\"is_\") or col.startswith(\"inter_is_\")]\n",
    "other_cols = [col for col in merged_df_final.columns if col not in insider_cols]\n",
    "\n",
    "# Reorder columns\n",
    "final_cols = other_cols + insider_cols\n",
    "\n",
    "# Final DataFrame with reordered columns\n",
    "merged_df_final = merged_df_final[final_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4174eac6",
   "metadata": {},
   "source": [
    "**Dropping columns with p > 0.05 from DML test**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625b0dee",
   "metadata": {},
   "source": [
    "The columns are not dropped earlier to ensure that all variables are available if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop insignificant columns from DML ###\n",
    "\n",
    "# Columns to drop\n",
    "cols_to_drop = [\n",
    "    \"is_txn_sell_x_is_tit_vice_president\",\n",
    "    \"is_txn_purchase_x_is_tit_ten_percent_owner\",\n",
    "    \"is_ppn\",\n",
    "    \"is_ssn\",\n",
    "    \"is_txn_sell_x_is_tit_cfo\",\n",
    "    \"is_txn_sell_x_is_tit_director\",\n",
    "    \"is_txn_sell_x_is_tit_other_officer\",\n",
    "    \"is_txn_sell_x_is_tit_coo\",\n",
    "] + [col for col in merged_df_final.columns if \"npr_count\" in col]\n",
    "\n",
    "# Drop columns if they exist\n",
    "existing = [col for col in cols_to_drop if col in merged_df_final.columns]\n",
    "merged_df_final = merged_df_final.drop(columns=existing)\n",
    "\n",
    "# # Display the first couple of rows\n",
    "# display(merged_df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214e371",
   "metadata": {},
   "source": [
    "**Save final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a5495",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assert that len after dropping columns is the same as not dropping ###\n",
    "assert len(merged_df_final) == len(merged_df_final.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383996c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save col_list for later use ###\n",
    "\n",
    "insider_cols = [col for col in merged_df_final.columns \n",
    "                if col.startswith(\"is_\") or col.startswith(\"inter_is_\")]\n",
    "\n",
    "gkx_cols = [col for col in merged_df_final.columns \n",
    "            if col.startswith(\"char_\")\n",
    "            or col.startswith(\"sic2_\") \n",
    "            or (col.startswith(\"inter_\") and not col.startswith(\"inter_is_\"))]\n",
    "\n",
    "info_cols = [col for col in merged_df_final.columns \n",
    "             if col not in insider_cols + gkx_cols]\n",
    "\n",
    "\n",
    "# Base path for output files\n",
    "base_path = \"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/Characteristics Prepared/With Outsider/\"\n",
    "\n",
    "\n",
    "# Save insider_cols\n",
    "with open(base_path + \"insider_cols.txt\", \"w\") as f:\n",
    "    for col in insider_cols:\n",
    "        f.write(f\"{col}\\n\")\n",
    "\n",
    "# Save gkx_cols\n",
    "with open(base_path + \"gkx_cols.txt\", \"w\") as f:\n",
    "    for col in gkx_cols:\n",
    "        f.write(f\"{col}\\n\")\n",
    "\n",
    "# Save info_cols\n",
    "with open(base_path + \"info_cols.txt\", \"w\") as f:\n",
    "    for col in info_cols:\n",
    "        f.write(f\"{col}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c6b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save prepared data ###\n",
    "\n",
    "# Path\n",
    "path = \"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/Characteristics Prepared/With Outsider/with_outsider.parquet\" # Update this path to your desired output folder\n",
    "\n",
    "# Save to parquet\n",
    "merged_df_final.to_parquet(\n",
    "    path,\n",
    "    engine=\"pyarrow\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18927d1e",
   "metadata": {},
   "source": [
    "For yearly datasets - if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34825a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Save prepared data ###\n",
    "\n",
    "# # Base output path\n",
    "# BASE_PATH = Path(\n",
    "#     \"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofCopenhagen/Kandidat/Thesis/2.0 Data/Characteristics Prepared\" # Update this path to your desired output folder\n",
    "# )\n",
    "\n",
    "# # Extract year from 'month'\n",
    "# merged_df_final[\"year\"] = pd.to_datetime(merged_df_final[\"month\"]).dt.year\n",
    "\n",
    "# # Save as partitioned Parquet dataset\n",
    "# path = BASE_PATH / \"final_partitioned\"\n",
    "# merged_df_final.to_parquet(\n",
    "#     path,\n",
    "#     partition_cols=[\"year\"],\n",
    "#     engine=\"pyarrow\",\n",
    "#     index=False\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
