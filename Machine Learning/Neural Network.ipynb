{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb4c7e00",
   "metadata": {},
   "source": [
    "# 0. Preliminiary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b4655",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas matplotlib seaborn scikit-learn tensorflow keras-tuner tqdm fastparquet --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7012e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e13fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding_window_split(\n",
    "    df: pd.DataFrame,\n",
    "    train_size: int,\n",
    "    val_size: int,\n",
    "    test_size: int,\n",
    "    step_size: int,\n",
    "    start_date: str | None = None,\n",
    "    end_date: str | None = None,\n",
    "):\n",
    "    \"\"\"Expanding window split for time series data.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing a 'month' column in datetime format.\n",
    "        train_size (int): Number of months to include in the training set.\n",
    "        val_size (int): Number of months to include in the validation set.\n",
    "        test_size (int): Number of months to include in the test set.\n",
    "        step_size (int): Number of months to step forward for each iteration.\n",
    "        start_date (str | None, optional): Start date for the data split. Defaults to None.\n",
    "        end_date (str | None, optional): End date for the data split. Defaults to None.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If 'month' column is not in datetime format.\n",
    "\n",
    "    Yields:\n",
    "        train, val, test (pd.DataFrame): DataFrames containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure 'month' column is in datetime format\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[\"month\"]):\n",
    "        raise TypeError(\"'month' column must be in datetime format\")\n",
    "\n",
    "    # Apply date filters if provided\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    \n",
    "    if start_date:\n",
    "        mask &= df[\"month\"] >= pd.Timestamp(start_date)\n",
    "    if end_date:\n",
    "        mask &= df[\"month\"] <= pd.Timestamp(end_date)\n",
    "\n",
    "    months = sorted(df.loc[mask, \"month\"].unique())\n",
    "\n",
    "    # Set end index\n",
    "    end_idx = train_size + val_size + test_size + 1\n",
    "    \n",
    "    # Create a while loop to iterate until the end index exceeds the number of unique months\n",
    "    while end_idx <= len(months):\n",
    "        train_months = months[: end_idx - (val_size + test_size)]\n",
    "        val_months   = months[end_idx - (val_size + test_size) : end_idx - test_size]\n",
    "        test_months  = months[end_idx - test_size : end_idx]\n",
    "\n",
    "        # Slice firm-month panel\n",
    "        train = df[df[\"month\"].isin(train_months)]\n",
    "        val = df[df[\"month\"].isin(val_months)]\n",
    "        test = df[df[\"month\"].isin(test_months)]\n",
    "\n",
    "        # Stream one result at a time\n",
    "        yield train, val, test\n",
    "\n",
    "        # Expand by step_size months\n",
    "        end_idx += step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff6d61",
   "metadata": {},
   "source": [
    "# 2.0 Neural Network Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4023513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Silence TensorFlow CUDA and CPU feature logs\n",
    "# ------------------------------------------------------------\n",
    "import os, logging, absl.logging\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Hide INFO/WARN/ERROR logs from TensorFlow\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b788f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L1\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt\n",
    "\n",
    "# ===== CPU / Threading =====\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"96\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"96\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"96\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"96\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"96\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"96\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"8\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "tf.config.threading.set_intra_op_parallelism_threads(96)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(8)\n",
    "\n",
    "# ===== Global hyperparameters =====\n",
    "BEST_L1_GLOBAL = None\n",
    "BEST_LR_GLOBAL = None\n",
    "\n",
    "\n",
    "def neural_network(\n",
    "    df,\n",
    "    target,\n",
    "    features=None,\n",
    "    use_all_features=False,\n",
    "    start_year=None,\n",
    "    end_year=None,\n",
    "    train_size=2,\n",
    "    val_size=2,\n",
    "    test_size=1,\n",
    "    step_size=1,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    lr=[0.0001, 0.001, 0.01, 0.1],\n",
    "    l1_reg=[1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    hidden_depth=3,\n",
    "    base_width=32,\n",
    "    ensemble=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Rolling expanding-window NN with monitoring for losses and tuning.\n",
    "    \"\"\"\n",
    "    global BEST_L1_GLOBAL, BEST_LR_GLOBAL\n",
    "\n",
    "    # ---------- Prepare df ----------\n",
    "    df = df.copy()\n",
    "    df[\"month\"] = pd.to_datetime(df[\"month\"], errors=\"coerce\")\n",
    "\n",
    "    # Year filter\n",
    "    if (start_year is not None) and (end_year is not None):\n",
    "        df = df[\n",
    "            (df[\"month\"].dt.year >= start_year)\n",
    "            & (df[\"month\"].dt.year <= end_year)\n",
    "        ].copy()\n",
    "        print(f\"Using data from {start_year} to {end_year}\")\n",
    "\n",
    "    # ---------- Features ----------\n",
    "    if use_all_features:\n",
    "        features = [\n",
    "            c\n",
    "            for c in df.columns\n",
    "            if c not in [\n",
    "                \"month\",\n",
    "                \"cik\",\n",
    "                \"permno\",\n",
    "                target,\n",
    "                \"prc\",\n",
    "                \"shrout\",\n",
    "                \"mktcap\",\n",
    "            ]\n",
    "        ]\n",
    "        print(f\"Using all {len(features)} features.\")\n",
    "    elif not features:\n",
    "        raise ValueError(\"Specify 'features' or set use_all_features=True.\")\n",
    "\n",
    "    df[features] = df[features].astype(\"float32\", copy=False)\n",
    "    df[target]   = df[target].astype(\"float32\", copy=False)\n",
    "\n",
    "    # ---------- Architecture ----------\n",
    "    def make_layers(depth, base=base_width):\n",
    "        return [max(2, base // (2**i)) for i in range(depth)]\n",
    "\n",
    "    layer_config = make_layers(hidden_depth)\n",
    "    print(f\"NN{hidden_depth} architecture: {layer_config}\")\n",
    "\n",
    "    # ---------- Model builder for tuner ----------\n",
    "    def build_model(hp):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(len(features),)))\n",
    "        for units in layer_config:\n",
    "            model.add(\n",
    "                Dense(\n",
    "                    units,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_regularizer=L1(hp.Choice(\"l1_reg\", l1_reg)),\n",
    "                    use_bias=False\n",
    "                )\n",
    "            )\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=hp.Choice(\"lr\", lr)),\n",
    "            loss=\"mse\",\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    # ---------- Containers ----------\n",
    "    y_fits, y_tests, dates, X_tests = [], [], [], []\n",
    "    dic_r2_all = {}\n",
    "    ensemble_weights = []\n",
    "    \n",
    "    # Containers for monitoring\n",
    "    window_histories = {} \n",
    "    tuning_results = []   \n",
    "\n",
    "    last_trval = None\n",
    "\n",
    "    # ---------- Rolling loop ----------\n",
    "    for window_id, (train, val, test) in enumerate(\n",
    "        tqdm(\n",
    "            expanding_window_split(\n",
    "                df=df,\n",
    "                train_size=train_size,\n",
    "                val_size=val_size,\n",
    "                test_size=test_size,\n",
    "                step_size=step_size,\n",
    "                start_date=f\"{start_year}-01-01\" if start_year else None,\n",
    "                end_date=f\"{end_year}-12-31\" if end_year else None,\n",
    "            ),\n",
    "            desc=f\"Rolling Neural Network NN{hidden_depth} windows\",\n",
    "            unit=\"window\",\n",
    "        ),\n",
    "        start=1,\n",
    "    ):\n",
    "        if test.empty:\n",
    "            continue\n",
    "\n",
    "        # ----- Split arrays -----\n",
    "        X_train, y_train = (\n",
    "            train[features].to_numpy(\"float32\"),\n",
    "            train[target].to_numpy(\"float32\"),\n",
    "        )\n",
    "        X_val, y_val = (\n",
    "            val[features].to_numpy(\"float32\"),\n",
    "            val[target].to_numpy(\"float32\"),\n",
    "        )\n",
    "        X_test, y_test = (\n",
    "            test[features].to_numpy(\"float32\"),\n",
    "            test[target].to_numpy(\"float32\"),\n",
    "        )\n",
    "\n",
    "        # >>> CHANGED: keep extra columns for saving later <<<\n",
    "        cols_for_output = [\n",
    "            \"permno\",\n",
    "            \"month\",\n",
    "            \"cik\",\n",
    "            \"ret_excess\",\n",
    "            \"prc\",\n",
    "            \"shrout\",\n",
    "            \"mktcap_lag\",\n",
    "            \"mktcap\",\n",
    "            \"macro_dp\",\n",
    "            \"macro_ep\",\n",
    "            \"macro_bm\",\n",
    "            \"macro_ntis\",\n",
    "            \"macro_tbl\",\n",
    "            \"macro_tms\",\n",
    "            \"macro_dfy\",\n",
    "            \"macro_svar\",\n",
    "        ] + features\n",
    "        cols_for_output = [c for c in cols_for_output if c in test.columns]\n",
    "        X_tests.append(test[cols_for_output].copy())\n",
    "\n",
    "        # ----- Hyperparameter tuning on first window only -----\n",
    "        if (BEST_L1_GLOBAL is None) or (BEST_LR_GLOBAL is None):\n",
    "            print(f\"\\n--- Tuning Hyperparameters on Window {window_id} ---\")\n",
    "            train_ds = (\n",
    "                tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                .batch(batch_size)\n",
    "                .prefetch(tf.data.AUTOTUNE)\n",
    "            )\n",
    "            val_ds = (\n",
    "                tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "                .batch(batch_size)\n",
    "                .prefetch(tf.data.AUTOTUNE)\n",
    "            )\n",
    "\n",
    "            tuner = kt.GridSearch(\n",
    "                build_model,\n",
    "                objective=\"val_loss\",\n",
    "                max_trials=len(l1_reg) * len(lr),\n",
    "                directory=\"tuning_logs\",\n",
    "                project_name=f\"NN_tune_window_{window_id}\",\n",
    "                overwrite=True\n",
    "            )\n",
    "            \n",
    "            tuner.search(\n",
    "                train_ds,\n",
    "                validation_data=val_ds,\n",
    "                epochs=min(epochs, 50),\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            # Capture all trials to see how parameters were chosen\n",
    "            print(\"Tuning Results (Val Loss):\")\n",
    "            for trial in tuner.oracle.get_best_trials(num_trials=len(l1_reg) * len(lr)):\n",
    "                hps = trial.hyperparameters.values\n",
    "                score = trial.score\n",
    "                print(f\"  LR: {hps['lr']:.4f}, L1: {hps['l1_reg']:.5f} -> Loss: {score:.5f}\")\n",
    "                tuning_results.append({\n",
    "                    \"lr\": hps['lr'], \n",
    "                    \"l1_reg\": hps['l1_reg'], \n",
    "                    \"val_loss\": score,\n",
    "                    \"window\": window_id\n",
    "                })\n",
    "\n",
    "            best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "            BEST_L1_GLOBAL = best_hp.get(\"l1_reg\")\n",
    "            BEST_LR_GLOBAL = best_hp.get(\"lr\")\n",
    "            print(f\"Selected Best: λ₁={BEST_L1_GLOBAL}, LR={BEST_LR_GLOBAL}\\n\")\n",
    "\n",
    "        # ----- Wrap best HPs -----\n",
    "        class _BestHPWrapper:\n",
    "            def get(self, key):\n",
    "                if key == \"l1_reg\": return BEST_L1_GLOBAL\n",
    "                if key == \"lr\": return BEST_LR_GLOBAL\n",
    "                raise KeyError(key)\n",
    "\n",
    "        best_hp = _BestHPWrapper()\n",
    "\n",
    "        # ----- Combine TRAIN + VALIDATION -----\n",
    "        X_trval = np.concatenate([X_train, X_val], axis=0)\n",
    "        y_trval = np.concatenate([y_train, y_val], axis=0)\n",
    "        last_trval = (X_trval, y_trval)\n",
    "\n",
    "        # ----- Ensemble training -----\n",
    "        def train_one(seed, X_trval, y_trval, X_test, best_hp):\n",
    "            import tensorflow as tf\n",
    "            from tensorflow.keras import Sequential\n",
    "            from tensorflow.keras.layers import Dense, BatchNormalization, Input\n",
    "            from tensorflow.keras.optimizers import Adam\n",
    "            from tensorflow.keras.regularizers import L1\n",
    "            from tensorflow.keras.callbacks import EarlyStopping\n",
    "            import gc as _gc\n",
    "\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(Input(shape=(X_trval.shape[1],)))\n",
    "            for units in layer_config:\n",
    "                model.add(\n",
    "                    Dense(\n",
    "                        units,\n",
    "                        activation=\"relu\",\n",
    "                        kernel_regularizer=L1(best_hp.get(\"l1_reg\")),\n",
    "                        use_bias=False\n",
    "                    )\n",
    "                )\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Dense(1, activation=\"linear\"))\n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=best_hp.get(\"lr\")),\n",
    "                loss=\"mse\",\n",
    "            )\n",
    "\n",
    "            early_stop = EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=5,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "\n",
    "            # Capture history object\n",
    "            history = model.fit(\n",
    "                X_trval,\n",
    "                y_trval,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.05,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            pred_fit = model.predict(X_trval, verbose=0).flatten()\n",
    "            pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "            weights = model.get_weights()\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "            _gc.collect()\n",
    "\n",
    "            return pred_fit, pred_test, weights, history.history\n",
    "\n",
    "        results = Parallel(\n",
    "            n_jobs=min(ensemble, 24),\n",
    "            backend=\"loky\",\n",
    "        )(\n",
    "            delayed(train_one)(12308 + e, X_trval, y_trval, X_test, best_hp)\n",
    "            for e in range(ensemble)\n",
    "        )\n",
    "\n",
    "        preds_fit_ensemble, preds_test_ensemble, weights_ensemble, histories_ensemble = zip(*results)\n",
    "        \n",
    "        # Save histories for this window\n",
    "        label = pd.to_datetime(test[\"month\"]).dt.strftime(\"%Y-%m\").iloc[0]\n",
    "        window_histories[label] = histories_ensemble\n",
    "\n",
    "        # ----- Average predictions & weights -----\n",
    "        y_pred_fit = np.mean(preds_fit_ensemble, axis=0)\n",
    "        y_pred_test = np.mean(preds_test_ensemble, axis=0)\n",
    "        avg_weights = [\n",
    "            np.mean(np.stack(layer_w, axis=0), axis=0)\n",
    "            for layer_w in zip(*weights_ensemble)\n",
    "        ]\n",
    "        ensemble_weights.append(avg_weights)\n",
    "\n",
    "        # ----- Evaluate -----\n",
    "        y_fits.append((y_trval, y_pred_fit))\n",
    "        y_tests.append((y_test, y_pred_test))\n",
    "\n",
    "        dates.append(label)\n",
    "\n",
    "        ss_res = np.sum((y_test - y_pred_test) ** 2)\n",
    "        ss_tot = np.sum(y_test ** 2)\n",
    "        r2 = 1 - ss_res / ss_tot\n",
    "        dic_r2_all[f\"r2.{label}\"] = r2\n",
    "\n",
    "        print(f\"[NN{hidden_depth}] Test period: {label} | Ensemble R² = {r2:.4f}\")\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    # ---------- Full-sample OOS R² ----------\n",
    "    y_test_all = np.concatenate([y for (y, _) in y_tests]) if y_tests else np.array([])\n",
    "    y_pred_all = np.concatenate([yhat for (_, yhat) in y_tests]) if y_tests else np.array([])\n",
    "\n",
    "    R2OOS_Full = (\n",
    "        1 - np.sum((y_test_all - y_pred_all) ** 2) / np.sum(y_test_all ** 2)\n",
    "        if y_test_all.size > 0\n",
    "        else np.nan\n",
    "    )\n",
    "\n",
    "    print(\"\\n===========================================\")\n",
    "    print(f\"Full Out-of-Sample R²: {R2OOS_Full:.4f}\")\n",
    "    print(\"===========================================\\n\")\n",
    "\n",
    "    # =========================================================\n",
    "    # [FIX] Train 'last_model' on the final window for saving\n",
    "    # =========================================================\n",
    "    print(\"Training final model on the last window (for saving)...\")\n",
    "    last_model = Sequential()\n",
    "    last_model.add(Input(shape=(len(features),)))\n",
    "    for units in layer_config:\n",
    "        last_model.add(\n",
    "            Dense(\n",
    "                units,\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=L1(BEST_L1_GLOBAL),\n",
    "                use_bias=False\n",
    "            )\n",
    "        )\n",
    "        last_model.add(BatchNormalization())\n",
    "    last_model.add(Dense(1, activation=\"linear\"))\n",
    "    \n",
    "    last_model.compile(\n",
    "        optimizer=Adam(learning_rate=BEST_LR_GLOBAL),\n",
    "        loss=\"mse\",\n",
    "    )\n",
    "\n",
    "    if last_trval is not None:\n",
    "        X_trval_last, y_trval_last = last_trval\n",
    "        last_model.fit(\n",
    "            X_trval_last,\n",
    "            y_trval_last,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.05,\n",
    "            callbacks=[\n",
    "                EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "            ],\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "    # [FIX] Added 'model': last_model to return dictionary\n",
    "    return {\n",
    "        \"R2_window\": dic_r2_all,\n",
    "        \"R2_full\": R2OOS_Full,\n",
    "        \"y_fits\": y_fits,\n",
    "        \"y_tests\": y_tests,\n",
    "        \"X_tests\": X_tests,\n",
    "        \"dates\": dates,\n",
    "        \"ensemble_weights\": ensemble_weights,\n",
    "        \"tuning_results\": tuning_results,  \n",
    "        \"window_histories\": window_histories,\n",
    "        \"model\": last_model \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60528cfd",
   "metadata": {},
   "source": [
    "# Neural Network Without Insider Trading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2d2e6",
   "metadata": {},
   "source": [
    "## 1.1 NN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c04b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. Load Feature List from Text File\n",
    "# ======================================================\n",
    "# Updated path with correct extension (.txt) and prefix\n",
    "feature_file = \"/work/Thesis/Data/gkx_cols.txt\"\n",
    "\n",
    "with open(feature_file, \"r\") as f:\n",
    "    baseline = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Successfully loaded {len(baseline)}\")\n",
    "\n",
    "# ======================================================\n",
    "# 2. Run Neural Network\n",
    "# ======================================================\n",
    "path = \"/work/Thesis/Data/finalized_true.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "res_nn1 = neural_network(\n",
    "    df=df,\n",
    "    features=baseline,        \n",
    "    use_all_features=False,\n",
    "    target=\"ret_excess\",\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    epochs=100,\n",
    "    batch_size=10000,\n",
    "    lr=[0.001, 0.01],\n",
    "    l1_reg=[1e-5, 1e-3],\n",
    "    hidden_depth=1,\n",
    "    ensemble=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1609ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten & save data\n",
    "y_true, y_pred = map(np.concatenate, zip(*res_nn1[\"y_tests\"]))\n",
    "x_all = pd.concat(res_nn1[\"X_tests\"], ignore_index=True)\n",
    "\n",
    "# >>> CHANGED: output columns & names for nn1_output.parquet <<<\n",
    "pd.DataFrame({\n",
    "    \"month\":           pd.to_datetime(x_all[\"month\"]),\n",
    "    \"cik\":             x_all[\"cik\"],\n",
    "    \"permno\":          x_all[\"permno\"],\n",
    "    \"ret_excess\":      y_true,\n",
    "    \"prc\":             x_all[\"prc\"],\n",
    "    \"shrout\":          x_all[\"shrout\"],\n",
    "    \"mktcap_lag\":      x_all[\"mktcap_lag\"],\n",
    "    \"pred_ret_excess\": y_pred,\n",
    "}).to_parquet(\"nn1_output.parquet\", index=False)\n",
    "\n",
    "# save model & weights\n",
    "res_nn1[\"model\"].save(\"nn1_model.keras\")\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"nn1_ensemble_weights.npz\", \n",
    "    weights=np.array(res_nn1[\"ensemble_weights\"], dtype=object), \n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "# save r2 metrics\n",
    "with open(\"nn1_r2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"r2_full\": float(res_nn1[\"R2_full\"]),\n",
    "        \"r2_window\": {k: float(v) for k, v in res_nn1[\"R2_window\"].items()}\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"saved: nn1_output.parquet, nn1_model.keras, weights, and r2 json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd13b7",
   "metadata": {},
   "source": [
    "## 1.2 NN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. Load Feature List from Text File\n",
    "# ======================================================\n",
    "# Updated path with correct extension (.txt) and prefix\n",
    "feature_file = \"/work/Thesis/Data/gkx_cols.txt\"\n",
    "\n",
    "with open(feature_file, \"r\") as f:\n",
    "    baseline = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Successfully loaded {len(baseline)}\")\n",
    "\n",
    "# ======================================================\n",
    "# 2. Run Neural Network\n",
    "# ======================================================\n",
    "path = \"/work/Thesis/Data/finalized_true.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "res_nn2 = neural_network(\n",
    "    df=df,\n",
    "    features=baseline,        \n",
    "    use_all_features=False,\n",
    "    target=\"ret_excess\",\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    epochs=100,\n",
    "    batch_size=10000,\n",
    "    lr=[0.001, 0.01],\n",
    "    l1_reg=[1e-5, 1e-3],\n",
    "    hidden_depth=2,\n",
    "    ensemble=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df5046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten & save data\n",
    "y_true, y_pred = map(np.concatenate, zip(*res_nn2[\"y_tests\"]))\n",
    "x_all = pd.concat(res_nn2[\"X_tests\"], ignore_index=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"month\":           pd.to_datetime(x_all[\"month\"]),\n",
    "    \"cik\":             x_all[\"cik\"],\n",
    "    \"permno\":          x_all[\"permno\"],\n",
    "    \"ret_excess\":      y_true,\n",
    "    \"prc\":             x_all[\"prc\"],\n",
    "    \"shrout\":          x_all[\"shrout\"],\n",
    "    \"mktcap_lag\":      x_all[\"mktcap_lag\"],\n",
    "    \"pred_ret_excess\": y_pred,\n",
    "}).to_parquet(\"nn2_output.parquet\", index=False)\n",
    "\n",
    "# save model & weights\n",
    "res_nn2[\"model\"].save(\"nn2_model.keras\")\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"nn2_ensemble_weights.npz\", \n",
    "    weights=np.array(res_nn2[\"ensemble_weights\"], dtype=object), \n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "# save r2 metrics\n",
    "with open(\"nn2_r2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"r2_full\": float(res_nn2[\"R2_full\"]),\n",
    "        \"r2_window\": {k: float(v) for k, v in res_nn2[\"R2_window\"].items()}\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"saved: nn2_output.parquet, nn2_model.keras, weights, and r2 json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2772dd3",
   "metadata": {},
   "source": [
    "## 1.3 NN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e751b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. Load Feature List from Text File\n",
    "# ======================================================\n",
    "# Updated path with correct extension (.txt) and prefix\n",
    "feature_file = \"/work/Thesis/Data/gkx_cols.txt\"\n",
    "\n",
    "with open(feature_file, \"r\") as f:\n",
    "    baseline = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Successfully loaded {len(baseline)}\")\n",
    "\n",
    "# ======================================================\n",
    "# 2. Run Neural Network\n",
    "# ======================================================\n",
    "path = \"/work/Thesis/Data/finalized_true.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "res_nn3 = neural_network(\n",
    "    df=df,\n",
    "    features=baseline,        \n",
    "    use_all_features=False,\n",
    "    target=\"ret_excess\",\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    epochs=100,\n",
    "    batch_size=10000,\n",
    "    lr=[0.001, 0.01],\n",
    "    l1_reg=[1e-5, 1e-3],\n",
    "    hidden_depth=3,\n",
    "    ensemble=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfc6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten & save data\n",
    "y_true, y_pred = map(np.concatenate, zip(*res_nn3[\"y_tests\"]))\n",
    "x_all = pd.concat(res_nn3[\"X_tests\"], ignore_index=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"month\":           pd.to_datetime(x_all[\"month\"]),\n",
    "    \"cik\":             x_all[\"cik\"],\n",
    "    \"permno\":          x_all[\"permno\"],\n",
    "    \"ret_excess\":      y_true,\n",
    "    \"prc\":             x_all[\"prc\"],\n",
    "    \"shrout\":          x_all[\"shrout\"],\n",
    "    \"mktcap_lag\":      x_all[\"mktcap_lag\"],\n",
    "    \"pred_ret_excess\": y_pred,\n",
    "}).to_parquet(\"nn3_output.parquet\", index=False)\n",
    "\n",
    "# save model & weights\n",
    "res_nn3[\"model\"].save(\"nn3_model.keras\")\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"nn3_ensemble_weights.npz\", \n",
    "    weights=np.array(res_nn3[\"ensemble_weights\"], dtype=object), \n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "# save r2 metrics\n",
    "with open(\"nn3_r2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"r2_full\": float(res_nn3[\"R2_full\"]),\n",
    "        \"r2_window\": {k: float(v) for k, v in res_nn3[\"R2_window\"].items()}\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"saved: nn3_output.parquet, nn3_model.keras, weights, and r2 json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd37394",
   "metadata": {},
   "source": [
    "## 1.4 NN4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. Load Feature List from Text File\n",
    "# ======================================================\n",
    "# Updated path with correct extension (.txt) and prefix\n",
    "feature_file = \"/work/Thesis/Data/gkx_cols.txt\"\n",
    "\n",
    "with open(feature_file, \"r\") as f:\n",
    "    baseline = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Successfully loaded {len(baseline)}\")\n",
    "\n",
    "# ======================================================\n",
    "# 2. Run Neural Network\n",
    "# ======================================================\n",
    "path = \"/work/Thesis/Data/finalized_true.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "res_nn4 = neural_network(\n",
    "    df=df,\n",
    "    features=baseline,        \n",
    "    use_all_features=False,\n",
    "    target=\"ret_excess\",\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    epochs=100,\n",
    "    batch_size=10000,\n",
    "    lr=[0.001, 0.01],\n",
    "    l1_reg=[1e-5, 1e-3],\n",
    "    hidden_depth=4,\n",
    "    ensemble=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. Flatten & Save Outputs\n",
    "# ======================================================\n",
    "y_true, y_pred = map(np.concatenate, zip(*res_nn4[\"y_tests\"]))\n",
    "x_all = pd.concat(res_nn4[\"X_tests\"], ignore_index=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"month\":           pd.to_datetime(x_all[\"month\"]),\n",
    "    \"cik\":             x_all[\"cik\"],\n",
    "    \"permno\":          x_all[\"permno\"],\n",
    "    \"ret_excess\":      y_true,\n",
    "    \"prc\":             x_all[\"prc\"],\n",
    "    \"shrout\":          x_all[\"shrout\"],\n",
    "    \"mktcap_lag\":      x_all[\"mktcap_lag\"],\n",
    "    \"pred_ret_excess\": y_pred,\n",
    "}).to_parquet(\"nn4_output.parquet\", index=False)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 2. Save NN4 Model\n",
    "# ======================================================\n",
    "res_nn4[\"model\"].save(\"nn4_model.keras\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 3. Save Ensemble Weights (ROBUST VERSION)\n",
    "#    - one key per ensemble member: w_0, w_1, ...\n",
    "# ======================================================\n",
    "ensemble = res_nn4[\"ensemble_weights\"]   # list of length n_ens; each item = list of 22 arrays\n",
    "\n",
    "weights_dict = {\n",
    "    f\"w_{i}\": np.array(w, dtype=object)  # store each model's 22 tensors as an object array\n",
    "    for i, w in enumerate(ensemble)\n",
    "}\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"nn4_ensemble_weights.npz\",\n",
    "    **weights_dict,\n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "print(f\"Saved {len(ensemble)} ensemble members, each with {len(ensemble[0])} weight tensors.\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 4. Save R² metrics\n",
    "# ======================================================\n",
    "with open(\"nn4_r2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"r2_full\": float(res_nn4[\"R2_full\"]),\n",
    "        \"r2_window\": {k: float(v) for k, v in res_nn4[\"R2_window\"].items()}\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Saved: nn4_output.parquet, nn4_model.keras, nn4_ensemble_weights.npz, nn4_r2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b94f51",
   "metadata": {},
   "source": [
    "## 1.5 NN5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3a5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. Load Feature List from Text File\n",
    "# ======================================================\n",
    "# Updated path with correct extension (.txt) and prefix\n",
    "feature_file = \"/work/Thesis/Data/gkx_cols.txt\"\n",
    "\n",
    "with open(feature_file, \"r\") as f:\n",
    "    baseline = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Successfully loaded {len(baseline)}\")\n",
    "\n",
    "# ======================================================\n",
    "# 2. Run Neural Network\n",
    "# ======================================================\n",
    "path = \"/work/Thesis/Data/finalized.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "res_nn5 = neural_network(\n",
    "    df=df,\n",
    "    features=baseline,        \n",
    "    use_all_features=False,\n",
    "    target=\"ret_excess\",\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    epochs=100,\n",
    "    batch_size=10000,\n",
    "    lr=[0.001, 0.01],\n",
    "    l1_reg=[1e-5, 1e-3],\n",
    "    hidden_depth=5,\n",
    "    ensemble=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55dcf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten & save data\n",
    "y_true, y_pred = map(np.concatenate, zip(*res_nn5[\"y_tests\"]))\n",
    "x_all = pd.concat(res_nn5[\"X_tests\"], ignore_index=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"permno\": x_all[\"permno\"].values,\n",
    "    \"month\":  pd.to_datetime(x_all[\"month\"]),\n",
    "    \"y_true\": y_true,\n",
    "    \"nn5_y_pred\": y_pred\n",
    "}).to_parquet(\"nn5_output.parquet\", index=False)\n",
    "\n",
    "# save model & weights\n",
    "res_nn5[\"model\"].save(\"nn5_model.keras\")\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"nn5_ensemble_weights.npz\", \n",
    "    weights=np.array(res_nn5[\"ensemble_weights\"], dtype=object), \n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "# save r2 metrics\n",
    "with open(\"nn5_r2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"r2_full\": float(res_nn5[\"R2_full\"]),\n",
    "        \"r2_window\": {k: float(v) for k, v in res_nn5[\"R2_window\"].items()}\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"saved: nn5_output.parquet, nn5_model.keras, weights, and r2 json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8852ef5",
   "metadata": {},
   "source": [
    "# Neural Network With Insider Trading (Insider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d0e35",
   "metadata": {},
   "source": [
    "## 2.1 NN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14630366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. Load Feature Lists from Text Files\n",
    "# ======================================================\n",
    "base_path = \"/work/Thesis/Data/3. Insider/\"\n",
    "\n",
    "# A. Load the Baseline (GKX) columns\n",
    "with open(base_path + \"gkx_cols.txt\", \"r\") as f:\n",
    "    baseline = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# B. Load the Reduced Insider columns\n",
    "with open(base_path + \"insider_cols.txt\", \"r\") as f:\n",
    "    insider_cols = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# ======================================================\n",
    "# 2. Combine the Lists\n",
    "# ======================================================\n",
    "combined_features = baseline + insider_cols\n",
    "\n",
    "print(f\"Baseline features: {len(baseline)}\")\n",
    "print(f\"Insider features:  {len(insider_cols)}\")\n",
    "print(f\"Total input size:  {len(combined_features)}\")\n",
    "\n",
    "# ======================================================\n",
    "# 4. Run Neural Network\n",
    "# ======================================================\n",
    "path = \"/work/Thesis/Data/3. Insider/with_insider.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "res_nn1_insider = neural_network(\n",
    "    df=df,\n",
    "    features=combined_features,   # <--- Uses the combined list\n",
    "    use_all_features=False,\n",
    "    target=\"ret_excess\",\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    epochs=100,\n",
    "    batch_size=10000,\n",
    "    lr=[0.001, 0.01],             # Expand ranges if you have time!\n",
    "    l1_reg=[1e-5, 1e-3],          \n",
    "    hidden_depth=1,               # Consider changing to 2 or 3 for interactions\n",
    "    ensemble=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten & save data\n",
    "y_true, y_pred = map(np.concatenate, zip(*res_nn1_insider[\"y_tests\"]))\n",
    "x_all = pd.concat(res_nn1_insider[\"X_tests\"], ignore_index=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"month\":           pd.to_datetime(x_all[\"month\"]),\n",
    "    \"cik\":             x_all[\"cik\"],\n",
    "    \"permno\":          x_all[\"permno\"],\n",
    "    \"ret_excess\":      y_true,\n",
    "    \"prc\":             x_all[\"prc\"],\n",
    "    \"shrout\":          x_all[\"shrout\"],\n",
    "    \"mktcap_lag\":      x_all[\"mktcap_lag\"],\n",
    "    \"pred_ret_excess\": y_pred,\n",
    "}).to_parquet(\"nn1_insider_output.parquet\", index=False)\n",
    "\n",
    "# save model & weights\n",
    "res_nn1_insider[\"model\"].save(\"nn1_insider_model.keras\")\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"nn1_insider_ensemble_weights.npz\", \n",
    "    weights=np.array(res_nn1_insider[\"ensemble_weights\"], dtype=object), \n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "# save r2 metrics\n",
    "with open(\"nn1_insider_r2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"r2_full\": float(res_nn1_insider[\"R2_full\"]),\n",
    "        \"r2_window\": {k: float(v) for k, v in res_nn1_insider[\"R2_window\"].items()}\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"saved: nn1_insider_output.parquet, nn1_insider_model.keras, weights, and r2 json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112fd50a",
   "metadata": {},
   "source": [
    "## 2.2 NN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. Load Feature Lists from Text Files\n",
    "# ======================================================\n",
    "base_path = \"/work/Thesis/Data/3. Insider/\"\n",
    "\n",
    "# A. Load the Baseline (GKX) columns\n",
    "with open(base_path + \"gkx_cols.txt\", \"r\") as f:\n",
    "    baseline = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# B. Load the Reduced Insider columns\n",
    "with open(base_path + \"insider_cols.txt\", \"r\") as f:\n",
    "    insider_cols = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# ======================================================\n",
    "# 2. Combine the Lists\n",
    "# ======================================================\n",
    "combined_features = baseline + insider_cols\n",
    "\n",
    "print(f\"Baseline features: {len(baseline)}\")\n",
    "print(f\"Insider features:  {len(insider_cols)}\")\n",
    "print(f\"Total input size:  {len(combined_features)}\")\n",
    "\n",
    "# ======================================================\n",
    "# 4. Run Neural Network\n",
    "# ======================================================\n",
    "path = \"/work/Thesis/Data/3. Insider/with_insider.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "res_nn2_insider = neural_network(\n",
    "    df=df,\n",
    "    features=combined_features,   # <--- Uses the combined list\n",
    "    use_all_features=False,\n",
    "    target=\"ret_excess\",\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    epochs=100,\n",
    "    batch_size=10000,\n",
    "    lr=[0.001, 0.01],             # Expand ranges if you have time!\n",
    "    l1_reg=[1e-5, 1e-3],          \n",
    "    hidden_depth=2,               # Consider changing to 2 or 3 for interactions\n",
    "    ensemble=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498bdbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten & save data\n",
    "y_true, y_pred = map(np.concatenate, zip(*res_nn2_insider[\"y_tests\"]))\n",
    "x_all = pd.concat(res_nn2_insider[\"X_tests\"], ignore_index=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"month\":           pd.to_datetime(x_all[\"month\"]),\n",
    "    \"cik\":             x_all[\"cik\"],\n",
    "    \"permno\":          x_all[\"permno\"],\n",
    "    \"ret_excess\":      y_true,\n",
    "    \"prc\":             x_all[\"prc\"],\n",
    "    \"shrout\":          x_all[\"shrout\"],\n",
    "    \"mktcap_lag\":      x_all[\"mktcap_lag\"],\n",
    "    \"pred_ret_excess\": y_pred,\n",
    "}).to_parquet(\"nn2_insider_output.parquet\", index=False)\n",
    "\n",
    "# save model & weights\n",
    "res_nn2_insider[\"model\"].save(\"nn2_insider_model.keras\")\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"nn2_insider_ensemble_weights.npz\", \n",
    "    weights=np.array(res_nn2_insider[\"ensemble_weights\"], dtype=object), \n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "# save r2 metrics\n",
    "with open(\"nn2_insider_r2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"r2_full\": float(res_nn2_insider[\"R2_full\"]),\n",
    "        \"r2_window\": {k: float(v) for k, v in res_nn2_insider[\"R2_window\"].items()}\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"saved: nn2_insider_output.parquet, nn2_insider_model.keras, weights, and r2 json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e3c968",
   "metadata": {},
   "source": [
    "## 2.3 NN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7305508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. Load Feature Lists from Text Files\n",
    "# ======================================================\n",
    "base_path = \"/work/Thesis/Data/3. Insider/\"\n",
    "\n",
    "# A. Load the Baseline (GKX) columns\n",
    "with open(base_path + \"gkx_cols.txt\", \"r\") as f:\n",
    "    baseline = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# B. Load the Reduced Insider columns\n",
    "with open(base_path + \"insider_cols.txt\", \"r\") as f:\n",
    "    insider_cols = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# ======================================================\n",
    "# 2. Combine the Lists\n",
    "# ======================================================\n",
    "combined_features = baseline + insider_cols\n",
    "\n",
    "print(f\"Baseline features: {len(baseline)}\")\n",
    "print(f\"Insider features:  {len(insider_cols)}\")\n",
    "print(f\"Total input size:  {len(combined_features)}\")\n",
    "\n",
    "# ======================================================\n",
    "# 4. Run Neural Network\n",
    "# ======================================================\n",
    "path = \"/work/Thesis/Data/3. Insider/with_insider.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "res_nn3_insider = neural_network(\n",
    "    df=df,\n",
    "    features=combined_features,   # <--- Uses the combined list\n",
    "    use_all_features=False,\n",
    "    target=\"ret_excess\",\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    epochs=100,\n",
    "    batch_size=10000,\n",
    "    lr=[0.001, 0.01],             # Expand ranges if you have time!\n",
    "    l1_reg=[1e-5, 1e-3],         \n",
    "    hidden_depth=3,               # Consider changing to 2 or 3 for interactions\n",
    "    ensemble=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten & save data\n",
    "y_true, y_pred = map(np.concatenate, zip(*res_nn3_insider[\"y_tests\"]))\n",
    "x_all = pd.concat(res_nn3_insider[\"X_tests\"], ignore_index=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"month\":           pd.to_datetime(x_all[\"month\"]),\n",
    "    \"cik\":             x_all[\"cik\"],\n",
    "    \"permno\":          x_all[\"permno\"],\n",
    "    \"ret_excess\":      y_true,\n",
    "    \"prc\":             x_all[\"prc\"],\n",
    "    \"shrout\":          x_all[\"shrout\"],\n",
    "    \"mktcap_lag\":      x_all[\"mktcap_lag\"],\n",
    "    \"pred_ret_excess\": y_pred,\n",
    "}).to_parquet(\"nn3_insider_output.parquet\", index=False)\n",
    "\n",
    "# save model & weights\n",
    "res_nn3_insider[\"model\"].save(\"nn3_insider_model.keras\")\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"nn3_insider_ensemble_weights.npz\", \n",
    "    weights=np.array(res_nn3_insider[\"ensemble_weights\"], dtype=object), \n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "# save r2 metrics\n",
    "with open(\"nn3_insider_r2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"r2_full\": float(res_nn3_insider[\"R2_full\"]),\n",
    "        \"r2_window\": {k: float(v) for k, v in res_nn3_insider[\"R2_window\"].items()}\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"saved: nn3_insider_output.parquet, nn3_insider_model.keras, weights, and r2 json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38092816",
   "metadata": {},
   "source": [
    "## 2.4 NN4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. Load Feature Lists from Text Files\n",
    "# ======================================================\n",
    "base_path = \"/work/Thesis/Data/3. Insider/\"\n",
    "\n",
    "# A. Load the Baseline (GKX) columns\n",
    "with open(base_path + \"gkx_cols.txt\", \"r\") as f:\n",
    "    baseline = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# B. Load the Reduced Insider columns\n",
    "with open(base_path + \"insider_cols.txt\", \"r\") as f:\n",
    "    insider_cols = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# ======================================================\n",
    "# 2. Combine the Lists\n",
    "# ======================================================\n",
    "combined_features = baseline + insider_cols\n",
    "\n",
    "print(f\"Baseline features: {len(baseline)}\")\n",
    "print(f\"Insider features:  {len(insider_cols)}\")\n",
    "print(f\"Total input size:  {len(combined_features)}\")\n",
    "\n",
    "# ======================================================\n",
    "# 4. Run Neural Network\n",
    "# ======================================================\n",
    "path = \"/work/Thesis/Data/3. Insider/with_insider.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "res_nn4_insider = neural_network(\n",
    "    df=df,\n",
    "    features=combined_features,   # <--- Uses the combined list\n",
    "    use_all_features=False,\n",
    "    target=\"ret_excess\",\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    epochs=100,\n",
    "    batch_size=10000,\n",
    "    lr=[0.001, 0.01],             # Expand ranges if you have time!\n",
    "    l1_reg=[1e-5, 1e-3],         \n",
    "    hidden_depth=4,               # Consider changing to 2 or 3 for interactions\n",
    "    ensemble=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb68f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW NEW ===================== NN4 (insider, non-lag): flatten & save data =====================\n",
    "y_true, y_pred = map(np.concatenate, zip(*res_nn4_insider[\"y_tests\"]))\n",
    "x_all = pd.concat(res_nn4_insider[\"X_tests\"], ignore_index=True)\n",
    "\n",
    "df_out = pd.DataFrame({\n",
    "    \"month\":           pd.to_datetime(x_all[\"month\"]),\n",
    "    \"cik\":             x_all[\"cik\"].values,\n",
    "    \"permno\":          x_all[\"permno\"].values,\n",
    "    \"ret_excess\":      y_true,\n",
    "    \"prc\":             x_all[\"prc\"].values,\n",
    "    \"shrout\":          x_all[\"shrout\"].values,\n",
    "    \"mktcap_lag\":      x_all[\"mktcap_lag\"].values,\n",
    "    \"pred_ret_excess\": y_pred\n",
    "}).to_parquet(\"nn4_insider_output.parquet\", index=False)\n",
    "\n",
    "# ===================== Save model =====================\n",
    "res_nn4_insider[\"model\"].save(\"nn4_insider_model.keras\")\n",
    "\n",
    "# ===================== Save ensemble weights (simple version) =====================\n",
    "np.savez_compressed(\n",
    "    \"nn4_insider_ensemble_weights.npz\",\n",
    "    weights=np.array(res_nn4_insider[\"ensemble_weights\"], dtype=object),\n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "# ===================== Save R² metrics =====================\n",
    "import json\n",
    "with open(\"nn4_insider_r2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"r2_full\": float(res_nn4_insider[\"R2_full\"]),\n",
    "        \"r2_window\": {k: float(v) for k, v in res_nn4_insider[\"R2_window\"].items()}\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"saved: nn4_insider_output.parquet, nn4_insider_model.keras, weights, and r2 json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04440d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. Load Feature Lists from Text Files\n",
    "# ======================================================\n",
    "base_path = \"/work/Thesis/Data/3. Insider/\"\n",
    "\n",
    "# A. Load the Baseline (GKX) columns\n",
    "with open(base_path + \"gkx_cols.txt\", \"r\") as f:\n",
    "    baseline = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# B. Load the Reduced Insider columns\n",
    "with open(base_path + \"insider_cols.txt\", \"r\") as f:\n",
    "    insider_cols = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# ======================================================\n",
    "# 2. Combine the Lists\n",
    "# ======================================================\n",
    "combined_features = baseline + insider_cols\n",
    "\n",
    "print(f\"Baseline features: {len(baseline)}\")\n",
    "print(f\"Insider features:  {len(insider_cols)}\")\n",
    "print(f\"Total input size:  {len(combined_features)}\")\n",
    "\n",
    "# ======================================================\n",
    "# 4. Run Neural Network\n",
    "# ======================================================\n",
    "path = \"/work/Thesis/Data/3. Insider/with_insider.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "res_nn4_insider = neural_network(\n",
    "    df=df,\n",
    "    features=combined_features,   # <--- Uses the combined list\n",
    "    use_all_features=False,\n",
    "    target=\"ret_excess\",\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    epochs=100,\n",
    "    batch_size=10000,\n",
    "    lr=[0.001, 0.01],             # Expand ranges if you have time!\n",
    "    l1_reg=[1e-5, 1e-3],         \n",
    "    hidden_depth=5,               # Consider changing to 2 or 3 for interactions\n",
    "    ensemble=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3069da8",
   "metadata": {},
   "source": [
    "# Neural Networks with insider trading (Outsider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41557c7f",
   "metadata": {},
   "source": [
    "## 2.1 NN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d322d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. Load Feature Lists from Text Files\n",
    "# ======================================================\n",
    "base_path = \"/work/Thesis/Data/2. Outsider/\"\n",
    "\n",
    "# A. Load the Baseline (GKX) columns\n",
    "with open(base_path + \"gkx_cols.txt\", \"r\") as f:\n",
    "    baseline = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# B. Load the Reduced Insider columns\n",
    "with open(base_path + \"insider_cols.txt\", \"r\") as f:\n",
    "    insider_cols = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# ======================================================\n",
    "# 2. Combine the Lists\n",
    "# ======================================================\n",
    "combined_features = baseline + insider_cols\n",
    "\n",
    "print(f\"Baseline features: {len(baseline)}\")\n",
    "print(f\"Insider features:  {len(insider_cols)}\")\n",
    "print(f\"Total input size:  {len(combined_features)}\")\n",
    "\n",
    "# ======================================================\n",
    "# 4. Run Neural Network\n",
    "# ======================================================\n",
    "path = \"/work/Thesis/Data/2. Outsider/with_outsider.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "# ======================================================\n",
    "# 4. Run Neural Networks: NN1, NN2, NN3, NN4 (insider, non-lag)\n",
    "# ======================================================\n",
    "\n",
    "# NN1 – shallow network\n",
    "res_nn1_outsider = neural_network(\n",
    "    df=df,\n",
    "    features=combined_features,\n",
    "    use_all_features=False,\n",
    "    target=\"ret_excess\",\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    epochs=100,\n",
    "    batch_size=10000,\n",
    "    lr=[0.001, 0.01],\n",
    "    l1_reg=[1e-5, 1e-3],\n",
    "    hidden_depth=1,        # <--- NN1\n",
    "    ensemble=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== NN1: flatten & save data =====================\n",
    "y_true, y_pred = map(np.concatenate, zip(*res_nn1_outsider[\"y_tests\"]))\n",
    "x_all = pd.concat(res_nn1_outsider[\"X_tests\"], ignore_index=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"month\":           pd.to_datetime(x_all[\"month\"]),\n",
    "    \"cik\":             x_all[\"cik\"],\n",
    "    \"permno\":          x_all[\"permno\"],\n",
    "    \"ret_excess\":      y_true,\n",
    "    \"prc\":             x_all[\"prc\"],\n",
    "    \"shrout\":          x_all[\"shrout\"],\n",
    "    \"mktcap_lag\":      x_all[\"mktcap_lag\"],\n",
    "    \"pred_ret_excess\": y_pred,\n",
    "}).to_parquet(\"nn1_outsider_output.parquet\", index=False)\n",
    "\n",
    "# save model & weights\n",
    "res_nn1_outsider[\"model\"].save(\"nn1_outsider_model.keras\")\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"nn1_outsider_ensemble_weights.npz\",\n",
    "    weights=np.array(res_nn1_outsider[\"ensemble_weights\"], dtype=object),\n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "# save r2 metrics\n",
    "with open(\"nn1_outsider_r2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"r2_full\": float(res_nn1_outsider[\"R2_full\"]),\n",
    "        \"r2_window\": {k: float(v) for k, v in res_nn1_outsider[\"R2_window\"].items()}\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"saved: nn1_outsider_output.parquet, nn1_outsider_model.keras, weights, and r2 json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060dbfa8",
   "metadata": {},
   "source": [
    "## 3.2 NN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39327104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. Load Feature Lists from Text Files\n",
    "# ======================================================\n",
    "base_path = \"/work/Thesis/Data/2. Outsider/\"\n",
    "\n",
    "# A. Load the Baseline (GKX) columns\n",
    "with open(base_path + \"gkx_cols.txt\", \"r\") as f:\n",
    "    baseline = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# B. Load the Reduced Insider columns\n",
    "with open(base_path + \"insider_cols.txt\", \"r\") as f:\n",
    "    insider_cols = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# ======================================================\n",
    "# 2. Combine the Lists\n",
    "# ======================================================\n",
    "combined_features = baseline + insider_cols\n",
    "\n",
    "print(f\"Baseline features: {len(baseline)}\")\n",
    "print(f\"Insider features:  {len(insider_cols)}\")\n",
    "print(f\"Total input size:  {len(combined_features)}\")\n",
    "\n",
    "# ======================================================\n",
    "# 4. Run Neural Network\n",
    "# ======================================================\n",
    "path = \"/work/Thesis/Data/2. Outsider/with_outsider.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "# NN2 – two hidden layers\n",
    "res_nn2_outsider = neural_network(\n",
    "    df=df,\n",
    "    features=combined_features,\n",
    "    use_all_features=False,\n",
    "    target=\"ret_excess\",\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    epochs=100,\n",
    "    batch_size=10000,\n",
    "    lr=[0.001, 0.01],\n",
    "    l1_reg=[1e-5, 1e-3],\n",
    "    hidden_depth=2,        # <--- NN2\n",
    "    ensemble=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de64410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== NN2: flatten & save data =====================\n",
    "y_true, y_pred = map(np.concatenate, zip(*res_nn2_outsider[\"y_tests\"]))\n",
    "x_all = pd.concat(res_nn2_outsider[\"X_tests\"], ignore_index=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"month\":           pd.to_datetime(x_all[\"month\"]),\n",
    "    \"cik\":             x_all[\"cik\"],\n",
    "    \"permno\":          x_all[\"permno\"],\n",
    "    \"ret_excess\":      y_true,\n",
    "    \"prc\":             x_all[\"prc\"],\n",
    "    \"shrout\":          x_all[\"shrout\"],\n",
    "    \"mktcap_lag\":      x_all[\"mktcap_lag\"],\n",
    "    \"pred_ret_excess\": y_pred,\n",
    "}).to_parquet(\"nn2_outsider_output.parquet\", index=False)\n",
    "\n",
    "# save model & weights\n",
    "res_nn2_outsider[\"model\"].save(\"nn2_outsider_model.keras\")\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"nn2_outsider_ensemble_weights.npz\",\n",
    "    weights=np.array(res_nn2_outsider[\"ensemble_weights\"], dtype=object),\n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "# save r2 metrics\n",
    "with open(\"nn2_outsider_r2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"r2_full\": float(res_nn2_outsider[\"R2_full\"]),\n",
    "        \"r2_window\": {k: float(v) for k, v in res_nn2_outsider[\"R2_window\"].items()}\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"saved: nn2_outsider_output.parquet, nn2_outsider_model.keras, weights, and r2 json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719fa62b",
   "metadata": {},
   "source": [
    "## 3.3 NN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1b7e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. Load Feature Lists from Text Files\n",
    "# ======================================================\n",
    "base_path = \"/work/Thesis/Data/2. Outsider/\"\n",
    "\n",
    "# A. Load the Baseline (GKX) columns\n",
    "with open(base_path + \"gkx_cols.txt\", \"r\") as f:\n",
    "    baseline = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# B. Load the Reduced Insider columns\n",
    "with open(base_path + \"insider_cols.txt\", \"r\") as f:\n",
    "    insider_cols = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# ======================================================\n",
    "# 2. Combine the Lists\n",
    "# ======================================================\n",
    "combined_features = baseline + insider_cols\n",
    "\n",
    "print(f\"Baseline features: {len(baseline)}\")\n",
    "print(f\"Insider features:  {len(insider_cols)}\")\n",
    "print(f\"Total input size:  {len(combined_features)}\")\n",
    "\n",
    "# ======================================================\n",
    "# 4. Run Neural Network\n",
    "# ======================================================\n",
    "path = \"/work/Thesis/Data/2. Outsider/with_outsider.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "# NN3 – three hidden layers\n",
    "res_nn3_outsider = neural_network(\n",
    "    df=df,\n",
    "    features=combined_features,\n",
    "    use_all_features=False,\n",
    "    target=\"ret_excess\",\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    epochs=100,\n",
    "    batch_size=10000,\n",
    "    lr=[0.001, 0.01],\n",
    "    l1_reg=[1e-5, 1e-3],\n",
    "    hidden_depth=3,        # <--- NN3\n",
    "    ensemble=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== NN2: flatten & save data =====================\n",
    "y_true, y_pred = map(np.concatenate, zip(*res_nn3_outsider[\"y_tests\"]))\n",
    "x_all = pd.concat(res_nn3_outsider[\"X_tests\"], ignore_index=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"month\":           pd.to_datetime(x_all[\"month\"]),\n",
    "    \"cik\":             x_all[\"cik\"],\n",
    "    \"permno\":          x_all[\"permno\"],\n",
    "    \"ret_excess\":      y_true,\n",
    "    \"prc\":             x_all[\"prc\"],\n",
    "    \"shrout\":          x_all[\"shrout\"],\n",
    "    \"mktcap_lag\":      x_all[\"mktcap_lag\"],\n",
    "    \"pred_ret_excess\": y_pred,\n",
    "}).to_parquet(\"nn3_outsider_output.parquet\", index=False)\n",
    "\n",
    "# save model & weights\n",
    "res_nn3_outsider[\"model\"].save(\"nn3_outsider_model.keras\")\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"nn3_outsider_ensemble_weights.npz\",\n",
    "    weights=np.array(res_nn3_outsider[\"ensemble_weights\"], dtype=object),\n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "# save r2 metrics\n",
    "with open(\"nn3_outsider_r2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"r2_full\": float(res_nn3_outsider[\"R2_full\"]),\n",
    "        \"r2_window\": {k: float(v) for k, v in res_nn3_outsider[\"R2_window\"].items()}\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"saved: nn3_outsider_output.parquet, nn3_outsider_model.keras, weights, and r2 json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f5fa3",
   "metadata": {},
   "source": [
    "## 3.4 NN4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b994c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. Load Feature Lists from Text Files\n",
    "# ======================================================\n",
    "base_path = \"/work/Thesis/Data/2. Outsider/\"\n",
    "\n",
    "# A. Load the Baseline (GKX) columns\n",
    "with open(base_path + \"gkx_cols.txt\", \"r\") as f:\n",
    "    baseline = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# B. Load the Reduced Insider columns\n",
    "with open(base_path + \"insider_cols.txt\", \"r\") as f:\n",
    "    insider_cols = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# ======================================================\n",
    "# 2. Combine the Lists\n",
    "# ======================================================\n",
    "combined_features = baseline + insider_cols\n",
    "\n",
    "print(f\"Baseline features: {len(baseline)}\")\n",
    "print(f\"Insider features:  {len(insider_cols)}\")\n",
    "print(f\"Total input size:  {len(combined_features)}\")\n",
    "\n",
    "# ======================================================\n",
    "# 4. Run Neural Network\n",
    "# ======================================================\n",
    "path = \"/work/Thesis/Data/2. Outsider/with_outsider.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "res_nn4_outsider = neural_network(\n",
    "    df=df,\n",
    "    features=combined_features,   # <--- Uses the combined list\n",
    "    use_all_features=False,\n",
    "    target=\"ret_excess\",\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    epochs=100,\n",
    "    batch_size=10000,\n",
    "    lr=[0.001, 0.01],             # Expand ranges if you have time!\n",
    "    l1_reg=[1e-5, 1e-3],         \n",
    "    hidden_depth=4,               # Consider changing to 2 or 3 for interactions\n",
    "    ensemble=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a2eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== NN4 (insider, non-lag): flatten & save data =====================\n",
    "y_true, y_pred = map(np.concatenate, zip(*res_nn4_outsider[\"y_tests\"]))\n",
    "x_all = pd.concat(res_nn4_outsider[\"X_tests\"], ignore_index=True)\n",
    "\n",
    "df_out = pd.DataFrame({\n",
    "    \"month\":           pd.to_datetime(x_all[\"month\"]),\n",
    "    \"cik\":             x_all[\"cik\"].values,\n",
    "    \"permno\":          x_all[\"permno\"].values,\n",
    "    \"ret_excess\":      y_true,\n",
    "    \"prc\":             x_all[\"prc\"].values,\n",
    "    \"shrout\":          x_all[\"shrout\"].values,\n",
    "    \"mktcap_lag\":      x_all[\"mktcap_lag\"].values,\n",
    "    \"pred_ret_excess\": y_pred\n",
    "}).to_parquet(\"nn4_outsider_output.parquet\", index=False)\n",
    "\n",
    "# ===================== Save model =====================\n",
    "res_nn4_outsider[\"model\"].save(\"nn4_outsider_model.keras\")\n",
    "\n",
    "# ===================== Save ensemble weights (simple version) =====================\n",
    "np.savez_compressed(\n",
    "    \"nn4_outsider_ensemble_weights.npz\",\n",
    "    weights=np.array(res_nn4_outsider[\"ensemble_weights\"], dtype=object),\n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "# ===================== Save R² metrics =====================\n",
    "import json\n",
    "with open(\"nn4_outsider_r2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"r2_full\": float(res_nn4_outsider[\"R2_full\"]),\n",
    "        \"r2_window\": {k: float(v) for k, v in res_nn4_outsider[\"R2_window\"].items()}\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"saved: nn4_outsider_output.parquet, nn4_outsider_model.keras, weights, and r2 json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b1b36",
   "metadata": {},
   "source": [
    "# Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048134fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# BLOCK-WISE VARIABLE IMPORTANCE FOR NN4 (BASELINE + INTERACTIONS)\n",
    "# ==============================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# GKX R² FUNCTION\n",
    "# ==============================================================\n",
    "\n",
    "def r2_gkx(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    GKX-style R²: 1 - sum(e²) / sum(y²).\n",
    "    Uses raw y (no demeaning) in the denominator.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=\"float64\")\n",
    "    y_pred = np.asarray(y_pred, dtype=\"float64\")\n",
    "    sse = np.sum((y_true - y_pred) ** 2)\n",
    "    sst = np.sum(y_true ** 2)\n",
    "    if sst == 0:\n",
    "        return np.nan\n",
    "    return 1 - sse / sst\n",
    "\n",
    "# ==============================================================\n",
    "# WORKER: ΔR² FOR ONE BASELINE FEATURE (MAIN + INTERACTIONS)\n",
    "# ==============================================================\n",
    "\n",
    "def _compute_feature_drop(\n",
    "    feat, idx_list, X_train, y_train, R2_full, SST_w,\n",
    "    window_id, nn_model_path, window_weights\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute ΔR² for ONE baseline feature in ONE window,\n",
    "    zeroing BOTH the main effect and all its interaction columns.\n",
    "    \"\"\"\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "    os.environ[\"TF_CPP_MIN_LOG_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "\n",
    "    model = tf.keras.models.load_model(nn_model_path, compile=False)\n",
    "    model.set_weights(window_weights.tolist())\n",
    "\n",
    "    X_mod = X_train.copy()\n",
    "    X_mod[:, idx_list] = 0.0  # zero ALL related cols\n",
    "\n",
    "    y_hat_zero = model.predict(X_mod, verbose=0).ravel()\n",
    "    R2_zero = r2_gkx(y_train, y_hat_zero)\n",
    "\n",
    "    dR2 = R2_full - R2_zero\n",
    "\n",
    "    return {\n",
    "        \"window\": window_id,\n",
    "        \"feature\": feat,\n",
    "        \"R2_baseline\": R2_full,\n",
    "        \"R2_removed\": R2_zero,\n",
    "        \"R2_drop\": dR2,\n",
    "        \"SST\": SST_w,\n",
    "        \"n_obs\": len(y_train),\n",
    "    }\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# MAIN FUNCTION — BLOCK-WISE VI (BASELINE + INTERACTIONS)\n",
    "# ==============================================================\n",
    "\n",
    "def variable_importance(\n",
    "    df_path,\n",
    "    target,\n",
    "    features_vi,\n",
    "    start_year,\n",
    "    end_year,\n",
    "    train_size,\n",
    "    val_size,\n",
    "    test_size,\n",
    "    step_size,\n",
    "    nn_model_path=None,\n",
    "    nn_weights_path=None,\n",
    "    max_rows_per_window=None,\n",
    "    random_state=None,\n",
    "    n_jobs=None,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Block-wise variable importance for baseline characteristics.\n",
    "\n",
    "    For each baseline feature f in features_vi:\n",
    "    - Identify block B_f = [f] + all interaction columns 'inter_f_x_*'\n",
    "    - Zero ALL columns in B_f and recompute R² (train-only) for each window.\n",
    "    - ΔR²_f = drop in R² when B_f is zeroed.\n",
    "    - Aggregate across windows with SST weights, and normalize to VI weights\n",
    "      ONLY across the baseline features (not all 915 inputs).\n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 1) Load data and filter time\n",
    "    # ----------------------------------------------------------\n",
    "    df = pd.read_parquet(df_path).copy()\n",
    "    df[\"month\"] = pd.to_datetime(df[\"month\"])\n",
    "\n",
    "    df = df[\n",
    "        (df[\"month\"].dt.year >= start_year) &\n",
    "        (df[\"month\"].dt.year <= end_year)\n",
    "    ].copy()\n",
    "\n",
    "    # Non-feature columns\n",
    "    drop_cols = [\"month\", \"year\", \"cik\", \"permno\", target,\n",
    "                 \"prc\", \"shrout\", \"mktcap_lag\"]\n",
    "    drop_cols = [c for c in drop_cols if c in df.columns]\n",
    "\n",
    "    # Candidate features in file order\n",
    "    features_all = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 2) Load NN4 + weights and align input dimension (915)\n",
    "    # ----------------------------------------------------------\n",
    "    base_model = tf.keras.models.load_model(nn_model_path, compile=False)\n",
    "    weights_arr = np.load(nn_weights_path, allow_pickle=True)[\"weights\"]\n",
    "\n",
    "    input_dim = base_model.input_shape[1]  # should be 915 for NN4\n",
    "\n",
    "    if len(features_all) < input_dim:\n",
    "        raise ValueError(\n",
    "            f\"Only {len(features_all)} feature columns, but model expects {input_dim}\"\n",
    "        )\n",
    "    if len(features_all) > input_dim:\n",
    "        # Keep only first 915 columns (matching training design)\n",
    "        features_all = features_all[:input_dim]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 3) Build block mapping: baseline feature -> [main + interactions]\n",
    "    # ----------------------------------------------------------\n",
    "    # Keep only baseline features present in features_all\n",
    "    features_vi = [f for f in features_vi if f in features_all]\n",
    "    if len(features_vi) == 0:\n",
    "        raise ValueError(\"None of the requested VI baseline features are in features_all.\")\n",
    "\n",
    "    drop_index_map = {}\n",
    "\n",
    "    for f in features_vi:\n",
    "        related_cols = []\n",
    "\n",
    "        # main effect\n",
    "        if f in features_all:\n",
    "            related_cols.append(f)\n",
    "\n",
    "        # interactions: inter_<f>_x_*\n",
    "        prefix = f\"inter_{f}_x_\"\n",
    "        for col in features_all:\n",
    "            if col.startswith(prefix):\n",
    "                related_cols.append(col)\n",
    "\n",
    "        if not related_cols:\n",
    "            # Fallback: zero at least main effect\n",
    "            related_cols = [f]\n",
    "\n",
    "        idx_list = [features_all.index(col) for col in related_cols]\n",
    "        drop_index_map[f] = idx_list\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[Block] {f}: {len(idx_list)} columns (main + interactions)\")\n",
    "\n",
    "    if n_jobs is None:\n",
    "        n_jobs = joblib.cpu_count()\n",
    "\n",
    "    rows_window = []\n",
    "    sst_total = 0.0\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 4) Rolling windows (train-only for VI)\n",
    "    # ----------------------------------------------------------\n",
    "    for window_id, (train, val, test) in enumerate(\n",
    "        expanding_window_split(\n",
    "            df=df,\n",
    "            train_size=train_size,\n",
    "            val_size=val_size,\n",
    "            test_size=test_size,\n",
    "            step_size=step_size,\n",
    "            start_date=f\"{start_year}-01-01\",\n",
    "            end_date=f\"{end_year}-12-31\",\n",
    "        ),\n",
    "        start=0,\n",
    "    ):\n",
    "\n",
    "        if window_id >= len(weights_arr):\n",
    "            break\n",
    "        if train.empty:\n",
    "            continue\n",
    "\n",
    "        # ----- TRAIN ONLY -----\n",
    "        X_train_df = train[features_all]\n",
    "        y_train = train[target].to_numpy(\"float32\")\n",
    "\n",
    "        # Optional row subsampling for safety\n",
    "        if max_rows_per_window and len(X_train_df) > max_rows_per_window:\n",
    "            idx = rng.choice(len(X_train_df), max_rows_per_window, replace=False)\n",
    "            X_train_df = X_train_df.iloc[idx]\n",
    "            y_train = y_train[idx]\n",
    "\n",
    "        X_train = X_train_df.to_numpy(\"float32\")\n",
    "\n",
    "        # Set window weights once\n",
    "        base_model.set_weights(weights_arr[window_id].tolist())\n",
    "        y_hat_full = base_model.predict(X_train, verbose=0).ravel()\n",
    "\n",
    "        R2_full = r2_gkx(y_train, y_hat_full)\n",
    "        SST_w = float(np.sum(y_train**2))\n",
    "\n",
    "        if np.isnan(R2_full) or SST_w == 0:\n",
    "            if verbose:\n",
    "                print(f\"[Window {window_id}] skipped (NaN R² or zero SST).\")\n",
    "            continue\n",
    "\n",
    "        sst_total += SST_w\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[Window {window_id}] R²_full(train) = {R2_full:.4f}, n={len(y_train)}\")\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # 5) Parallel block-wise feature removal\n",
    "        # ------------------------------------------------------\n",
    "        parallel_out = Parallel(n_jobs=n_jobs, backend=\"loky\")(\n",
    "            delayed(_compute_feature_drop)(\n",
    "                feat,\n",
    "                drop_index_map[feat],   # main + interactions\n",
    "                X_train,\n",
    "                y_train,\n",
    "                R2_full,\n",
    "                SST_w,\n",
    "                window_id,\n",
    "                nn_model_path,\n",
    "                weights_arr[window_id],\n",
    "            )\n",
    "            for feat in features_vi\n",
    "        )\n",
    "\n",
    "        rows_window.extend(parallel_out)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 6) Aggregate across windows — ONLY TRUE REDUCTIONS\n",
    "    # ----------------------------------------------------------\n",
    "    dfw = pd.DataFrame(rows_window)\n",
    "\n",
    "    if dfw.empty:\n",
    "        raise ValueError(\"No VI rows computed — check filters / windows / data.\")\n",
    "\n",
    "    dfw[\"True_Reduction\"] = dfw[\"R2_drop\"].clip(lower=0)\n",
    "\n",
    "    num = dfw.groupby(\"feature\").apply(\n",
    "        lambda g: np.sum(g[\"SST\"] * g[\"True_Reduction\"])\n",
    "    )\n",
    "\n",
    "    R2_reduction_global = num / sst_total\n",
    "\n",
    "    # Normalize ONLY across the baseline features (the 94 variables)\n",
    "    R2_pos = np.clip(R2_reduction_global, 0, None)\n",
    "    VI_weight = R2_pos / R2_pos.sum()\n",
    "\n",
    "    vi_global = pd.DataFrame({\n",
    "        \"feature\": num.index,\n",
    "        \"R2_reduction_global\": R2_reduction_global.values,\n",
    "        \"VI_weight\": VI_weight.values,\n",
    "    }).sort_values(\"R2_reduction_global\", ascending=False)\n",
    "\n",
    "    # Optionally show globally excluded vars (never reduce R²)\n",
    "    tol = 1e-12\n",
    "    excluded = vi_global.loc[\n",
    "        vi_global[\"R2_reduction_global\"] <= tol\n",
    "    ][\"feature\"].tolist()\n",
    "\n",
    "    print(\"\\n==============================================\")\n",
    "    print(\" VARIABLES EXCLUDED GLOBALLY (never reduce R²)\")\n",
    "    print(\"==============================================\")\n",
    "    for v in excluded:\n",
    "        print(\"  -\", v)\n",
    "    print(\"==============================================\\n\")\n",
    "\n",
    "    return vi_global, dfw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ff655",
   "metadata": {},
   "source": [
    "# NN4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a68af7e",
   "metadata": {},
   "source": [
    "## Without insider trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae02927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# BASELINE PREDICTORS (94) — ONE BLOCK EACH\n",
    "# ==============================================================\n",
    "\n",
    "baseline_predictors = [\n",
    "    'char_mvel1',\n",
    "    'char_beta',\n",
    "    'char_betasq',\n",
    "    'char_chmom',\n",
    "    'char_dolvol',\n",
    "    'char_idiovol',\n",
    "    'char_indmom',\n",
    "    'char_mom1m',\n",
    "    'char_mom6m',\n",
    "    'char_mom12m',\n",
    "    'char_mom36m',\n",
    "    'char_pricedelay',\n",
    "    'char_turn',\n",
    "    'char_absacc',\n",
    "    'char_acc',\n",
    "    'char_age',\n",
    "    'char_agr',\n",
    "    'char_bm',\n",
    "    'char_bm_ia',\n",
    "    'char_cashdebt',\n",
    "    'char_cashpr',\n",
    "    'char_cfp',\n",
    "    'char_cfp_ia',\n",
    "    'char_chatoia',\n",
    "    'char_chcsho',\n",
    "    'char_chempia',\n",
    "    'char_chinv',\n",
    "    'char_chpmia',\n",
    "    'char_convind',\n",
    "    'char_currat',\n",
    "    'char_depr',\n",
    "    'char_divi',\n",
    "    'char_divo',\n",
    "    'char_dy',\n",
    "    'char_egr',\n",
    "    'char_ep',\n",
    "    'char_gma',\n",
    "    'char_grcapx',\n",
    "    'char_grltnoa',\n",
    "    'char_herf',\n",
    "    'char_hire',\n",
    "    'char_invest',\n",
    "    'char_lev',\n",
    "    'char_lgr',\n",
    "    'char_mve_ia',\n",
    "    'char_operprof',\n",
    "    'char_orgcap',\n",
    "    'char_pchcapx_ia',\n",
    "    'char_pchcurrat',\n",
    "    'char_pchdepr',\n",
    "    'char_pchgm_pchsale',\n",
    "    'char_pchquick',\n",
    "    'char_pchsale_pchinvt',\n",
    "    'char_pchsale_pchrect',\n",
    "    'char_pchsale_pchxsga',\n",
    "    'char_pchsaleinv',\n",
    "    'char_pctacc',\n",
    "    'char_ps',\n",
    "    'char_quick',\n",
    "    'char_rd',\n",
    "    'char_rd_mve',\n",
    "    'char_rd_sale',\n",
    "    'char_realestate',\n",
    "    'char_roic',\n",
    "    'char_salecash',\n",
    "    'char_saleinv',\n",
    "    'char_salerec',\n",
    "    'char_secured',\n",
    "    'char_securedind',\n",
    "    'char_sgr',\n",
    "    'char_sin',\n",
    "    'char_sp',\n",
    "    'char_tang',\n",
    "    'char_tb',\n",
    "    'char_aeavol',\n",
    "    'char_cash',\n",
    "    'char_chtx',\n",
    "    'char_cinvest',\n",
    "    'char_ear',\n",
    "    'char_nincr',\n",
    "    'char_roaq',\n",
    "    'char_roavol',\n",
    "    'char_roeq',\n",
    "    'char_rsup',\n",
    "    'char_stdacc',\n",
    "    'char_stdcf',\n",
    "    'char_ms',\n",
    "    'char_baspread',\n",
    "    'char_ill',\n",
    "    'char_maxret',\n",
    "    'char_retvol',\n",
    "    'char_std_dolvol',\n",
    "    'char_std_turn',\n",
    "    'char_zerotrade',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/work/Thesis/Data/finalized_true.parquet\"\n",
    "target_col = \"ret_excess\"\n",
    "\n",
    "vi_global, vi_window = variable_importance(\n",
    "    df_path=path,\n",
    "    target=target_col,\n",
    "    features_vi=baseline_predictors,\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    nn_model_path=\"nn4_model.keras\",\n",
    "    nn_weights_path=\"nn4_ensemble_weights.npz\",\n",
    "    max_rows_per_window=None,   # or 20000 if kernel dies\n",
    "    random_state=42,\n",
    "    n_jobs=25,                   # start with 4, not 60, to be safe\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"=== Global VI (block-wise, train-only) ===\")\n",
    "print(vi_global.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5524990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "vi_global.to_csv(\"NN1_VI_global.csv\", index=False)\n",
    "vi_window.to_csv(\"NN1_VI_window.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1599d16c",
   "metadata": {},
   "source": [
    "## With insider trading (outsider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00063d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_insider = [\n",
    "    'char_mvel1',\n",
    "    'char_beta',\n",
    "    'char_betasq',\n",
    "    'char_chmom',\n",
    "    'char_dolvol',\n",
    "    'char_idiovol',\n",
    "    'char_indmom',\n",
    "    'char_mom1m',\n",
    "    'char_mom6m',\n",
    "    'char_mom12m',\n",
    "    'char_mom36m',\n",
    "    'char_pricedelay',\n",
    "    'char_turn',\n",
    "    'char_absacc',\n",
    "    'char_acc',\n",
    "    'char_age',\n",
    "    'char_agr',\n",
    "    'char_bm',\n",
    "    'char_bm_ia',\n",
    "    'char_cashdebt',\n",
    "    'char_cashpr',\n",
    "    'char_cfp',\n",
    "    'char_cfp_ia',\n",
    "    'char_chatoia',\n",
    "    'char_chcsho',\n",
    "    'char_chempia',\n",
    "    'char_chinv',\n",
    "    'char_chpmia',\n",
    "    'char_convind',\n",
    "    'char_currat',\n",
    "    'char_depr',\n",
    "    'char_divi',\n",
    "    'char_divo',\n",
    "    'char_dy',\n",
    "    'char_egr',\n",
    "    'char_ep',\n",
    "    'char_gma',\n",
    "    'char_grcapx',\n",
    "    'char_grltnoa',\n",
    "    'char_herf',\n",
    "    'char_hire',\n",
    "    'char_invest',\n",
    "    'char_lev',\n",
    "    'char_lgr',\n",
    "    'char_mve_ia',\n",
    "    'char_operprof',\n",
    "    'char_orgcap',\n",
    "    'char_pchcapx_ia',\n",
    "    'char_pchcurrat',\n",
    "    'char_pchdepr',\n",
    "    'char_pchgm_pchsale',\n",
    "    'char_pchquick',\n",
    "    'char_pchsale_pchinvt',\n",
    "    'char_pchsale_pchrect',\n",
    "    'char_pchsale_pchxsga',\n",
    "    'char_pchsaleinv',\n",
    "    'char_pctacc',\n",
    "    'char_ps',\n",
    "    'char_quick',\n",
    "    'char_rd',\n",
    "    'char_rd_mve',\n",
    "    'char_rd_sale',\n",
    "    'char_realestate',\n",
    "    'char_roic',\n",
    "    'char_salecash',\n",
    "    'char_saleinv',\n",
    "    'char_salerec',\n",
    "    'char_secured',\n",
    "    'char_securedind',\n",
    "    'char_sgr',\n",
    "    'char_sin',\n",
    "    'char_sp',\n",
    "    'char_tang',\n",
    "    'char_tb',\n",
    "    'char_aeavol',\n",
    "    'char_cash',\n",
    "    'char_chtx',\n",
    "    'char_cinvest',\n",
    "    'char_ear',\n",
    "    'char_nincr',\n",
    "    'char_roaq',\n",
    "    'char_roavol',\n",
    "    'char_roeq',\n",
    "    'char_rsup',\n",
    "    'char_stdacc',\n",
    "    'char_stdcf',\n",
    "    'char_ms',\n",
    "    'char_baspread',\n",
    "    'char_ill',\n",
    "    'char_maxret',\n",
    "    'char_retvol',\n",
    "    'char_std_dolvol',\n",
    "    'char_std_turn',\n",
    "    'char_zerotrade',\n",
    "    #Insider trading data\n",
    "    \"is_npr_volume\",\n",
    "    \"is_net_cluster\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/work/Thesis/Data/2. Outsider/with_outsider.parquet\"\n",
    "target_col = \"ret_excess\"\n",
    "\n",
    "vi_global_outsider, vi_window_outsider = variable_importance(\n",
    "    df_path=path,\n",
    "    target=target_col,\n",
    "    features_vi=baseline_insider,\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    nn_model_path=\"nn4_outsider_model.keras\",\n",
    "    nn_weights_path=\"nn4_outsider_ensemble_weights.npz\",\n",
    "    max_rows_per_window=None,   # or 20000 if kernel dies\n",
    "    random_state=42,\n",
    "    n_jobs=40,                   # start with 4, not 60, to be safe\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"=== Global VI (block-wise, train-only) ===\")\n",
    "print(vi_global_outsider.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1233e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "vi_global_outsider.to_csv(\"nn4_outsider_VI_global.csv\", index=False)\n",
    "vi_window_outsider.to_csv(\"nn4_outsider_VI_window.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13326ad9",
   "metadata": {},
   "source": [
    "## With insider trading (insider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/work/Thesis/Data/3. Insider/with_insider.parquet\"\n",
    "target_col = \"ret_excess\"\n",
    "\n",
    "vi_global_insider, vi_window_insider = variable_importance(\n",
    "    df_path=path,\n",
    "    target=target_col,\n",
    "    features_vi=baseline_insider,\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    nn_model_path=\"nn4_insider_model.keras\",\n",
    "    nn_weights_path=\"nn4_insider_ensemble_weights.npz\",\n",
    "    max_rows_per_window=None,   # or 20000 if kernel dies\n",
    "    random_state=42,\n",
    "    n_jobs=35,                   # start with 4, not 60, to be safe\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"=== Global VI (block-wise, train-only) ===\")\n",
    "print(vi_global_insider.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Global VI (block-wise, train-only) ===\")\n",
    "print(vi_global_insider_non_lag.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfaa687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "vi_global_insider.to_csv(\"nn4_insider_VI_global.csv\", index=False)\n",
    "vi_window_insider.to_csv(\"nn4_insider_VI_window.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223d3809",
   "metadata": {},
   "source": [
    "# NN3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c1b29",
   "metadata": {},
   "source": [
    "## Without insider trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47260b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# BASELINE PREDICTORS (94) — ONE BLOCK EACH\n",
    "# ==============================================================\n",
    "\n",
    "baseline_predictors = [\n",
    "    'char_mvel1',\n",
    "    'char_beta',\n",
    "    'char_betasq',\n",
    "    'char_chmom',\n",
    "    'char_dolvol',\n",
    "    'char_idiovol',\n",
    "    'char_indmom',\n",
    "    'char_mom1m',\n",
    "    'char_mom6m',\n",
    "    'char_mom12m',\n",
    "    'char_mom36m',\n",
    "    'char_pricedelay',\n",
    "    'char_turn',\n",
    "    'char_absacc',\n",
    "    'char_acc',\n",
    "    'char_age',\n",
    "    'char_agr',\n",
    "    'char_bm',\n",
    "    'char_bm_ia',\n",
    "    'char_cashdebt',\n",
    "    'char_cashpr',\n",
    "    'char_cfp',\n",
    "    'char_cfp_ia',\n",
    "    'char_chatoia',\n",
    "    'char_chcsho',\n",
    "    'char_chempia',\n",
    "    'char_chinv',\n",
    "    'char_chpmia',\n",
    "    'char_convind',\n",
    "    'char_currat',\n",
    "    'char_depr',\n",
    "    'char_divi',\n",
    "    'char_divo',\n",
    "    'char_dy',\n",
    "    'char_egr',\n",
    "    'char_ep',\n",
    "    'char_gma',\n",
    "    'char_grcapx',\n",
    "    'char_grltnoa',\n",
    "    'char_herf',\n",
    "    'char_hire',\n",
    "    'char_invest',\n",
    "    'char_lev',\n",
    "    'char_lgr',\n",
    "    'char_mve_ia',\n",
    "    'char_operprof',\n",
    "    'char_orgcap',\n",
    "    'char_pchcapx_ia',\n",
    "    'char_pchcurrat',\n",
    "    'char_pchdepr',\n",
    "    'char_pchgm_pchsale',\n",
    "    'char_pchquick',\n",
    "    'char_pchsale_pchinvt',\n",
    "    'char_pchsale_pchrect',\n",
    "    'char_pchsale_pchxsga',\n",
    "    'char_pchsaleinv',\n",
    "    'char_pctacc',\n",
    "    'char_ps',\n",
    "    'char_quick',\n",
    "    'char_rd',\n",
    "    'char_rd_mve',\n",
    "    'char_rd_sale',\n",
    "    'char_realestate',\n",
    "    'char_roic',\n",
    "    'char_salecash',\n",
    "    'char_saleinv',\n",
    "    'char_salerec',\n",
    "    'char_secured',\n",
    "    'char_securedind',\n",
    "    'char_sgr',\n",
    "    'char_sin',\n",
    "    'char_sp',\n",
    "    'char_tang',\n",
    "    'char_tb',\n",
    "    'char_aeavol',\n",
    "    'char_cash',\n",
    "    'char_chtx',\n",
    "    'char_cinvest',\n",
    "    'char_ear',\n",
    "    'char_nincr',\n",
    "    'char_roaq',\n",
    "    'char_roavol',\n",
    "    'char_roeq',\n",
    "    'char_rsup',\n",
    "    'char_stdacc',\n",
    "    'char_stdcf',\n",
    "    'char_ms',\n",
    "    'char_baspread',\n",
    "    'char_ill',\n",
    "    'char_maxret',\n",
    "    'char_retvol',\n",
    "    'char_std_dolvol',\n",
    "    'char_std_turn',\n",
    "    'char_zerotrade',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/work/Thesis/Data/2. Outsider/with_outsider.parquet\"\n",
    "target_col = \"ret_excess\"\n",
    "\n",
    "vi_global_nn3, vi_window_nn3 = variable_importance(\n",
    "    df_path=path,\n",
    "    target=target_col,\n",
    "    features_vi=baseline_predictors,\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    nn_model_path=\"nn3_model.keras\",\n",
    "    nn_weights_path=\"nn3_ensemble_weights.npz\",\n",
    "    max_rows_per_window=None,   # or 20000 if kernel dies\n",
    "    random_state=42,\n",
    "    n_jobs=25,                   # start with 4, not 60, to be safe\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"=== Global VI (block-wise, train-only) ===\")\n",
    "print(vi_gvi_global_nn3lobal.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e62665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "vi_global_nn3.to_csv(\"NN3_VI_global.csv\", index=False)\n",
    "vi_window_nn3.to_csv(\"NN3_VI_window.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d7686",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vi_global_nn3.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7acad4",
   "metadata": {},
   "source": [
    "## With insider trading (outsider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_insider = [\n",
    "    'char_mvel1',\n",
    "    'char_beta',\n",
    "    'char_betasq',\n",
    "    'char_chmom',\n",
    "    'char_dolvol',\n",
    "    'char_idiovol',\n",
    "    'char_indmom',\n",
    "    'char_mom1m',\n",
    "    'char_mom6m',\n",
    "    'char_mom12m',\n",
    "    'char_mom36m',\n",
    "    'char_pricedelay',\n",
    "    'char_turn',\n",
    "    'char_absacc',\n",
    "    'char_acc',\n",
    "    'char_age',\n",
    "    'char_agr',\n",
    "    'char_bm',\n",
    "    'char_bm_ia',\n",
    "    'char_cashdebt',\n",
    "    'char_cashpr',\n",
    "    'char_cfp',\n",
    "    'char_cfp_ia',\n",
    "    'char_chatoia',\n",
    "    'char_chcsho',\n",
    "    'char_chempia',\n",
    "    'char_chinv',\n",
    "    'char_chpmia',\n",
    "    'char_convind',\n",
    "    'char_currat',\n",
    "    'char_depr',\n",
    "    'char_divi',\n",
    "    'char_divo',\n",
    "    'char_dy',\n",
    "    'char_egr',\n",
    "    'char_ep',\n",
    "    'char_gma',\n",
    "    'char_grcapx',\n",
    "    'char_grltnoa',\n",
    "    'char_herf',\n",
    "    'char_hire',\n",
    "    'char_invest',\n",
    "    'char_lev',\n",
    "    'char_lgr',\n",
    "    'char_mve_ia',\n",
    "    'char_operprof',\n",
    "    'char_orgcap',\n",
    "    'char_pchcapx_ia',\n",
    "    'char_pchcurrat',\n",
    "    'char_pchdepr',\n",
    "    'char_pchgm_pchsale',\n",
    "    'char_pchquick',\n",
    "    'char_pchsale_pchinvt',\n",
    "    'char_pchsale_pchrect',\n",
    "    'char_pchsale_pchxsga',\n",
    "    'char_pchsaleinv',\n",
    "    'char_pctacc',\n",
    "    'char_ps',\n",
    "    'char_quick',\n",
    "    'char_rd',\n",
    "    'char_rd_mve',\n",
    "    'char_rd_sale',\n",
    "    'char_realestate',\n",
    "    'char_roic',\n",
    "    'char_salecash',\n",
    "    'char_saleinv',\n",
    "    'char_salerec',\n",
    "    'char_secured',\n",
    "    'char_securedind',\n",
    "    'char_sgr',\n",
    "    'char_sin',\n",
    "    'char_sp',\n",
    "    'char_tang',\n",
    "    'char_tb',\n",
    "    'char_aeavol',\n",
    "    'char_cash',\n",
    "    'char_chtx',\n",
    "    'char_cinvest',\n",
    "    'char_ear',\n",
    "    'char_nincr',\n",
    "    'char_roaq',\n",
    "    'char_roavol',\n",
    "    'char_roeq',\n",
    "    'char_rsup',\n",
    "    'char_stdacc',\n",
    "    'char_stdcf',\n",
    "    'char_ms',\n",
    "    'char_baspread',\n",
    "    'char_ill',\n",
    "    'char_maxret',\n",
    "    'char_retvol',\n",
    "    'char_std_dolvol',\n",
    "    'char_std_turn',\n",
    "    'char_zerotrade',\n",
    "    #Insider trading data\n",
    "    \"is_npr_volume\",\n",
    "    \"is_net_cluster\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee6b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/work/Thesis/Data/2. Outsider/with_outsider.parquet\"\n",
    "target_col = \"ret_excess\"\n",
    "\n",
    "vi_global_outsider_nn3, vi_window_outsider_nn3 = variable_importance(\n",
    "    df_path=path,\n",
    "    target=target_col,\n",
    "    features_vi=baseline_insider,\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    nn_model_path=\"nn3_outsider_model.keras\",\n",
    "    nn_weights_path=\"nn3_outsider_ensemble_weights.npz\",\n",
    "    max_rows_per_window=None,   # or 20000 if kernel dies\n",
    "    random_state=42,\n",
    "    n_jobs=40,                   # start with 4, not 60, to be safe\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"=== Global VI (block-wise, train-only) ===\")\n",
    "print(vi_global_outsider_nn3.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2570ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "vi_global_outsider_nn3.to_csv(\"nn3_outsider_VI_global.csv\", index=False)\n",
    "vi_global_outsider_nn3.to_csv(\"nn3_outsider_VI_window.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca92ce3a",
   "metadata": {},
   "source": [
    "## With insider trading (insider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f818a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/work/Thesis/Data/3. Insider/with_insider.parquet\"\n",
    "target_col = \"ret_excess\"\n",
    "\n",
    "vi_global_insider_nn3, vi_window_insider_nn3 = variable_importance(\n",
    "    df_path=path,\n",
    "    target=target_col,\n",
    "    features_vi=baseline_insider,\n",
    "    start_year=2005,\n",
    "    end_year=2021,\n",
    "    train_size=60,\n",
    "    val_size=36,\n",
    "    test_size=12,\n",
    "    step_size=12,\n",
    "    nn_model_path=\"nn3_insider_model.keras\",\n",
    "    nn_weights_path=\"nn3_insider_ensemble_weights.npz\",\n",
    "    max_rows_per_window=None,   # or 20000 if kernel dies\n",
    "    random_state=42,\n",
    "    n_jobs=35,                   # start with 4, not 60, to be safe\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"=== Global VI (block-wise, train-only) ===\")\n",
    "print(vi_window_insider_nn3.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3febfeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "vi_global_insider_nn3.to_csv(\"nn3_insider_VI_global.csv\", index=False)\n",
    "vi_global_insider_nn3.to_csv(\"nn3_insider_VI_window.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
